{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f827b4d",
   "metadata": {},
   "source": [
    "# Fixed Point Precision Study on LSTM-250\n",
    "\n",
    "\n",
    "This note book is the final stage of the model preparation for benchmarking. The next stage after this notebook is to simply implement the model in C using the knowledge gained from this notebook.\n",
    "The fixed-point operations defined here tries to simulate the computations performed in SPAR. This might change over-time.\n",
    "\n",
    "**NOTE:**\n",
    "The programs/code-snippets in this notebook follows C-like interfaces on purpose.\n",
    "This is done so that, these code can be easily translated into C for the next stage of study.\n",
    "\n",
    "**Goals:**\n",
    "- Load the Model and Dataset from the SQLite3 databases.\n",
    "- Validate numpy model on the dataset.\n",
    "- Define Fixed-Point methods.\n",
    "- Implement model in fixed-point.\n",
    "- Experiment with Fixed-Point precisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33489e62",
   "metadata": {},
   "source": [
    "# Load the Floating-Point Model and Dataset\n",
    "\n",
    "Here, the model and the dataset exported by the Export-DB notebook is loaded and verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86326c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the cache and import sqlite3 utilities\n",
    "!rm -rf __pycache__/\n",
    "from utilsqlite3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53a993",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b27044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and check the dataset table\n",
    "Dataset_path = './saved/timit_test_data-79.68p.s3db'\n",
    "\n",
    "table_names = getTableNames(Dataset_path)\n",
    "print('table_names:', table_names)\n",
    "\n",
    "# Read the header table\n",
    "header_records =  getRecords(Dataset_path, 'Header')\n",
    "header_dict = {}\n",
    "print('')\n",
    "for r in header_records: \n",
    "    print(r[1:3])\n",
    "    header_dict[r[1]] = r[2]\n",
    "    \n",
    "\n",
    "# Get the table names\n",
    "Data_table = header_dict['dataset.table']\n",
    "FeatureSeq_table = header_dict['feature_sequence.table']\n",
    "Label_table = header_dict['labels.table']\n",
    "print('')\n",
    "print('Data_table:', Data_table)\n",
    "print('Feature_table:', FeatureSeq_table)\n",
    "print('Label_table:', Label_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda678c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the label_to_index dictionary\n",
    "labels_records = getRecords(Dataset_path, Label_table)\n",
    "print('labels_records:', labels_records)\n",
    "\n",
    "Label_to_index = {label:index for (index, label) in labels_records}\n",
    "Index_to_label = {index:label for (index, label) in labels_records}\n",
    "print('Label_to_index:', Label_to_index)\n",
    "print('Index_to_label:', Index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7794757",
   "metadata": {},
   "source": [
    "### Build the Dataset Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10175038",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Dataset item class\n",
    "@dataclass\n",
    "class DataItem:\n",
    "    label: str\n",
    "    label_index: int\n",
    "    predicted_index: int\n",
    "    sequence_len: int\n",
    "    feature_seq: List[List[float]]\n",
    "        \n",
    "    def getItemSummary(self):\n",
    "        return str((self.label, self.label_index, self.predicted_index, self.sequence_len, self.feature_seq.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f275eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the features and data-item records then merge them\n",
    "# Make the sequence_id:feature_sequence map\n",
    "feat_records = getRecords(Dataset_path, FeatureSeq_table)\n",
    "feat_records.sort()    # sort by (seq-id, row-index)\n",
    "seq_rec_map = {}\n",
    "for fitem in feat_records:    # fitem: (seq-id, row-index, col_0, col_1 ...)\n",
    "    seq_id, row_index = fitem[0], fitem[1]\n",
    "    feat_vec = fitem[2:]\n",
    "    if seq_id not in seq_rec_map:\n",
    "        seq_rec_map[seq_id] = []\n",
    "    assert len(seq_rec_map[seq_id]) == row_index, \"EROR: Row index not sorted\"\n",
    "    seq_rec_map[seq_id].append(feat_vec)\n",
    "\n",
    "    \n",
    "# Read the data-items and put them in DataItem array\n",
    "Dataset = []\n",
    "data_records = getRecords(Dataset_path, Data_table)\n",
    "data_schema = getColNames(Dataset_path, Data_table)\n",
    "print('data_schema:', data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for drec in data_records:\n",
    "    seq_len = drec[4]\n",
    "    seq_id = drec[5]\n",
    "    feat_seq = np.array(seq_rec_map[seq_id])\n",
    "    assert len(feat_seq) == seq_len, f\"EROR: Feature Sequence length mismatch, seq_id: {seq_id}\"\n",
    "    label, label_index, pred_index = drec[1:4]   \n",
    "    item = DataItem(label, label_index, pred_index, seq_len, feat_seq)\n",
    "    Dataset.append(item)\n",
    "\n",
    "item = Dataset[0]\n",
    "print('Dataset:', len(Dataset))\n",
    "print('item:', item.getItemSummary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe46f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del table_names, seq_id, seq_len, seq_rec_map, row_index, r, pred_index\n",
    "del label, label_index, labels_records, item, header_records, header_dict\n",
    "del fitem, feat_vec, feat_seq, feat_records\n",
    "del drec, data_schema, data_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4308d318",
   "metadata": {},
   "source": [
    "## Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6081bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and check the model parameters table\n",
    "Model_path = './saved/trained-lstm250-79.68p.s3db'\n",
    "\n",
    "table_names = getTableNames(Model_path)\n",
    "print('table_names:', table_names)\n",
    "\n",
    "# Read the header table\n",
    "header_records =  getRecords(Model_path, 'Header')\n",
    "Header_dict = {}\n",
    "print('')\n",
    "for r in header_records: \n",
    "    print(r[1:3])\n",
    "    Header_dict[r[1]] = r[2]\n",
    "\n",
    "    \n",
    "# Extract model performance summary\n",
    "Accuracy = Header_dict['accuracy']\n",
    "Correct_count = Header_dict['correct_count']\n",
    "Model_perf = f'Model Performance:   accuracy: {Accuracy:.2f}%   correct_count: {Correct_count}'  # to be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50baa561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Hparam table\n",
    "hparam_records = getRecords(Model_path, 'Hparam_T')\n",
    "Hparam = {}\n",
    "for r in hparam_records: \n",
    "    print(r[1:3])\n",
    "    Hparam[r[1]] = r[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a table saved using createMatrixTable() in Export-DB notebook as a list of tuples\n",
    "def readMatrixTable(db_path, table_name):\n",
    "    # read the records\n",
    "    rec_list = getRecords(db_path, table_name)\n",
    "    # build the matrix\n",
    "    rec_list.sort()         # sort by row_no (first column)\n",
    "    matrix = []\n",
    "    for rec in rec_list:\n",
    "        matrix.append(rec[1:])  # stripe off the row_no columns\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Returns the weights and biases as a dictionary\n",
    "def readModelParam(db_path, table_names):\n",
    "    model_params = {}\n",
    "    for name in table_names:\n",
    "        # read the matrix as a list of tuples\n",
    "        mat = readMatrixTable(db_path, name)\n",
    "        # Check if it is a matrix or a vector\n",
    "        if len(mat)==1: is_vector = True\n",
    "        else: is_vector = False\n",
    "        # convert to numpy array\n",
    "        if is_vector: mat = np.array(mat[0])    # make a 1D array for vectors\n",
    "        else: mat = np.array(mat)\n",
    "        # save it for returning\n",
    "        model_params[name] = mat\n",
    "    return model_params\n",
    "        \n",
    "\n",
    "# Get the parameter table names\n",
    "param_tables = [\n",
    "    'lstm_weight_ih_l0',\n",
    "    'lstm_weight_hh_l0',\n",
    "    'lstm_bias_ih_l0',\n",
    "    'lstm_bias_hh_l0',\n",
    "    'lstm_weight_ih_l1',\n",
    "    'lstm_weight_hh_l1',\n",
    "    'lstm_bias_ih_l1',\n",
    "    'lstm_bias_hh_l1',\n",
    "    'lstm_weight_ih_l2',\n",
    "    'lstm_weight_hh_l2',\n",
    "    'lstm_bias_ih_l2',\n",
    "    'lstm_bias_hh_l2',\n",
    "    'fc_weight',\n",
    "    'fc_bias',\n",
    "]\n",
    "\n",
    "model_params = readModelParam(Model_path, param_tables)\n",
    "for k, v in model_params.items():\n",
    "    print(f'{k:20}:', v.shape, v.dtype)\n",
    "    \n",
    "\n",
    "# Delete names\n",
    "del k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters class\n",
    "@dataclass\n",
    "class lstm250_Params:\n",
    "    lstm_weight_ih_l0: np.ndarray\n",
    "    lstm_weight_hh_l0: np.ndarray\n",
    "    lstm_bias_ih_l0: np.ndarray\n",
    "    lstm_bias_hh_l0: np.ndarray\n",
    "    lstm_weight_ih_l1: np.ndarray\n",
    "    lstm_weight_hh_l1: np.ndarray\n",
    "    lstm_bias_ih_l1: np.ndarray\n",
    "    lstm_bias_hh_l1: np.ndarray\n",
    "    lstm_weight_ih_l2: np.ndarray\n",
    "    lstm_weight_hh_l2: np.ndarray\n",
    "    lstm_bias_ih_l2: np.ndarray\n",
    "    lstm_bias_hh_l2: np.ndarray\n",
    "    fc_weight: np.ndarray\n",
    "    fc_bias: np.ndarray\n",
    "\n",
    "\n",
    "# Instantiate the model parameter class with float32 datatype\n",
    "Model_params = lstm250_Params(\n",
    "    model_params['lstm_weight_ih_l0'].astype(np.float32),\n",
    "    model_params['lstm_weight_hh_l0'].astype(np.float32),\n",
    "    model_params['lstm_bias_ih_l0'].astype(np.float32),\n",
    "    model_params['lstm_bias_hh_l0'].astype(np.float32),\n",
    "    model_params['lstm_weight_ih_l1'].astype(np.float32),\n",
    "    model_params['lstm_weight_hh_l1'].astype(np.float32),\n",
    "    model_params['lstm_bias_ih_l1'].astype(np.float32),\n",
    "    model_params['lstm_bias_hh_l1'].astype(np.float32),\n",
    "    model_params['lstm_weight_ih_l2'].astype(np.float32),\n",
    "    model_params['lstm_weight_hh_l2'].astype(np.float32),\n",
    "    model_params['lstm_bias_ih_l2'].astype(np.float32),\n",
    "    model_params['lstm_bias_hh_l2'].astype(np.float32),\n",
    "    model_params['fc_weight'].astype(np.float32),\n",
    "    model_params['fc_bias'].astype(np.float32),\n",
    ")\n",
    "\n",
    "\n",
    "# Show the parameter info\n",
    "for field in dataclasses.fields(Model_params):\n",
    "    field_value = getattr(Model_params, field.name)\n",
    "    print(f'{field.name:18}:', field_value.shape, field_value.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6e59ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "del table_names, r, param_tables, model_params, header_records\n",
    "del field, field_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f486db3",
   "metadata": {},
   "source": [
    "# Verify Model on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6088f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid activation in numpy\n",
    "def npSigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Defining the LSTM cell\n",
    "# weight_ih: weights for input x\n",
    "# weight_hh: weights for hidden state h_prev\n",
    "def npLSTMCell(x, h_prev, c_prev, weight_ih, weight_hh, bias_ih, bias_hh, input_size, hidden_size):\n",
    "    gates = weight_ih @ x  +  weight_hh @ h_prev  +  bias_ih + bias_hh\n",
    "\n",
    "    i = npSigmoid(gates[:hidden_size])\n",
    "    f = npSigmoid(gates[hidden_size:2*hidden_size])\n",
    "    g = np.tanh(gates[2*hidden_size:3*hidden_size])   # equivalent of ~Ct in above figure\n",
    "    o = npSigmoid(gates[3*hidden_size:])             # equivalent of Ot in above figure\n",
    "    c = f * c_prev + i * g   # Ct\n",
    "    h = o * np.tanh(c)    # Ht\n",
    "    return h, c\n",
    "\n",
    "\n",
    "# Implements 3 layers of LSTM\n",
    "def npLSTM3(x, h_prev, c_prev, lstm_weights, input_size, hidden_size):\n",
    "    # Perform compatability checks\n",
    "    layer_count=3\n",
    "    assert len(h_prev) == layer_count, \"You need to provide initial values for all layers\"\n",
    "    assert len(c_prev) == layer_count, \"You need to provide initial values for all layers\"\n",
    "    assert len(x) == input_size, \"Input size mismatch\"\n",
    "    # Layer-1\n",
    "    h0_cur, c0_cur = npLSTMCell(x, h_prev[0], c_prev[0], \n",
    "                                lstm_weights.lstm_weight_ih_l0, lstm_weights.lstm_weight_hh_l0,\n",
    "                                lstm_weights.lstm_bias_ih_l0, lstm_weights.lstm_bias_hh_l0,\n",
    "                                input_size, hidden_size)\n",
    "    # Layer-2\n",
    "    h1_cur, c1_cur = npLSTMCell(h0_cur, h_prev[1], c_prev[1], \n",
    "                                lstm_weights.lstm_weight_ih_l1, lstm_weights.lstm_weight_hh_l1,\n",
    "                                lstm_weights.lstm_bias_ih_l1, lstm_weights.lstm_bias_hh_l1,\n",
    "                                hidden_size, hidden_size)\n",
    "    # Layer-3\n",
    "    h2_cur, c2_cur = npLSTMCell(h1_cur, h_prev[2], c_prev[2], \n",
    "                                lstm_weights.lstm_weight_ih_l2, lstm_weights.lstm_weight_hh_l2,\n",
    "                                lstm_weights.lstm_bias_ih_l2, lstm_weights.lstm_bias_hh_l2,\n",
    "                                hidden_size, hidden_size)\n",
    "    return (h0_cur, h1_cur, h2_cur), (c0_cur, c1_cur, c2_cur)\n",
    "    \n",
    "    \n",
    "\n",
    "# Implementation of ully connected layer \n",
    "def npFClayer(x, weight, bias):\n",
    "    return weight @ x + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28038c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Model using numpy\n",
    "def npModel(feat_seq):\n",
    "    # Initial states\n",
    "    h0_3 = np.zeros((3, Hparam['hidden_size']))\n",
    "    c0_3 = np.zeros((3, Hparam['hidden_size']))\n",
    "\n",
    "    # Pass sequence through the LSTM cell\n",
    "    h_prev, c_prev = h0_3, c0_3\n",
    "    for tok in feat_seq:\n",
    "        ht, ct = npLSTM3(tok, h_prev, c_prev, Model_params, Hparam['input_size'], Hparam['hidden_size'])\n",
    "        h_prev, c_prev = ht, ct\n",
    "    last_hidden = ht[2]\n",
    "    out = npFClayer(last_hidden, Model_params.fc_weight, Model_params.fc_bias)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Select an item to run through the numpy model for validation\n",
    "item = Dataset[0]\n",
    "out_np = npModel(item.feature_seq)\n",
    "print('out_np:', out_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535588f3",
   "metadata": {},
   "source": [
    "## Validate the Model on the Entire Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e341013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a numpy array, returns the index of the maximum value\n",
    "def get_likely_index_np(nparray):\n",
    "    return nparray.argmax()\n",
    "\n",
    "\n",
    "# Driver for manual numpy based model\n",
    "def predict_npModel(item):\n",
    "    # Use the model to predict the label of the waveform\n",
    "    output = npModel(item.feature_seq)\n",
    "    pred = get_likely_index_np(output)   # indexing to get the prediction from batch\n",
    "    return pred\n",
    "\n",
    "\n",
    "# item: (label, label_index, sequence_length, feature_length, predicted_index, feature_seqeunce)\n",
    "item = Dataset[120]\n",
    "print(item.getItemSummary())\n",
    "\n",
    "pred = predict_npModel(item)\n",
    "print('pred:', pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8e96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "run_validation = False\n",
    "\n",
    "# Validate the Given model on the whole dataset\n",
    "# ptmodel: set it to True for the PyTorch model\n",
    "def validateModel(predict_fn):\n",
    "    dataset = Dataset\n",
    "    expect_miss = 0      # keeps track of no. of mismatche between prediction in dataset vs model prediction\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for item in tqdm(dataset):\n",
    "        pred_index = item.predicted_index\n",
    "        lbl_index  = item.label_index\n",
    "        pred = predict_fn(item)\n",
    "        if pred != pred_index: expect_miss += 1    # prediction does not match prediction in dataset\n",
    "        if pred == lbl_index: correct_count += 1   # prediction matched the actual label-index\n",
    "        total_count += 1\n",
    "    # Compute and print statistics\n",
    "    accuracy = (100.0 * correct_count) / total_count\n",
    "    print(f'Validation accuracy: {accuracy:.2f}%   correct_count: {correct_count}   expected-miss: {expect_miss}   total_count: {total_count}')\n",
    "    return accuracy, correct_count, expect_miss, total_count\n",
    "\n",
    "\n",
    "# Run on entire test-dataset\n",
    "if run_validation:\n",
    "    accuracy, correct_count, expect_miss, total_count = validateModel(predict_npModel)\n",
    "    print('Expected', Model_perf)\n",
    "    del accuracy, correct_count, expect_miss, total_count\n",
    "else:\n",
    "    print(\"INFO: Not running validation here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facde434",
   "metadata": {},
   "source": [
    "# Define Fixed-Point Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the cache before importing\n",
    "!rm -rf __pycache__/\n",
    "from AK_FixedPoint import *\n",
    "\n",
    "# Run Unit tests to make sure everything is okay\n",
    "!python3 unittest_fxp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5ab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run unit tests to make sure everything is okay\n",
    "!python3 unittest_fxp_math.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae6f9b",
   "metadata": {},
   "source": [
    "## Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a06dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf as INF\n",
    "\n",
    "\n",
    "# Performs matrix-vector multiplication and keeps track of error.\n",
    "# status_obj: instance of fxp_Status to get status back\n",
    "# Returns the output vector and a tupel with intermediate results for debugging: resutl, (...)\n",
    "def fxp_matmul_mv(fxp_mat, fxp_vec, status_obj=None, debug=False):\n",
    "    # Make sure all assumptions are met\n",
    "    assert len(fxp_mat._data.shape) == 2, \"fxp_mat must be built from a 2D Numpy array\"\n",
    "    assert len(fxp_vec._data.shape) == 1, \"fxp_mat must be built from a 1D Numpy array\"\n",
    "    assert fxp_mat._data.shape[1] == fxp_vec._data.shape[0], \"Matrix column count not equal vector length\"\n",
    "    \n",
    "    # Get the data-type parameters\n",
    "    t_width = fxp_vec._total_width\n",
    "    f_width = fxp_vec._frac_width\n",
    "    compute_status = True if status_obj != None else False\n",
    "    \n",
    "    # multiply row-wise\n",
    "    prod_np = (fxp_mat._data * fxp_vec._data)   # multiplying raw values\n",
    "    # compute error status for multiplying into 2x wider result (less likely to have errors in this step)\n",
    "    fxp_prod = fxp_makeWider(fxp_mat, 2)  # build 2x wider fxp object\n",
    "    fxp_prod._data = prod_np              # copy the raw product values\n",
    "    prod_stat = fxp_fitData(fxp_prod, compute_status)   # now fit within this precision\n",
    "    if compute_status: \n",
    "        if debug: print('prod_stat:', prod_stat)\n",
    "        fxp_accumulateStatus(status_obj, prod_stat)  # record the multiplication errors\n",
    "        \n",
    "    # Now scale down to original precision before accumulation; record error status\n",
    "    prod_np_down = prod_np >> f_width       # discard lower fraction bits\n",
    "    fxp_prod_down = fxp_makeSame(fxp_mat)   # fxp object with original precision\n",
    "    fxp_prod_down._data = prod_np_down\n",
    "    prod_down_stat = fxp_fitData(fxp_prod_down, compute_status)\n",
    "    if compute_status: \n",
    "        if debug: print('prod_down_stat:', prod_down_stat)\n",
    "        fxp_accumulateStatus(status_obj, prod_down_stat)  # accumulate the scaling errors\n",
    "        \n",
    "    # accumulate along rows; record error status\n",
    "    accum_np = np.sum(fxp_prod_down._data, axis=1)\n",
    "    fxp_accum = fxp_makeSame(fxp_vec)\n",
    "    fxp_accum._data = accum_np\n",
    "    accum_stat = fxp_fitData(fxp_accum, compute_status)\n",
    "    if compute_status: \n",
    "        if debug: print('accum_stat:', accum_stat)\n",
    "        fxp_accumulateStatus(status_obj, accum_stat)  # accumulate the scaling errors\n",
    "    return fxp_accum, (prod_np, fxp_prod, prod_np_down, fxp_prod_down, accum_np, fxp_accum)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531574c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "mat_inp = [\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 5, 7, 2],\n",
    "    [9, 3, 5, 0],\n",
    "]\n",
    "vec_inp = [4, 8, 1, 2]\n",
    "mat_np = np.array(mat_inp)\n",
    "vec_np = np.array(vec_inp)\n",
    "res_np = mat_np @ vec_np\n",
    "print(res_np)\n",
    "\n",
    "total_width = 10\n",
    "frac_width = 4\n",
    "stat = fxp_Status(False, 0, -INF, INF, -INF, INF)\n",
    "fxp_mat_inp, _ = fxp_ctor(total_width, frac_width, mat_np)\n",
    "fxp_vec_inp, _ = fxp_ctor(total_width, frac_width, vec_np)\n",
    "fxp_result, dbg = fxp_matmul_mv(fxp_mat_inp, fxp_vec_inp, stat, debug=True)\n",
    "\n",
    "\n",
    "print('Overall status:', stat)\n",
    "\n",
    "print('')\n",
    "fxp_printInfo(fxp_result)\n",
    "fxp_printValue(fxp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc95b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the intermediate results\n",
    "prod_np, fxp_prod, prod_np_down, fxp_prod_down, accum_np, fxp_accum = dbg\n",
    "print(mat_np * vec_np)\n",
    "print('')\n",
    "\n",
    "print(prod_np >> (2*frac_width))\n",
    "\n",
    "print('')\n",
    "fxp_printInfo(fxp_prod)\n",
    "fxp_printValue(fxp_prod)\n",
    "\n",
    "print('')\n",
    "fxp_printInfo(fxp_prod_down)\n",
    "fxp_printValue(fxp_prod_down)\n",
    "\n",
    "\n",
    "print('')\n",
    "fxp_printInfo(fxp_accum)\n",
    "fxp_printValue(fxp_accum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3de4ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del vec_np, vec_inp, total_width, stat, res_np, prod_np, prod_np_down, pred, out_np\n",
    "del mat_np, mat_inp, item, hparam_records, fxp_vec_inp, fxp_prod, fxp_prod_down, fxp_mat_inp\n",
    "del fxp_accum, frac_width, dbg, accum_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b11ff1",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most accurate fxp-sigmoid implementation for the given precision\n",
    "def fxpSigmoid_accurate(fxp_num):\n",
    "    x = fxp_getAsFloat(fxp_num)    # convert to float\n",
    "    fl_sig = npSigmoid(x)          # compute sigmoid in float\n",
    "    # Convert to fixed point with original precision and return\n",
    "    fxp_sig, stat = fxp_ctor(fxp_num._total_width, fxp_num._frac_width, fl_sig)\n",
    "    return fxp_sig, stat\n",
    "\n",
    "\n",
    "# Test sigmoid activation\n",
    "inp_vec = [0, 0.1, 0.2, 0.3, 1, 2, 3]\n",
    "\n",
    "inp_vec_np = np.array(inp_vec)\n",
    "out_vec_np = npSigmoid(inp_vec_np)\n",
    "print('out_vec_np:', out_vec_np)\n",
    "\n",
    "fxp_inp, _ = fxp_ctor(30, 15, inp_vec)\n",
    "fxp_sig, _ = fxpSigmoid_accurate(fxp_inp)\n",
    "print('fxp_sig:', fxp_getAsFloat(fxp_sig))\n",
    "\n",
    "\n",
    "# assertion test\n",
    "tolerance = 1e-4\n",
    "diff = np.abs(out_vec_np - fxp_getAsFloat(fxp_sig))\n",
    "if (diff > tolerance).any(): assert 0, f\"EROR: Sigmoid mismatch {diff}\"\n",
    "    \n",
    "\n",
    "# Delete names\n",
    "del inp_vec, inp_vec_np, out_vec_np, fxp_inp, fxp_sig, tolerance, diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The most accurate fxp-sigmoid implementation for the given precision\n",
    "def fxpTanh_accurate(fxp_num):\n",
    "    x = fxp_getAsFloat(fxp_num)    # convert to float\n",
    "    fl_tanh = np.tanh(x)            # compute sigmoid in float\n",
    "    # Convert to fixed point with original precision and return\n",
    "    fxp_tanh, stat = fxp_ctor(fxp_num._total_width, fxp_num._frac_width, fl_tanh)\n",
    "    return fxp_tanh, stat\n",
    "\n",
    "\n",
    "# Test sigmoid activation\n",
    "inp_vec = [0, 0.1, -0.1, 0.2, -0.2, 0.3, -0.3, 1, 2, 3, -1, -2, -3]\n",
    "\n",
    "inp_vec_np = np.array(inp_vec)\n",
    "out_vec_np = np.tanh(inp_vec_np)\n",
    "print('out_vec_np:', out_vec_np)\n",
    "\n",
    "fxp_inp,  _ = fxp_ctor(30, 15, inp_vec)\n",
    "fxp_tanh, _ = fxpTanh_accurate(fxp_inp)\n",
    "print('fxp_tanh:', fxp_getAsFloat(fxp_tanh))\n",
    "\n",
    "\n",
    "# assertion test\n",
    "tolerance = 1e-4\n",
    "diff = np.abs(out_vec_np - fxp_getAsFloat(fxp_tanh))\n",
    "if (diff > tolerance).any(): assert 0, f\"EROR: Tanh mismatch {diff}\"\n",
    "    \n",
    "\n",
    "# Delete names\n",
    "del inp_vec, inp_vec_np, out_vec_np, fxp_inp, fxp_tanh, tolerance, diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da5497",
   "metadata": {},
   "source": [
    "# Implement Model in Fixed-Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81629816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict\n",
    "\n",
    "\n",
    "# Convert Model Parameters to fixed point\n",
    "Fxp_total_width = 30\n",
    "Fxp_frac_width = 25\n",
    "\n",
    "\n",
    "# Given an instance of lstm250_Params, convert the parameters into fixed point numbers\n",
    "# params: instance of lstm250_Params\n",
    "def convertParamsFxp(params, total_width, frac_width):\n",
    "    param_dict = asdict(params)\n",
    "    fxp_param_dict = {}\n",
    "    for pname, value in param_dict.items():\n",
    "        fxp_value, stat = fxp_ctor(total_width, frac_width, value)\n",
    "        if stat.overflow: f\"WARN: Overflow of {pname}, count: {stat.overflow_count}\"\n",
    "        fxp_param_dict[pname] = fxp_value\n",
    "    fxp_params = lstm250_Params(**fxp_param_dict)\n",
    "    return fxp_params\n",
    "\n",
    "\n",
    "Fxp_model_param = convertParamsFxp(Model_params, Fxp_total_width, Fxp_frac_width)\n",
    "fxp_printInfo(Fxp_model_param.fc_weight)\n",
    "Fxp_model_param.fc_bias._data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b1af8",
   "metadata": {},
   "source": [
    "## Model Definition in Fixed-Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107692bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the LSTM cell\n",
    "# weight_ih: weights for input x\n",
    "# weight_hh: weights for hidden state h_prev\n",
    "def fxp_LSTMCell(x, h_prev, c_prev, weight_ih, weight_hh, bias_ih, bias_hh, input_size, hidden_size):\n",
    "    # Compute gates vector\n",
    "    # gates = weight_ih @ x  +  weight_hh @ h_prev  +  bias_ih + bias_hh\n",
    "    x1, _ = fxp_matmul_mv(weight_ih, x)\n",
    "    x2, _ = fxp_matmul_mv(weight_hh, h_prev)\n",
    "    x3    = fxp_add(x1, x2)\n",
    "    x4    = fxp_add(bias_ih, bias_hh)\n",
    "    gates = fxp_add(x3, x4)\n",
    "    \n",
    "    # Separate the gates\n",
    "    ig = fxp_copy(gates)\n",
    "    fg = fxp_copy(gates)\n",
    "    gg = fxp_copy(gates)\n",
    "    og = fxp_copy(gates)\n",
    "    ig._data = ig._data[:hidden_size]\n",
    "    fg._data = fg._data[hidden_size:2*hidden_size]\n",
    "    gg._data = gg._data[2*hidden_size:3*hidden_size]\n",
    "    og._data = og._data[3*hidden_size:]\n",
    "\n",
    "    # Compute gate outputs\n",
    "    i, _ = fxpSigmoid_accurate(ig)  # npSigmoid(gates[:hidden_size])\n",
    "    f, _ = fxpSigmoid_accurate(fg)  # npSigmoid(gates[hidden_size:2*hidden_size])\n",
    "    g, _ = fxpTanh_accurate(gg)     # np.tanh(gates[2*hidden_size:3*hidden_size])   # equivalent of ~Ct in above figure\n",
    "    o, _ = fxpSigmoid_accurate(og)  # npSigmoid(gates[3*hidden_size:])             # equivalent of Ot in above figure\n",
    "    \n",
    "    # compute new states\n",
    "    # c = f * c_prev + i * g   # Ct\n",
    "    x1 = fxp_mult(f, c_prev)\n",
    "    x2 = fxp_mult(i, g)\n",
    "    c = fxp_add(x1, x2)\n",
    "    # h = o * np.tanh(c)    # Ht\n",
    "    x1, _ = fxpTanh_accurate(c)\n",
    "    h = fxp_mult(o, x1)\n",
    "    return h, c\n",
    "\n",
    "\n",
    "\n",
    "# Implements 3 layers of LSTM\n",
    "def fxp_LSTM3(x, h_prev, c_prev, lstm_weights, input_size, hidden_size):\n",
    "    # Perform compatability checks\n",
    "    layer_count=3\n",
    "    assert len(h_prev) == layer_count, \"You need to provide initial values for all layers\"\n",
    "    assert len(c_prev) == layer_count, \"You need to provide initial values for all layers\"\n",
    "    assert len(x._data) == input_size, \"Input size mismatch\"\n",
    "    # Layer-1\n",
    "    h0_cur, c0_cur = fxp_LSTMCell(x, h_prev[0], c_prev[0], \n",
    "                                lstm_weights.lstm_weight_ih_l0, lstm_weights.lstm_weight_hh_l0,\n",
    "                                lstm_weights.lstm_bias_ih_l0, lstm_weights.lstm_bias_hh_l0,\n",
    "                                input_size, hidden_size)\n",
    "    # Layer-2\n",
    "    h1_cur, c1_cur = fxp_LSTMCell(h0_cur, h_prev[1], c_prev[1], \n",
    "                                lstm_weights.lstm_weight_ih_l1, lstm_weights.lstm_weight_hh_l1,\n",
    "                                lstm_weights.lstm_bias_ih_l1, lstm_weights.lstm_bias_hh_l1,\n",
    "                                hidden_size, hidden_size)\n",
    "    # Layer-3\n",
    "    h2_cur, c2_cur = fxp_LSTMCell(h1_cur, h_prev[2], c_prev[2], \n",
    "                                lstm_weights.lstm_weight_ih_l2, lstm_weights.lstm_weight_hh_l2,\n",
    "                                lstm_weights.lstm_bias_ih_l2, lstm_weights.lstm_bias_hh_l2,\n",
    "                                hidden_size, hidden_size)\n",
    "    return (h0_cur, h1_cur, h2_cur), (c0_cur, c1_cur, c2_cur)\n",
    "    \n",
    "    \n",
    "\n",
    "# Implementation of ully connected layer \n",
    "def fxp_FClayer(x, weight, bias):\n",
    "    x1, _ = fxp_matmul_mv(weight, x)\n",
    "    x2 = fxp_add(x1, bias)\n",
    "    return x2\n",
    "\n",
    "\n",
    "\n",
    "def lstm250_forward_fxp(params, feat_seq, debug=False):\n",
    "    # Initial states\n",
    "    h0_3 = np.zeros((3, Hparam['hidden_size']))\n",
    "    c0_3 = np.zeros((3, Hparam['hidden_size']))\n",
    "    # convert to fixed-point\n",
    "    h0_3 = [fxp_ctor(Fxp_total_width, Fxp_frac_width, h0)[0]  for h0 in h0_3]\n",
    "    c0_3 = [fxp_ctor(Fxp_total_width, Fxp_frac_width, c0)[0]  for c0 in c0_3]\n",
    "\n",
    "    # Pass sequence through the LSTM cell\n",
    "    h_prev, c_prev = h0_3, c0_3\n",
    "    for tok in feat_seq:\n",
    "        ht, ct = fxp_LSTM3(tok, h_prev, c_prev, Fxp_model_param, Hparam['input_size'], Hparam['hidden_size'])\n",
    "        h_prev, c_prev = ht, ct\n",
    "    last_hidden = ht[2]\n",
    "    out = fxp_FClayer(last_hidden, Fxp_model_param.fc_weight, Fxp_model_param.fc_bias)\n",
    "    return out\n",
    "\n",
    "\n",
    "# Uses the forward pass and converts the result into predicted_index\n",
    "def lstm150_predict_fxp(params, feat_seq_fxp, debug=False):\n",
    "    out_vec = lstm250_forward_fxp(Fxp_model_param, feat_seq_fxp, debug=debug)\n",
    "    return np.argmax(out_vec._data)   # return the index of the highest probable class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82713d51",
   "metadata": {},
   "source": [
    "### Unit Tests on Model Parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test01_fxp_LSTMCell():\n",
    "    print('\\n---- test01_fxp_LSTMCell ----')\n",
    "    # Initial states\n",
    "    h0 = np.zeros(Hparam['hidden_size'])\n",
    "    c0 = np.zeros(Hparam['hidden_size'])\n",
    "    item = Dataset[0]\n",
    "    # Compute floating point results\n",
    "    x = item.feature_seq[0]\n",
    "    h, c = npLSTMCell(x, h0, c0, Model_params.lstm_weight_ih_l0, \n",
    "                                 Model_params.lstm_weight_hh_l0, \n",
    "                                 Model_params.lstm_bias_ih_l0,\n",
    "                                 Model_params.lstm_bias_hh_l0,\n",
    "                                 Hparam['input_size'],\n",
    "                                 Hparam['hidden_size'])   \n",
    "    # Compute fixed point results\n",
    "    twidth, fwidth = Fxp_total_width, Fxp_frac_width\n",
    "    h0, _ = fxp_ctor(twidth, fwidth, h0)\n",
    "    c0, _ = fxp_ctor(twidth, fwidth, c0)\n",
    "    x, _  = fxp_ctor(twidth, fwidth, item.feature_seq[0])\n",
    "    hx, cx = fxp_LSTMCell(x, h0, c0, Fxp_model_param.lstm_weight_ih_l0, \n",
    "                                     Fxp_model_param.lstm_weight_hh_l0, \n",
    "                                     Fxp_model_param.lstm_bias_ih_l0,\n",
    "                                     Fxp_model_param.lstm_bias_hh_l0,\n",
    "                                     Hparam['input_size'],\n",
    "                                     Hparam['hidden_size'])  \n",
    "    # Compare\n",
    "    tolerance = 1e-5\n",
    "    diff_h = np.abs(h - fxp_getAsFloat(hx))\n",
    "    print('diff_h min, max:', np.min(diff_h), np.max(diff_h))\n",
    "    if (diff_h > tolerance).any(): assert 0, f\"EROR: diff_h.max: {np.max(diff_h)}\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def test02_fxp_FClayer():\n",
    "    print('\\n---- test02_fxp_FClayer ----')\n",
    "    item = Dataset[0]\n",
    "    x = item.feature_seq[0]\n",
    "    wt = Model_params.lstm_weight_ih_l0\n",
    "    bs = Model_params.lstm_bias_ih_l0\n",
    "    \n",
    "    # compute floating-point result\n",
    "    fc_fl = npFClayer(x, wt, bs)\n",
    "    \n",
    "    # compute fixed-point result\n",
    "    x , _ = fxp_ctor(Fxp_total_width, Fxp_frac_width, x)\n",
    "    wt, _ = fxp_ctor(Fxp_total_width, Fxp_frac_width, wt)\n",
    "    bs, _ = fxp_ctor(Fxp_total_width, Fxp_frac_width, bs)\n",
    "    fc_fxp = fxp_FClayer(x, wt, bs)\n",
    "    \n",
    "    # Compare\n",
    "    tolerance = 1e-5\n",
    "    diff_fc = np.abs(fc_fl - fxp_getAsFloat(fc_fxp))\n",
    "    print('diff_fc min, max:', np.min(diff_fc), np.max(diff_fc))\n",
    "    if (diff_fc > tolerance).any(): assert 0, f\"EROR: diff_fc.max: {np.max(diff_h)}\"\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "def test03_fxp_LSTM3():\n",
    "    print('\\n---- test03_fxp_LSTM3 ----')\n",
    "    h0_3 = np.zeros((3, Hparam['hidden_size']))\n",
    "    c0_3 = np.zeros((3, Hparam['hidden_size']))\n",
    "    x = Dataset[2].feature_seq[0]\n",
    "    # compute floating point result\n",
    "    h3, c3 = npLSTM3(x, h0_3, c0_3, Model_params, Hparam['input_size'], Hparam['hidden_size'])\n",
    "    all_fl = h3 + c3\n",
    "    \n",
    "    # Compute fixed point result\n",
    "    x, _ = fxp_ctor(Fxp_total_width, Fxp_frac_width, x)\n",
    "    h0_3 = [fxp_ctor(Fxp_total_width, Fxp_frac_width, h0)[0]  for h0 in h0_3]\n",
    "    c0_3 = [fxp_ctor(Fxp_total_width, Fxp_frac_width, c0)[0]  for c0 in c0_3]\n",
    "    h3x, c3x = fxp_LSTM3(x, h0_3, c0_3, Fxp_model_param, Hparam['input_size'], Hparam['hidden_size'])\n",
    "    all_fxp = h3x + c3x\n",
    "    \n",
    "    # Compare\n",
    "    tolerance = 1e-5\n",
    "    for i in range(len(all_fl)):\n",
    "        diff = np.abs(all_fl[i] - fxp_getAsFloat(all_fxp[i]))    \n",
    "        print(f'diff[{i}] min, max:', np.min(diff), np.max(diff))\n",
    "        if (diff > tolerance).any(): assert 0, f\"EROR: diff[{i}].max: {np.max(diff)}\"\n",
    "    \n",
    "\n",
    "def test04_fxp_lstm250_forward():\n",
    "    print('\\n---- test04_fxp_lstm250_forward ----')\n",
    "    item = Dataset[0]\n",
    "    feat_seq = item.feature_seq[:15]\n",
    "    \n",
    "    # Compute floating point result\n",
    "    out_fl = npModel(feat_seq)\n",
    "    \n",
    "    # Compute fixed-point result\n",
    "    feat_seq_fxp = [fxp_ctor(Fxp_total_width, Fxp_frac_width, feat_vec)[0] for feat_vec in feat_seq]\n",
    "    out_fxp = lstm250_forward_fxp(Fxp_model_param, feat_seq_fxp)\n",
    "    \n",
    "    # Compare\n",
    "    tolerance = 1e-4\n",
    "    assert out_fl.shape == out_fxp._data.shape, \"EROR: Output shape mismatch\"\n",
    "    diff = np.abs(out_fl - fxp_getAsFloat(out_fxp))\n",
    "    print('diff min, max:', np.min(diff), np.max(diff))\n",
    "    if (diff > tolerance).any(): assert 0, f\"EROR: diff.max: {np.max(diff)}\"\n",
    "\n",
    "        \n",
    "def test05_fxp_lstm250_predict():\n",
    "    print('\\n---- test05_fxp_lstm250_predict ----')\n",
    "    item = Dataset[0]\n",
    "    pred_fl = predict_npModel(item)\n",
    "    print('pred_fl:', pred_fl)\n",
    "    \n",
    "    # Compute fixed point prediction\n",
    "    feat_seq = item.feature_seq\n",
    "    feat_seq_fxp = [fxp_ctor(Fxp_total_width, Fxp_frac_width, feat_vec)[0] for feat_vec in feat_seq]\n",
    "    pred_fxp = lstm150_predict_fxp(Fxp_model_param, feat_seq_fxp)\n",
    "    print('pred_fxp:', pred_fxp)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Run tests\n",
    "test01_fxp_LSTMCell()\n",
    "test02_fxp_FClayer()\n",
    "test03_fxp_LSTM3()\n",
    "test04_fxp_lstm250_forward()\n",
    "test05_fxp_lstm250_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c05104",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
