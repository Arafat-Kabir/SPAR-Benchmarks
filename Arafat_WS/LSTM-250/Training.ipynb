{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5468b1",
   "metadata": {},
   "source": [
    "# LSTM-250 For Phoneme Recognition on TIMIT\n",
    "\n",
    "**Architecture: mfcc-LSTM:250-LSTM:250-LSTM:250-FC:39**  \n",
    "So, basically a 3 layered LSTM with 250 hidden units/state-vars.\n",
    "\n",
    "Here, \n",
    "- LSTM: LSTM layer with given number of hidden units (state)\n",
    "- FC: Fully connected layer for output mapping (no activation in the model)\n",
    "\n",
    "**Naming Conventions**\n",
    "- Small letter variables are local to a section\n",
    "- Capitalized variables are used across sections\n",
    "- Functions are always assumed to be used across sections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6f5db",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006691f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aec781",
   "metadata": {},
   "source": [
    "# Loading Dataset\n",
    "\n",
    "The DARPA TIMIT dataset can be downloaded from \n",
    "[here](https://www.kaggle.com/datasets/mfekadu/darpa-timit-acousticphonetic-continuous-speech).\n",
    "After downloading it, simply unzip it into `./dataset/TIMIT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94134473",
   "metadata": {},
   "outputs": [],
   "source": [
    "Timit_path = \"./dataset/TIMIT/\"\n",
    "Data_path = \"./dataset/TIMIT/data/\"\n",
    "df_train = pd.read_csv(os.path.join(Timit_path, 'train_data.csv'))\n",
    "df_test = pd.read_csv(os.path.join(Timit_path, 'test_data.csv'))\n",
    "DF_dataset = pd.concat([df_train, df_test])\n",
    "DF_dataset = DF_dataset[DF_dataset['is_converted_audio'] == False]\n",
    "DF_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b657d4",
   "metadata": {},
   "source": [
    "# Split Dataset\n",
    "\n",
    "## Merge Entries\n",
    "One entry in the dataframe represents one file. Audio, word, and phonetics are stored separately in different files. Then, we would need to combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb6d6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_ds = {}\n",
    "\n",
    "for idx, row in tqdm(DF_dataset.iterrows(), total=len(DF_dataset)):\n",
    "    path = row['path_from_data_dir']\n",
    "    entry_id = path.split('.')[0]\n",
    "\n",
    "    if entry_id not in merged_ds:\n",
    "        merged_ds[entry_id] = {}\n",
    "\n",
    "    if row['is_audio'] is True:\n",
    "        merged_ds[entry_id]['audio_file'] = os.path.join(Data_path, path)\n",
    "    elif row['is_word_file'] is True:\n",
    "        merged_ds[entry_id]['word_file'] = os.path.join(Data_path, path)\n",
    "    elif row['is_phonetic_file'] is True:\n",
    "        merged_ds[entry_id]['phonetic_file'] = os.path.join(Data_path, path)\n",
    "    elif row['is_sentence_file'] is True:\n",
    "        merged_ds[entry_id]['sentence_file'] = os.path.join(Data_path, path)\n",
    "    \n",
    "    if row['test_or_train'].strip()=='TEST':\n",
    "        merged_ds[entry_id]['is_test'] = True\n",
    "    else:\n",
    "        merged_ds[entry_id]['is_test'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9744c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data fields\n",
    "records = list(merged_ds.keys())\n",
    "print('Record Keys:', records[:3])\n",
    "\n",
    "fields = list(merged_ds[records[0]].keys())\n",
    "print('Fields:', fields)\n",
    "\n",
    "print('\\nField values')\n",
    "for k,v in merged_ds[records[0]].items():\n",
    "    print(f'{k:>15}',':',v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698a4eb",
   "metadata": {},
   "source": [
    "## Split into Train and Test Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2088c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the keys\n",
    "train_keys = []\n",
    "test_keys = []\n",
    "for key in merged_ds:\n",
    "    if merged_ds[key]['is_test']: test_keys.append(key)\n",
    "    else: train_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81a4bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset based on the separated keys\n",
    "Train_dataset = { key:merged_ds[key] for key in train_keys }\n",
    "Test_dataset  = { key:merged_ds[key] for key in test_keys }\n",
    "train_len = len(Train_dataset)\n",
    "test_len = len(Test_dataset)\n",
    "train_ratio = train_len*100/(train_len+test_len)\n",
    "test_ratio = test_len*100/(train_len+test_len)\n",
    "print('Train_dataset#:', train_len, '\\nTest_dataset#:', test_len)\n",
    "print('Ratio: Train-Test', f'{train_ratio:.1f}%-{test_ratio:.1f}%' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164b1acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_durations(dict_data):\n",
    "    total_durations = 0\n",
    "    for entry in tqdm(dict_data.values()):\n",
    "        audio_data, rate = librosa.load(entry['audio_file'], sr=None)\n",
    "        duration = len(audio_data) / rate\n",
    "        total_durations += duration\n",
    "    return int(total_durations)\n",
    "\n",
    "\n",
    "print(f\"Duration of Train: {get_durations(Train_dataset) // 60} mins\")\n",
    "print(f\"Duration of Test : {get_durations(Test_dataset) // 60} mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aa73a9",
   "metadata": {},
   "source": [
    "# Explore Dataset to Understand\n",
    "\n",
    "Note: \n",
    "- The dataset contains a list of file paths.\n",
    "- The file types are specified as keys.\n",
    "    - `audio_file`: path to audio file\n",
    "    - `is_test`: True/False indicating if it belongs to training/testing dataset\n",
    "    - `sentence_file`: path to the text file containing the sentence spoken in the audio file\n",
    "    - `word_file`: a list of (start index, end index, word) perline in the audio\n",
    "    - `phonetic_file`: a list of (start index, end index, phoneme) perline in the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b51745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract an item from the train_set\n",
    "train_keys = list(Train_dataset.keys())\n",
    "item_info = Train_dataset[train_keys[20]]\n",
    "item_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e422c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play an audio file\n",
    "with open(item_info['sentence_file'], 'r') as infile: print(infile.read())\n",
    "wave, rate = librosa.load(item_info['audio_file'], sr=None)\n",
    "ipd.Audio(wave, rate=rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca685352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the phoneme slices\n",
    "phoneme_slice = {}\n",
    "with open(item_info['phonetic_file'], 'r') as infile:\n",
    "    for line in infile:\n",
    "        start, end, phone = line.split()\n",
    "        phoneme_slice[phone] = (int(start), int(end))\n",
    "print('Phoneme count:', len(phoneme_slice))\n",
    "phoneme_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8749b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a phoneme in play it\n",
    "phone = 'er'\n",
    "start, end = phoneme_slice[phone]\n",
    "phone_wave = wave[start:end]\n",
    "ipd.Audio(phone_wave, rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e807b65",
   "metadata": {},
   "source": [
    "# Build Dataset Classes for Dataloader: Phoneme\n",
    "\n",
    "Notes:\n",
    "- This is a cached data loader\n",
    "- Given an index, returns a tuple as (wave, sample_rate, label)\n",
    "    - wave: librosa waveform\n",
    "    - label: phoneme\n",
    "- Can request a list of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590153ac",
   "metadata": {},
   "source": [
    "## Sentence record to phoneme record converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence record into a list of phoneme record.\n",
    "# Returns a list of tuples: (phoneme, path-to-audio, start-index, end-index)\n",
    "def buildPhoneRecords(sentence_record):\n",
    "    audio_path = sentence_record['audio_file']\n",
    "    phone_path  = sentence_record['phonetic_file']\n",
    "    phone_records = []\n",
    "    with open(phone_path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            start, end, phone = line.split()\n",
    "            phone_records.append((phone, audio_path, int(start), int(end)))\n",
    "    return phone_records\n",
    "\n",
    "\n",
    "# Extract an item from the train_dataset to test above function\n",
    "train_keys = list(Train_dataset.keys())\n",
    "sentence_record = Train_dataset[train_keys[20]]\n",
    "print('sentence_record:', sentence_record)\n",
    "\n",
    "phone_records = buildPhoneRecords(sentence_record)\n",
    "print('Phoneme records:', len(phone_records))\n",
    "phone_records[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e12aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play a phoneme record\n",
    "phone, audio_path, start, end = phone_records[46]\n",
    "wave, rate = librosa.load(audio_path, sr=None)\n",
    "wave = wave[start:end]\n",
    "print('phone:', phone)\n",
    "ipd.Audio(wave, rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3ead0",
   "metadata": {},
   "source": [
    "## Dataset Class\n",
    "\n",
    "This dataset class takes in the TIMIT dataset dictionary built earlier then breaks it down to build a phoneme-level dataset.   \n",
    "It returns an example as tuple: **(phoneme, wave, sample_rate)**\n",
    "\n",
    "Notes:\n",
    "- TimitPhoneDataset[0]: (phoneme, wave, sample_rate)\n",
    "- phone_records[0]: (phoneme, path-to-audio, start-index, end-index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1234e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TimitPhoneDataset(Dataset):\n",
    "    def __init__(self, dataset_dict):\n",
    "        self.base_dataset = dataset_dict\n",
    "        self.phone_records = []\n",
    "        count_missing = 0        # some records have missing phonetic files, let's count it\n",
    "        for key, sent_rec in dataset_dict.items():\n",
    "            # Skip missing phonetic-file records\n",
    "            if 'phonetic_file' not in sent_rec:\n",
    "                count_missing += 1\n",
    "                continue\n",
    "            # break the sentence reconds into phoneme records\n",
    "            phone_recs = buildPhoneRecords(sent_rec)\n",
    "            self.phone_records.extend(phone_recs)\n",
    "        if(count_missing>0): print(f\"WARN: Missing phonetic files for {count_missing} records\")\n",
    "        self.audio_cache = {}   # for caching audio files\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.phone_records)\n",
    " \n",
    "    # Returns: (phoneme, wave, sample_rate)\n",
    "    def __getitem__(self, index):\n",
    "        # self.phone_records[i]: (phoneme, path-to-audio, start-index, end-index)\n",
    "        phone, audio_path, start, end = self.phone_records[index]\n",
    "        wave, rate = self.__getWaveSlice(audio_path, start, end)\n",
    "        return phone, wave, rate\n",
    "    \n",
    "    \n",
    "    # Returns the audio slice given filename, start, end\n",
    "    # Return: (wave-slice, sample_rate)\n",
    "    def __getWaveSlice(self, audio_path, start, end):\n",
    "        # Check in audio cache. On miss, load into cache\n",
    "        if audio_path in self.audio_cache:\n",
    "            wave, rate = self.audio_cache[audio_path]\n",
    "        else:\n",
    "            wave, rate = librosa.load(audio_path, sr=None)\n",
    "            self.audio_cache[audio_path] = (wave, rate)\n",
    "        # Return the selected slice\n",
    "        return wave[start:end], rate\n",
    "    \n",
    "    # Returns a set of all labels in this dataset\n",
    "    def getLabels(self):\n",
    "        labels = set()\n",
    "        for rec in self.phone_records:\n",
    "            labels.add(rec[0])\n",
    "        return labels\n",
    "    \n",
    "    # Returns a dictionary of all labels:counts in this dataset\n",
    "    def getLabelCounts(self):\n",
    "        labels = {}\n",
    "        for rec in self.phone_records:\n",
    "            if rec[0] not in labels: labels[rec[0]] = 0\n",
    "            labels[rec[0]] += 1\n",
    "        return labels\n",
    "\n",
    "    # This function can be used to trim down the dataset for faster experimentation\n",
    "    # Removes all records except the ones with in listKeep\n",
    "    def keepLabels(self, listKeep):\n",
    "        setKeep = set(listKeep)\n",
    "        keep_rec = []\n",
    "        for prec in self.phone_records:\n",
    "            # only keep the specified words\n",
    "            if prec[0] in setKeep:\n",
    "                keep_rec.append(prec)\n",
    "        self.phone_records = keep_rec\n",
    "    \n",
    "    # This can be used to change the label.\n",
    "    # dict_map: (current label, remapped label)\n",
    "    def remapLabels(self, dict_map):        \n",
    "        # self.phone_records[i]: (phoneme, path-to-audio, start-index, end-index)\n",
    "        for index, _ in enumerate(self.phone_records):\n",
    "            phone, audio_path, start, end = self.phone_records[index]\n",
    "            mapped_phone = dict_map[phone]\n",
    "            self.phone_records[index] = (mapped_phone, audio_path, start, end)\n",
    "        \n",
    "        \n",
    "    \n",
    "# Instantiate the datasets\n",
    "print('Loading Train set ...')\n",
    "Train_phone_set = TimitPhoneDataset(Train_dataset)\n",
    "print('Done!')\n",
    "\n",
    "print('\\nLoading Test set ...')\n",
    "Test_phone_set = TimitPhoneDataset(Test_dataset)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473325d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if it works as expected\n",
    "phone, wave, rate = Train_phone_set[351]\n",
    "print(phone)\n",
    "ipd.display(ipd.Audio(wave, rate=rate))\n",
    "\n",
    "phone, wave, rate = Test_phone_set[400]\n",
    "print(phone)\n",
    "ipd.display(ipd.Audio(wave, rate=rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d375fbcb",
   "metadata": {},
   "source": [
    "## Remap Phonemes 61 to 39 classes\n",
    "\n",
    "As we can see the result below, there are 61 phones. However, we don't need to use all of them. \"tcl\", for example, is just a pause where there is a \"t\". So, let's keep it up and simplify them a bit.\n",
    "\n",
    "**NOTE:** The 61 phones defined in TIMIT dataset are remapped to 39 English phones. For details checkout the papers \n",
    "[[1]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=46546)\n",
    "[[2]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638947)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a55a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_labels = Train_phone_set.getLabelCounts()\n",
    "print('all_train_labels Len:', len(all_train_labels))\n",
    "print(all_train_labels.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e39d2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimitBet 61 phoneme mapping to 39 phonemes\n",
    "# by Lee, K.-F., & Hon, H.-W. (1989). Speaker-independent phone recognition using hidden Markov models. IEEE Transactions on Acoustics, Speech, and Signal Processing, 37(11), 1641–1648. doi:10.1109/29.46546 \n",
    "phon61_map39 = {\n",
    "    'iy':'iy',  'ih':'ih',   'eh':'eh',  'ae':'ae',    'ix':'ih',   'ax':'ah',   'ah':'ah',   'uw':'uw',\n",
    "    'ux':'uw',  'uh':'uh',   'ao':'aa',  'aa':'aa',    'ey':'ey',   'ay':'ay',   'oy':'oy',   'aw':'aw',\n",
    "    'ow':'ow',  'l':'l',     'el':'l',   'r':'r',      'y':'y',     'w':'w',     'er':'er',   'axr':'er',\n",
    "    'm':'m',    'em':'m',    'n':'n',    'nx':'n',     'en':'n',    'ng':'ng',   'eng':'ng',  'ch':'ch',\n",
    "    'jh':'jh',  'dh':'dh',   'b':'b',    'd':'d',      'dx':'dx',   'g':'g',     'p':'p',     't':'t',\n",
    "    'k':'k',    'z':'z',     'zh':'sh',  'v':'v',      'f':'f',     'th':'th',   's':'s',     'sh':'sh',\n",
    "    'hh':'hh',  'hv':'hh',   'pcl':'h#', 'tcl':'h#',   'kcl':'h#',  'qcl':'h#',  'bcl':'h#',  'dcl':'h#',\n",
    "    'gcl':'h#', 'h#':'h#',   '#h':'h#',  'pau':'h#',   'epi': 'h#', 'nx':'n',    'ax-h':'ah', 'q':'h#' \n",
    "}\n",
    "\n",
    "Train_phone_set.remapLabels(phon61_map39)\n",
    "Test_phone_set.remapLabels(phon61_map39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c396a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_labels = Train_phone_set.getLabelCounts()\n",
    "all_test_labels = Train_phone_set.getLabelCounts()\n",
    "print('all_train_labels Len:', len(all_train_labels))\n",
    "print('all_test_labels Len :', len(all_test_labels))\n",
    "print('Are labels the same :', all_train_labels == all_test_labels)\n",
    "print(all_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7beed",
   "metadata": {},
   "source": [
    "# Trim Down Dataset\n",
    "\n",
    "If needed, trim down the dataset here for faster experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b557e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset statistics\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(all_train_labels.keys(), all_train_labels.values())\n",
    "plt.title('Train Phoneme Distribution')\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.bar(all_test_labels.keys(), all_test_labels.values())\n",
    "plt.title('Test Phoneme Distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete exceptionally common keys\n",
    "del_keys = ['h#', 'ih','er','n','ah']\n",
    "for key in del_keys: \n",
    "    if key in all_train_labels: del(all_train_labels[key])\n",
    "    \n",
    "# keep n most common keys\n",
    "keep_count = 5\n",
    "sorted_labels = [(cnt, word) for word, cnt in all_train_labels.items()]\n",
    "sorted_labels = sorted(sorted_labels, reverse=True)\n",
    "#sorted_labels\n",
    "keep_labels = sorted_labels[:keep_count]\n",
    "print('\\nkeep_labels:', keep_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaa3b96",
   "metadata": {},
   "source": [
    "## Deactivated for training on whole dataset\n",
    "```python\n",
    "# Trim Down Dataset: Deactivate this cell to avoid trimming\n",
    "keep_labels = [tup[1] for tup in keep_labels]\n",
    "print('\\nBefore: Train#', len(Train_phone_set), '  Test#', len(Test_phone_set))\n",
    "Train_phone_set.keepLabels(keep_labels)\n",
    "Test_phone_set.keepLabels(keep_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75691a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After : Train#', len(Train_phone_set), '  Test#', len(Test_phone_set))\n",
    "\n",
    "Labels = list(Train_phone_set.getLabels())    # save for later use\n",
    "print('\\nLabels:', len(Labels), '\\n', Labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa04b31",
   "metadata": {},
   "source": [
    "# Define Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44ec324",
   "metadata": {},
   "source": [
    "```python\n",
    "# Features: MFCC coefficients\n",
    "Feature_length = 40\n",
    "FFT_window = 32\n",
    "\n",
    "Feature_note = f'Features are extracted using librosa.feature.mfcc(y=wave, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft). n_mfcc: {Feature_length}  n_fft: {FFT_window}'\n",
    "\n",
    "# Takes a dataset item and returns the feature vector\n",
    "# item: (phoneme, wave, sample_rate)\n",
    "def extract_features(item):\n",
    "    _, wave, sample_rate = item\n",
    "    n_mfcc = Feature_length\n",
    "    n_fft = FFT_window   # FFT window\n",
    "    # Increase sample rate as the phonemes are pretty short recording (made the training worse)\n",
    "    #scale_fact = 4\n",
    "    #wave = librosa.resample(y=wave, orig_sr=sample_rate, target_sr=sample_rate*scale_fact)\n",
    "    #sample_rate *= scale_fact\n",
    "    return librosa.feature.mfcc(y=wave, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ab495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features: MFCC coefficients\n",
    "FFT_window = 32\n",
    "Mel_count = 40\n",
    "Interp_width = 3\n",
    "\n",
    "\n",
    "# Takes a dataset item and returns the feature vector\n",
    "# item: (phoneme, wave, sample_rate)\n",
    "def extract_features_v1(item):\n",
    "    _, wave, sample_rate = item\n",
    "    # Compute Mel-scale filter bank coefficients\n",
    "    n_fft = FFT_window  # Number of FFT points\n",
    "    n_mels = Mel_count   # Number of Mel bins\n",
    "    hop_length = 8  # Hop length for computing the STFT\n",
    "    mel_spec = librosa.feature.melspectrogram(y=wave, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec)  # Convert to log-scale\n",
    "\n",
    "    # Compute energy term (sum of squared magnitudes of the STFT)\n",
    "    #stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    #energy = librosa.feature.rms(S=stft)\n",
    "\n",
    "    # Extract the first 40 coefficients and the energy term\n",
    "    mel_coefficients = log_mel_spec[:Mel_count]\n",
    "    #energy = energy.squeeze()\n",
    "    return mel_coefficients\n",
    "\n",
    "\n",
    "# Takes a dataset item and returns the feature vector\n",
    "# item: (phoneme, wave, sample_rate)\n",
    "def extract_features(item):\n",
    "    _, wave, sample_rate = item\n",
    "    # Compute Mel-scale filter bank coefficients\n",
    "    n_fft = FFT_window  # Number of FFT points\n",
    "    n_mels = Mel_count   # Number of Mel bins\n",
    "    hop_length = 16     # Hop length for computing the STFT\n",
    "    mel_spec = librosa.feature.melspectrogram(y=wave, sr=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec)  # Convert to log-scale\n",
    "    # Compute first-order temporal derivatives\n",
    "    delta_mel = librosa.feature.delta(log_mel_spec, width=Interp_width)\n",
    "    # Compute second-order temporal derivatives\n",
    "    delta2_mel = librosa.feature.delta(log_mel_spec, width=Interp_width, order=2)\n",
    "    # build feature vector from above features coefficients\n",
    "    #print('long_mel_spec:', log_mel_spec.shape)\n",
    "    #print('delta:', delta_mel.shape)\n",
    "    #print('delta2:', delta2_mel.shape)\n",
    "    feat_vec = np.vstack([log_mel_spec, delta_mel, delta2_mel])\n",
    "    #feat_vec_norm = normalizeAlongColumns(feat_vec)\n",
    "    #print('feat_vec:', feat_vec.shape)\n",
    "    return feat_vec\n",
    "\n",
    "\n",
    "# Normalizes a numpy 2D array along columns\n",
    "def normalizeAlongColumns(nparr, eps=0.001):\n",
    "    assert len(nparr.shape) == 2, \"Expected 2D Numpy array\"\n",
    "    mean = np.mean(nparr, axis=0)\n",
    "    std  = np.std(nparr, axis=0)\n",
    "    std  = np.where(std==0, eps, std)   # replace 0 with eps, to avoid division by zero\n",
    "    normalized = (nparr - mean) / std\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1a6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Extract features of a sample and plot them\n",
    "# Generated by ChatGPT\n",
    "def plot_audio_and_mfcc_gpt(audio, sample_rate, mfcc_features):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    \n",
    "    # Plot the waveform\n",
    "    plt.subplot(1, 2, 1)\n",
    "    duration = len(audio) / sample_rate\n",
    "    time = np.linspace(0, duration, len(audio))\n",
    "    plt.plot(time, audio)\n",
    "    plt.title('Waveform')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    \n",
    "    # Plot the MFCC feature\n",
    "    plt.subplot(1, 2, 2)\n",
    "    librosa.display.specshow(mfcc_features, x_axis='time')\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('MFCC')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('MFCC Coefficients')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Extract mfcc coefficients\n",
    "item = Train_phone_set[2100]\n",
    "feat_vec = extract_features(item)\n",
    "print('feat_vec:', feat_vec.shape)\n",
    "\n",
    "\n",
    "Feature_length = feat_vec.shape[0]    # no. of rows is the number of features here\n",
    "#Feature_note = f'Features are extracted using librosa.feature.mfcc(y=wave, sr=sample_rate, n_mfcc=n_mfcc, n_fft=n_fft). n_mfcc: {Feature_length}  n_fft: {FFT_window}'\n",
    "Feature_note = \"Undefined\"\n",
    "\n",
    "phone, wave, rate = item\n",
    "print(phone)\n",
    "ipd.display(ipd.Audio(wave, rate=rate))\n",
    "plot_audio_and_mfcc_gpt(wave, rate, feat_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9405e4a5",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d7a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "\n",
    "# LSTM model definition\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.debug = False    # Set it to true to print debug info\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    # Expects a padded_sequence of batched input and the lengths of the sequences\n",
    "    def forward(self, pad_seq, lengths):        \n",
    "        if self.debug: print('DEBUG START: LSTM model ---')\n",
    "\n",
    "        # Extract batch size for initialization of hidden state\n",
    "        batch_size = len(pad_seq)\n",
    "            \n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        # Convert padded sequence to variable length packed sequence for LSTM\n",
    "        packed_seq = rnn_utils.pack_padded_sequence(pad_seq, lengths, enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        # Forward propagate LSTM, returns a packed sequence\n",
    "        out_packed, _ = self.lstm(packed_seq, (h0, c0))\n",
    "        \n",
    "        # Extract final hidden states of each sequence for the output layer\n",
    "        out_pad, out_lens = rnn_utils.pad_packed_sequence(out_packed, batch_first=True)\n",
    "        out_indx = out_lens - 1   # indices of the last valid hidden state in the padded sequence\n",
    "        last_hidden = out_pad[range(batch_size), out_indx].contiguous()  # select the last valid state in each sequence, and make them contiguous for efficiency\n",
    "                \n",
    "        if self.debug:\n",
    "            print('last_hidden size:', last_hidden.size())\n",
    "            print('last_hidden:\\n', last_hidden)\n",
    "        \n",
    "        # Decode the hidden state of the last time step only (for whole batch)\n",
    "        out = self.fc(last_hidden)\n",
    "        if self.debug: print('DEBUG END: LSTM model ---')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce8201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LSTM\n",
    "input_size  = Feature_length    # no. of mfcc coefficients\n",
    "hidden_size = 250               # no. of hiddent units in LSTM\n",
    "num_layers  = 3\n",
    "num_classes = len(Labels)\n",
    "model_lstm250  = LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "model_lstm250.to(device)\n",
    "print(model_lstm250)\n",
    "\n",
    "\n",
    "# Save the model parameters\n",
    "Hparam = {\n",
    "    'input_size' : input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers' : num_layers,\n",
    "    'num_classes': num_classes, \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51174c3",
   "metadata": {},
   "source": [
    "# Define Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a7ae13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities to convert between label and index\n",
    "\n",
    "Label2index = {label:index for index, label in enumerate(Labels)}\n",
    "\n",
    "def label_to_index(phone):\n",
    "    # Return the position of the word in labels\n",
    "    return torch.tensor(Label2index[phone])\n",
    "\n",
    "def index_to_label(index):\n",
    "    # Return the word corresponding to the index in labels\n",
    "    # This is the inverse of label_to_index\n",
    "    return Labels[index]\n",
    "\n",
    "\n",
    "# Test the utilities\n",
    "phone_start = Labels[2]\n",
    "index = label_to_index(phone_start)\n",
    "phone_recovered = index_to_label(index)\n",
    "\n",
    "print(phone_start, \"-->\", index, \"-->\", phone_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa92c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding is needed to make the batch <tensor> from <list> of variable length sequences\n",
    "# The padding values are not passed to the LSTM during trainig/testing\n",
    "def pad_sequence_lstm(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch\n",
    "\n",
    "# Gets the list of audio and labels as batch then\n",
    "# converts them into sequence of features for the model.\n",
    "# Adds padding to build the batch tensor\n",
    "def collate_fn_lstm(batch):\n",
    "    # A data tuple has the form: (phoneme, wave, sample_rate)\n",
    "    tensors, targets, lengths = [], [], []   # lengths is needed for pack_padded_sequence during training/testing\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for item in batch:\n",
    "        label, waveform, rate = item\n",
    "        features = extract_features(item)\n",
    "        features = torch.from_numpy(features).t()    # Transpose needed to make it (points, no. of features)\n",
    "        tensors += [features]\n",
    "        targets += [label_to_index(label)]\n",
    "        lengths.append(features.size()[0])\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence_lstm(tensors)\n",
    "    targets = torch.stack(targets)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    return tensors, targets, lengths\n",
    "\n",
    "\n",
    "\n",
    "# Test padding function\n",
    "batch = [   torch.tensor([2,3,4,5]), \n",
    "            torch.tensor([2,3]),\n",
    "            torch.tensor([5,6,7])    ]\n",
    "pad_seq = pad_sequence_lstm(batch)\n",
    "print('pad_seq:\\n', pad_seq)\n",
    "\n",
    "# Test collate function\n",
    "batch = [Test_phone_set[i*21] for i in range(5)]\n",
    "col_tensor, col_targets, col_lengths = collate_fn_lstm(batch)\n",
    "print('\\ncollate_out:\\n', col_tensor.size(), '\\n', col_targets, '\\n', col_lengths)\n",
    "\n",
    "# Manual features to compare\n",
    "print('')\n",
    "phone, wave, rate = batch[1]\n",
    "print('phone:', phone)\n",
    "ipd.Audio(wave, rate=rate)\n",
    "mfcc = librosa.feature.mfcc(y=wave, sr=rate, n_mfcc=Feature_length, n_fft=FFT_window)\n",
    "print('mfcc shape:', mfcc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2417df4",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a065539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_size_train = 512\n",
    "Batch_size_test = 1024\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "Train_loader = torch.utils.data.DataLoader(\n",
    "    Train_phone_set,\n",
    "    batch_size=Batch_size_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_lstm,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "Test_loader = torch.utils.data.DataLoader(\n",
    "    Test_phone_set,\n",
    "    batch_size=Batch_size_test,\n",
    "    #shuffle=False,\n",
    "    shuffle=True,        # for fractional validation\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn_lstm,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64970b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataloaders\n",
    "for (sequence, labels, lengths) in Train_loader:\n",
    "    print('sequence:', sequence.size(), 'type:', sequence.type())\n",
    "    print('labels  :', labels.size(), 'type:', labels.type())\n",
    "    print('lengths :', lengths.size(), 'type:', lengths.type())\n",
    "    break\n",
    "    \n",
    "print('')\n",
    "for (sequence, labels, lengths) in Test_loader:\n",
    "    print('sequence:', sequence.size(), 'type:', sequence.type())\n",
    "    print('labels  :', labels.size(), 'type:', labels.type())\n",
    "    print('lengths :', lengths.size(), 'type:', lengths.type())\n",
    "    break\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8957b1e2",
   "metadata": {},
   "source": [
    "# Training and Testing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9defd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "# plot_fig_ax: (figure, ax), pass the plotting figure and axis handles\n",
    "#   returned by matplotlib for realtime update of training loss.\n",
    "def train_model(model, epoch, log_interval, plot_fig_ax=None, debug=False, display=True):\n",
    "    model.train()\n",
    "    model.debug = debug\n",
    "    # Set up the batch iterator with/without progressbar\n",
    "    if display:\n",
    "        total_batch = ceil(len(Train_phone_set)/Batch_size_train)\n",
    "        batch_iter = tqdm(enumerate(Train_loader), total=total_batch, desc=f'Epoch {epoch}')\n",
    "    else:\n",
    "        batch_iter = enumerate(Train_loader)\n",
    "    # Run the training loop\n",
    "    for batch_idx, (sequence, target, lengths) in batch_iter:\n",
    "        sequence = sequence.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Run through model and compute loss\n",
    "        output = model(sequence, lengths)\n",
    "        loss = criterion(output, target)    # compute batch loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if display==True and batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{:6}/{} ({:2.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                      epoch, batch_idx * len(sequence), len(Train_loader.dataset),\n",
    "                      100. * batch_idx / len(Train_loader), loss.data.item()) )\n",
    "        # Debug\n",
    "        if debug and batch_idx == 100: \n",
    "            print('DBG: Breaking prematurely')\n",
    "            break\n",
    "\n",
    "        # record and plot loss\n",
    "        train_loss.append(loss.item())\n",
    "        if plot_fig_ax:\n",
    "            fig, ax = plot_fig_ax\n",
    "            ax.plot(train_loss, color='b')\n",
    "            fig.canvas.draw()\n",
    "    model.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c709305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "# Returns percent accuracy\n",
    "# User fraction: <1.0 to run validation on a fraction of the test data\n",
    "def test_model(model, fraction=1.0, debug=False, display=True):\n",
    "    model.eval()\n",
    "    model.debug = debug\n",
    "    # Set up the batch iterator with/without progressbar\n",
    "    total_batch = ceil(len(Test_phone_set)/Batch_size_test)\n",
    "    if display:\n",
    "        batch_iter = tqdm(Test_loader, total=total_batch, desc=f'Validation')\n",
    "    else:\n",
    "        batch_iter = Test_loader\n",
    "    # Compute the stop batch no.\n",
    "    stop_count = int(ceil(total_batch*fraction))\n",
    "    # Run the test dataset through the model\n",
    "    loss, correct = 0, 0\n",
    "    tested_count = 0\n",
    "    for sequence, targets, lengths in batch_iter:\n",
    "        sequence = sequence.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(sequence, lengths)\n",
    "        loss += criterion(outputs, targets).data.item()\n",
    "        pred = get_likely_index(outputs) # get the index of the max log-probability\n",
    "        correct += number_of_correct(pred, targets)\n",
    "        tested_count += len(targets)  # increment the tested item counter\n",
    "        # Run only on the given fraction\n",
    "        stop_count -= 1\n",
    "        if stop_count < 0: break\n",
    "    model.debug = False\n",
    "    \n",
    "    # Print statistics\n",
    "    loss /= tested_count\n",
    "    test_loss.append(loss)\n",
    "    accuracy = (100.0 * correct) / tested_count\n",
    "    test_accuracy.append(accuracy)\n",
    "    \n",
    "    if display:\n",
    "        print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                 loss, correct, tested_count, accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dab008",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d4c45",
   "metadata": {},
   "source": [
    "Defining Interface Components for easier management of training sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from ipywidgets.widgets import HTML\n",
    "import IPython.display as ipd\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Make a widget to show status text at the top of the cell\n",
    "status_text = HTML()\n",
    "status_init = \"<b>Status:</b> Start the training\"\n",
    "status_start = \"<b>Status:</b> Training started ...\"\n",
    "status_end   = \"<br><b style='color:green'>Done!</b>\"\n",
    "substatus_txt = HTML()\n",
    "\n",
    "def updateTrainStatus(target_models, cur_accuracy, best_accuracy):\n",
    "    param_style = 'style=\"color:indianred\"'\n",
    "    text = f'<b>Status:</b> target-model #: <b {param_style}>{{}}</b> cur-accuracy: <b {param_style}>{{:.2f}}%</b>   best-accuracy: <b {param_style}>{{:.2f}}%</b>'\n",
    "    status_text.value = text.format(len(target_models), cur_accuracy, best_accuracy)\n",
    "\n",
    "\n",
    "# Call this function where you want to show the interactive plot\n",
    "fig_train = None\n",
    "ax_trainloss = None\n",
    "ax_testloss = None\n",
    "ax_testaccu = None\n",
    "\n",
    "def showTrainPlot():\n",
    "    global fig_train, ax_trainloss, ax_testloss, ax_testaccu\n",
    "    fig_height = 3.2\n",
    "    fig_train = plt.figure(\"Training Progress\", figsize=(3*fig_height, fig_height))\n",
    "    # Training loss axis\n",
    "    ax_trainloss = fig_train.add_subplot(1, 3, 1)\n",
    "    ax_trainloss.set_title(\"Training Loss\")\n",
    "    # Test (validation) loss axis\n",
    "    ax_testloss = fig_train.add_subplot(1, 3, 2)\n",
    "    ax_testloss.set_title(\"Test Loss\")\n",
    "    # Test (validation) accuracy axis\n",
    "    ax_testaccu = fig_train.add_subplot(1, 3, 3)\n",
    "    ax_testaccu.set_title(\"Test Accuracy\")\n",
    "    \n",
    "\n",
    "# Call this function to update the plot\n",
    "def updateTrainPlot(test_loss, test_accuracy):\n",
    "    ax_testloss.plot(test_loss, color='g')\n",
    "    ax_testaccu.plot(test_accuracy, color='g')\n",
    "    fig_train.canvas.draw()\n",
    "\n",
    "\n",
    "# Define the container to save the best models\n",
    "Saved_models = {-1:'Dummy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c766f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = optim.SGD(model_lstm250.parameters(), lr=0.01, momentum=0.5)\n",
    "optimizer = optim.Adam(model_lstm250.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "lrScheduler = optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)  # reduce the learning after given steps by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828e5bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop parameters\n",
    "n_epoch = 20\n",
    "target_accuracy = 70.0  # percent\n",
    "train_display = False\n",
    "test_display = False\n",
    "test_fraction = 0.1   # run validation on random fraction of the test dataset\n",
    "log_interval_percent = 100\n",
    "log_interval = (len(Train_phone_set)//Batch_size_train) * log_interval_percent // 100\n",
    "\n",
    "\n",
    "# Tracking variables\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "target_models = {}\n",
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "\n",
    "# Set up training dashboard\n",
    "epoch_iter = tqdm(range(n_epoch), desc=\"Training\")\n",
    "status_text.value = status_init\n",
    "ipd.display(status_text)\n",
    "ipd.display(substatus_txt)\n",
    "showTrainPlot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24106ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Train the model and save the ones with accuracy >= target_accuracy\n",
    "status_text.value = status_start\n",
    "for epoch in epoch_iter:\n",
    "    # Training and testing\n",
    "    substatus_txt.value = \"Running Training\"\n",
    "    train_model(model_lstm250, epoch, log_interval, \n",
    "                plot_fig_ax=(fig_train, ax_trainloss), \n",
    "                debug=False, display=train_display)\n",
    "    substatus_txt.value = \"Running Validation ...\"\n",
    "    accuracy = test_model(model_lstm250, fraction=test_fraction, debug=False, display=test_display)\n",
    "    lrScheduler.step()\n",
    "    substatus_txt.value = \"\"\n",
    "    \n",
    "    # Save models achieving target accuracy\n",
    "    if accuracy >= target_accuracy:\n",
    "        accuracy = round(accuracy, 4)   # to reduce the key granularity\n",
    "        target_models[accuracy] = deepcopy(model_lstm250.state_dict())\n",
    "    \n",
    "    # Save the best model and update the status text\n",
    "    if accuracy > best_accuracy: \n",
    "        best_accuracy = accuracy\n",
    "        best_model = deepcopy(model_lstm250.state_dict())\n",
    "    updateTrainStatus(target_models, accuracy, best_accuracy)\n",
    "    updateTrainPlot(test_loss, test_accuracy)\n",
    "\n",
    "status_text.value += status_end\n",
    "\n",
    "\n",
    "# Print and save the best performing models, and show the training summary\n",
    "summary = []\n",
    "cnt = len(target_models)\n",
    "summary.append(f'Target met by: {cnt}')\n",
    "if cnt > 0: \n",
    "    summary.append('Saving target_models')\n",
    "    Saved_models.update(target_models)  # copy the target_models into the Saved_models\n",
    "summary.append(f'Saved_models#: {len(Saved_models)}')\n",
    "summary.append(f'Saved max acc: {max(Saved_models)}%')\n",
    "summary.append(f'Best in this iter: {best_accuracy:.2f}%')\n",
    "print('\\n'.join(summary))\n",
    "status_text.value += '<br>' + '<br>'.join(summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f4207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the best model\n",
    "saved_acc = max(Saved_models)\n",
    "if saved_acc > best_accuracy:\n",
    "    print(f'Loading from Saved models, accuracy: {saved_acc:.2f}%')\n",
    "    best_model_dict = Saved_models[saved_acc]\n",
    "else:\n",
    "    print(f'Loading from last training session, validation accuracy: {best_accuracy:.2f}%')\n",
    "    best_model_dict = best_model\n",
    "    \n",
    "model_lstm250.load_state_dict(best_model_dict)\n",
    "\n",
    "\n",
    "# Run the model on the entire test dataset\n",
    "accuracy = test_model(model_lstm250)\n",
    "print(f'Accuracy on whole test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0045a72",
   "metadata": {},
   "source": [
    "# Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53100472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the prediction\n",
    "def predict(item):  # item: an item in the dataset\n",
    "    model_lstm250.eval()\n",
    "    # Extract features\n",
    "    batch = [item]  # make a batch with single example\n",
    "    tensor, target, lengths = collate_fn_lstm(batch)\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model_lstm250(tensor, lengths)\n",
    "    pred = get_likely_index(output)[0]   # indexing to get the prediction from batch    \n",
    "    return pred\n",
    "\n",
    "\n",
    "# Run a prediction\n",
    "select_index = 1000\n",
    "item = Test_phone_set[select_index]\n",
    "pred_index = predict(item)\n",
    "pred_label  = index_to_label(pred_index)\n",
    "phone, wave, rate = item\n",
    "print(f\"Expected: {phone}. Predicted: {pred_label}.\")\n",
    "ipd.Audio(wave, rate=rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae546f",
   "metadata": {},
   "source": [
    "# Save Model and Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92190ac0",
   "metadata": {},
   "source": [
    "The **test dataset** and the prediction by the trained model is saved as the following dictionary:\n",
    "- note: Text containing information for the human user of this dataset\n",
    "- label_dict: {label: index}\n",
    "- dataset_schema: description of the structure of dataset items\n",
    "- dataset: list of items\n",
    "    - item: (label, label_index, predicted_index, sequence_length, feature_length, feature_sequence_2D)\n",
    "\n",
    "\n",
    "The **trained model** is saved as the following dictionary:\n",
    "- note: Text containing information for the human user of this model\n",
    "- accuracy: percent accuracy as float\n",
    "- correct_count: no. of correct prediction as integer\n",
    "- Hparam: model hyper parameters as dictionary\n",
    "- state_dict: The nn.Model.state_dict() of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dccb1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing accuracy and building the dataset records for exporting\n",
    "correct_count = 0\n",
    "dataset = []\n",
    "for item in tqdm(Test_phone_set):\n",
    "    pred_index = predict(item)\n",
    "    seq, target, length = collate_fn_lstm([item])        # make a batch with single item\n",
    "    seq, target, length = seq[0], target[0], length[0]   # remove the batch dimension\n",
    "    label, *_ = item\n",
    "    item = [label, target.item(), pred_index.item(), len(seq), len(seq[0]), seq.tolist()]\n",
    "    pred_label = index_to_label(pred_index)\n",
    "    if pred_label==label: correct_count += 1\n",
    "    dataset.append(item)\n",
    "\n",
    "accuracy = (100.0 * correct_count) / len(Test_phone_set)\n",
    "\n",
    "print('dataset:', len(dataset), '  Test_phone_set:', len(Test_phone_set))\n",
    "print('correct_count:', correct_count)\n",
    "print(f'accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a6a91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Model with hyper-parameters and accuracy\n",
    "save_path = './saved/trained_lstm250.pt'\n",
    "model_dict = {\n",
    "    'note'         : Feature_note,\n",
    "    'accuracy'     : accuracy,\n",
    "    'correct_count': correct_count,\n",
    "    'Hparam'       : Hparam,       # Hparam was defined while instantiating the model\n",
    "    'state_dict'   : deepcopy(model_lstm250.state_dict())\n",
    "}\n",
    "torch.save(model_dict, save_path)\n",
    "!ls -ltrh ./saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataset records\n",
    "item = dataset[0][:-1]  # Everything except feature_vector\n",
    "seq = dataset[0][-1]\n",
    "\n",
    "print('item:', item)  \n",
    "print('feat_vec:', type(seq), torch.tensor(seq).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dataset dictionary for exporting\n",
    "label_dict = Label2index\n",
    "schema = \"(label, label_index, predicted_index, sequence_length, feature_length, feature_sequence_2D)\"\n",
    "export_test = {\n",
    "    'note'      : Feature_note,\n",
    "    'label_dict': label_dict,\n",
    "    'dataset_schema': schema,\n",
    "    'dataset': dataset\n",
    "}\n",
    "\n",
    "# Test exported dataset dictionary\n",
    "# Print all keys and items, except the dataset (too big for printing)\n",
    "keys = list(export_test.keys())\n",
    "keys.remove('dataset')\n",
    "for k in keys:\n",
    "    print(k+':', export_test[k])\n",
    "\n",
    "# Print a single item information from the dataset\n",
    "item = dataset[0][:-1]  # Everything except feature_vector\n",
    "feat_vec = dataset[0][-1]\n",
    "print('item:', item)  \n",
    "print('feat_vec:', type(feat_vec), len(feat_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204212b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset dictionary as PyTorch object\n",
    "save_path = './saved/test_dataset.pt'\n",
    "torch.save(export_test, save_path)\n",
    "!ls -ltrh ./saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567ff20",
   "metadata": {},
   "source": [
    "# Test prediction from Exported Dataset item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271b25c",
   "metadata": {},
   "source": [
    "## Dataset related utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca3229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints a dataset item\n",
    "def print_dataitem(item):\n",
    "    mstr = f\"label: {item[0]}, label_index: {item[1]}, predicted_index: {item[2]}, sequence_length: {item[3]}, feature_length: {item[4]},\"\n",
    "    mstr2 = f\"feature_vector size: ({len(item[5])}, {len(item[5][0])})\"\n",
    "    print(mstr, mstr2)\n",
    "    \n",
    "    \n",
    "# Given an item form the test_dataset, returns an example for predict() function below\n",
    "def make_example(data_item):\n",
    "    sequence = torch.tensor(data_item[5])\n",
    "    length = torch.tensor(data_item[3])\n",
    "    return sequence, length\n",
    "\n",
    "\n",
    "# test prediction from dataset item\n",
    "def predict_dataset(example, model=None):  # example: (feature_seqeunce, sequence_length)\n",
    "    model.eval()\n",
    "    # Use the model to predict the label of the waveform\n",
    "    sequence = example[0].to(device)\n",
    "    length = example[1]\n",
    "    output = model(sequence.unsqueeze(0), length.unsqueeze(0))   # add batch dimension and pass through model\n",
    "    #print(output)\n",
    "    pred = get_likely_index(output)[0]   # remove batch dimension\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a280d0",
   "metadata": {},
   "source": [
    "## Load and Test the Saved Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a74819e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_path = './saved/trained_lstm250.pt'\n",
    "model_dict = torch.load(model_path)\n",
    "print(model_dict.keys())\n",
    "print(model_dict['Hparam'].keys())\n",
    "print(f\"     accuracy: {model_dict['accuracy']}%\")\n",
    "print(f\"correct_count: {model_dict['correct_count']}\")\n",
    "\n",
    "loaded_hparam = model_dict['Hparam']\n",
    "#loaded_model = LSTM(loaded_hparam['input_size'], loaded_hparam['num_classes']).to(device)\n",
    "loaded_model  = LSTM(loaded_hparam['input_size'], loaded_hparam['hidden_size'], loaded_hparam['num_layers'], loaded_hparam['num_classes'])\n",
    "loaded_model.load_state_dict(model_dict['state_dict'])\n",
    "loaded_model.to(device)\n",
    "print('\\n', loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e2308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and predict using an item from it\n",
    "ds_path = './saved/test_dataset.pt'\n",
    "loaded_ds = torch.load(ds_path)\n",
    "item = loaded_ds['dataset'][0]\n",
    "example = make_example(item)\n",
    "pred = predict_dataset(example, model=loaded_model)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf09435",
   "metadata": {},
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "Now you can continue to the next notebook to implement the model with basic matrix-vector operations without using any PyTorch ML/DL related features. Be careful with the **dataset schema**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
