{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c232c11",
   "metadata": {},
   "source": [
    "# LSTM-250 Model Export as SQLite3 Database\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Loads Dataset and Model from Numpy notebook.\n",
    "- Model and dataset exported as sqlite3 database for implementation in C.\n",
    "\n",
    "**NOTE:** The dataset exported by the training notebook may have incorrect predicted index due to several iterations of model training and not updating the dataset. We'll re-run the predictions here and update the predicted index in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d25674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e790384",
   "metadata": {},
   "source": [
    "# Load Dataset and the Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c83c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_model_path = './session/numpy-model-dataset.pt'\n",
    "loaded_dict = torch.load(np_model_path)\n",
    "print(loaded_dict.keys())\n",
    "\n",
    "# Extract the items from the dictionary\n",
    "Accuracy = loaded_dict['accuracy']\n",
    "Correct_count = loaded_dict['correct_count']\n",
    "Hparam = loaded_dict['Hparam']\n",
    "Index_to_label = loaded_dict['index_to_label']\n",
    "Weights_np = loaded_dict['Weights_np']\n",
    "DataItems = loaded_dict['DataItems']\n",
    "\n",
    "# Delete redundant names\n",
    "del np_model_path, loaded_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee4583",
   "metadata": {},
   "source": [
    "# Export Numpy Model as sqlite3 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10fe243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the cache and import sqlite3 utilities\n",
    "!rm -rf __pycache__/\n",
    "from utilsqlite3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae66d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database file\n",
    "DB_path = './saved/trained-lstm250.s3db'\n",
    "createDB(DB_path, overwrite=True)\n",
    "!ls -ltrh ./saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120504b1",
   "metadata": {},
   "source": [
    "## Write the Header Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89af27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the header table\n",
    "def createHeaderTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = '''CREATE TABLE IF NOT EXISTS Header (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        key TEXT,\n",
    "                        value NUMERIC,\n",
    "                        description TEXT\n",
    "                    )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createHeaderTable(DB_path)\n",
    "table_names = getTableNames(DB_path)\n",
    "print(table_names)\n",
    "del table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts a record into the Header table (will be called by insertRecordList() utility function)\n",
    "def insertHeaderRecord(cursor, record):\n",
    "    key, value, description = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    cursor.execute('''INSERT INTO Header (key, value, description)\n",
    "                      VALUES (?, ?, ?)''', (key, value, description))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB_path, insertHeaderRecord, [('example_key', 'example_value', 'example_description')])\n",
    "getRecords(DB_path, 'Header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13518568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make valid table names (cannot have . in their name)\n",
    "# Table name: lstm/fc_weight/bias_xx_lx\n",
    "print(Weights_np.keys())\n",
    "Weight_tables = {}\n",
    "for key, param in Weights_np.items():\n",
    "    tname = key.replace('.', '_')          # replace any remaining '.' with '_'\n",
    "    tkey  = key+'.table'                   # key for the Header table\n",
    "    Weight_tables[tkey] = (tname, param)   # save as (tname, param) to be used later\n",
    "    print(f'{tkey:23}', ':', tname)\n",
    "\n",
    "# Delete redundant names\n",
    "del key, param, tname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ebde74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the records as a list\n",
    "Hparam_table = 'Hparam_T'\n",
    "\n",
    "header_records = [\n",
    "    ('name', 'LSTM-250', ''),\n",
    "    ('architecture', '123-LSTM:250-LSTM:250-LSTM:250-FC:39', 'It is an LSTM with 3 layers with 250 units with a fully-connected layer at the output with 39 classes. Trained on TIMIT dataset for phoneme classification.'),\n",
    "    ('accuracy', Accuracy, 'Accuracy% of the trained model on the test dataset.'),\n",
    "    ('correct_count', Correct_count, 'Number of correct predictions by the trained model on the test dataset.'),\n",
    "    ('Hparam.table',   Hparam_table, 'This is the name of the table that contains different parameters of the model.'),\n",
    "]\n",
    "for k,v in Weight_tables.items():\n",
    "    header_records.append( (k, v[0], 'Name of the table containing the parameter ' + k.replace('.table','')) )\n",
    "\n",
    "\n",
    "# Insert the header records\n",
    "deleteRows(DB_path, 'Header')  # delete previous records\n",
    "insertRecordList(DB_path, insertHeaderRecord, header_records)\n",
    "records = getRecords(DB_path, 'Header')\n",
    "for r in records: print(r[:-1])   # print all but description field\n",
    "    \n",
    "# Delete names\n",
    "del records, k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba6e5a",
   "metadata": {},
   "source": [
    "## Write the Hparam Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the Hparam table\n",
    "def createHparamTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = f'''CREATE TABLE IF NOT EXISTS {Hparam_table} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        key TEXT,\n",
    "                        value NUMERIC,\n",
    "                        description TEXT\n",
    "                    )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createHparamTable(DB_path)\n",
    "table_names = getTableNames(DB_path)\n",
    "print(table_names)\n",
    "\n",
    "# Delete neames\n",
    "del table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts a record into the Hparam table\n",
    "def insertHparamRecord(cursor, record):\n",
    "    key, value, description = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    cursor.execute(f'''INSERT INTO {Hparam_table} (key, value, description)\n",
    "                      VALUES (?, ?, ?)''', (key, value, description))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB_path, insertHparamRecord, [('example_key', 123, 'example_description')])\n",
    "getRecords(DB_path, Hparam_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the Hparam records\n",
    "print('Hparam from pytorch:', Hparam)\n",
    "\n",
    "hparam_records = [\n",
    "    ('input_size', Hparam['input_size'], 'Input size of the LSTM'),\n",
    "    ('hidden_size', Hparam['hidden_size'], 'Size of hidden states the LSTM layers'),\n",
    "    ('num_layers', Hparam['num_layers'], 'Number of LSTM layers'),\n",
    "    ('num_classes', Hparam['num_classes'], 'Output size of the Fully-Connected output layer'),\n",
    "]\n",
    "for k, v in Weights_np.items():\n",
    "    hparam_records.append( (k+'.shape', str(v.shape), 'Dimensions of the parameter '+k) )\n",
    "    \n",
    "\n",
    "deleteRows(DB_path, Hparam_table)\n",
    "insertRecordList(DB_path, insertHparamRecord, hparam_records)\n",
    "getRecords(DB_path, Hparam_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58363c0",
   "metadata": {},
   "source": [
    "## Write Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcddbf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves a numpy 2D array as a table in the database.\n",
    "# Columns: ID, row_no, col_0, col_1, ..., col_n\n",
    "def createMatrixTable(db_path, table_name, nparray, overwrite=False):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop the table if it exists and overwrite requested\n",
    "    table_exist = existTable(db_path, table_name)\n",
    "    if overwrite and table_exist:\n",
    "        print(f'WARN: Overwriting table {table_name}')\n",
    "        dropTable(db_path, table_name)\n",
    "        \n",
    "    # Create the table\n",
    "    rows, cols = nparray.shape\n",
    "    column_names = \"row_no, \" + \", \".join([f\"col_{i}\" for i in range(cols)])  # Generate the column names string\n",
    "    cursor.execute(f\"CREATE TABLE {table_name} ({column_names})\")\n",
    "\n",
    "    # Insert the array rows into the table\n",
    "    for i in range(rows):\n",
    "        vals = f'{i}, ' + ', '.join(map(str, nparray[i]))\n",
    "        cursor.execute(f\"INSERT INTO {table_name} VALUES ({vals})\")\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# test createMatrixTable()\n",
    "createMatrixTable(DB_path, 'test', Weights_np['fc.weight'], overwrite=True)\n",
    "col_names = getColNames(DB_path, 'test')\n",
    "records = getRecords(DB_path, 'test')\n",
    "print('col_names:', col_names[:5], '...', col_names[-5:])\n",
    "print('records[i]:', records[2][:5], '...')\n",
    "\n",
    "# Delete names\n",
    "del col_names, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c86934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "for k,v in Weight_tables.items():\n",
    "    tname, params = v    # extract the table name and parameters\n",
    "    # Convert 1D arrays to 2D array if necessary\n",
    "    if len(params.shape) == 1: \n",
    "        params = np.expand_dims(params, axis=0)\n",
    "    # create the table\n",
    "    createMatrixTable(DB_path, tname, params, overwrite=True)\n",
    "    print(f'INFO: Created table {tname}')\n",
    "\n",
    "\n",
    "print('')\n",
    "print(getTableNames(DB_path))\n",
    "\n",
    "del k, v, tname, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da48c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra tables\n",
    "keep_tables = {'sqlite_sequence', 'Header', 'Hparam_T', \n",
    "               'lstm_weight_ih_l0', 'lstm_weight_hh_l0', 'lstm_bias_ih_l0', 'lstm_bias_hh_l0', \n",
    "               'lstm_weight_ih_l1', 'lstm_weight_hh_l1', 'lstm_bias_ih_l1', 'lstm_bias_hh_l1', \n",
    "               'lstm_weight_ih_l2', 'lstm_weight_hh_l2', 'lstm_bias_ih_l2', 'lstm_bias_hh_l2', \n",
    "               'fc_weight', 'fc_bias'}\n",
    "\n",
    "all_tables = getTableNames(DB_path)\n",
    "cnt = 0\n",
    "for name in all_tables:\n",
    "    if name not in keep_tables:\n",
    "        dropTable(DB_path, name)\n",
    "        print(f'WARN: Dropped table {name}')\n",
    "        cnt += 1\n",
    "print(f'INFO: {cnt} tables dropped')\n",
    "\n",
    "all_tables = getTableNames(DB_path)\n",
    "ipd.display(all_tables)\n",
    "\n",
    "# Delete names\n",
    "del all_tables, cnt, name, keep_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72449eab",
   "metadata": {},
   "source": [
    "## Import the Saved Model and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509daa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the meta tables\n",
    "rec_list = getRecords(DB_path, 'Header')\n",
    "print('Header:')\n",
    "for r in rec_list: print(r[1:-1])\n",
    "\n",
    "print('')\n",
    "rec_list = getRecords(DB_path, 'Hparam_T')\n",
    "print('Hparam:')\n",
    "for r in rec_list: print(r[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b71bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a table saved using createMatrixTable as a list of tuples\n",
    "def readMatrixTable(db_path, table_name):\n",
    "    # read the records\n",
    "    rec_list = getRecords(db_path, table_name)\n",
    "    # build the matrix\n",
    "    rec_list.sort()         # sort by row_no (first column)\n",
    "    matrix = []\n",
    "    for rec in rec_list:\n",
    "        matrix.append(rec[1:])  # stripe off the row_no columns\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# test this functions\n",
    "mat1 = np.array(readMatrixTable(DB_path, 'fc_weight'))\n",
    "mat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the weights and biases as a dictionary\n",
    "def readModelParam(db_path, table_names):\n",
    "    model_params = {}\n",
    "    for name in table_names:\n",
    "        # read the matrix as a list of tuples\n",
    "        mat = readMatrixTable(db_path, name)\n",
    "        # Check if it is a matrix or a vector\n",
    "        if len(mat)==1: is_vector = True\n",
    "        else: is_vector = False\n",
    "        # convert to numpy array\n",
    "        if is_vector: mat = np.array(mat[0])    # make a 1D array for vectors\n",
    "        else: mat = np.array(mat)\n",
    "        # save it for returning\n",
    "        model_params[name] = mat\n",
    "    return model_params\n",
    "        \n",
    "\n",
    "# test this function\n",
    "param_tables = [v[0] for v in Weight_tables.values()]   # get the parameter table names\n",
    "\n",
    "model_params = readModelParam(DB_path, param_tables)\n",
    "for k, v in model_params.items():\n",
    "    print(f'{k}:', v.shape, v.dtype)\n",
    "    \n",
    "\n",
    "# Delete names\n",
    "del k, v, mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with original weights\n",
    "org_params = {v[0]:v[1] for v in Weight_tables.values()}    # make weight table for comparison\n",
    "\n",
    "\n",
    "def compare_model_params(model_params, org_params, tolerance):\n",
    "    for k in model_params:\n",
    "        print('\\nComparing:', k)\n",
    "        db_val = model_params[k]\n",
    "        org_val = org_params[k]\n",
    "        dmin = np.min(org_val)\n",
    "        dmax = np.max(org_val)\n",
    "        print('min:', dmin, '  max:', dmax)\n",
    "        diff_val = np.max(np.abs(db_val - org_val))   # get the maximum difference\n",
    "        print('diff:', diff_val)\n",
    "        assert np.allclose(db_val, org_val, rtol=tolerance)  # use numpy built-in check\n",
    "\n",
    "\n",
    "# compare with tolerance\n",
    "tolerance = 1e-6\n",
    "compare_model_params(model_params, org_params, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0f1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tolerance, tkey, rec_list, r, org_params, param_tables, model_params\n",
    "del hparam_records, header_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4054d",
   "metadata": {},
   "source": [
    "# Export the Dataset as sqlite3 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ce1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database file\n",
    "DB_ds_path = './saved/timit_test_data.s3db'\n",
    "createDB(DB_ds_path, overwrite=True)\n",
    "!ls -ltr saved/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bdc08b",
   "metadata": {},
   "source": [
    "## Save the Header Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d5a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table and check \n",
    "createHeaderTable(DB_ds_path)\n",
    "table_names = getTableNames(DB_ds_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3f8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the records as a list\n",
    "Label_table = 'Labels_T'\n",
    "Dataitem_table = 'DataItems_T'\n",
    "FeatureSeq_table = 'FeatureSequences_T'\n",
    "\n",
    "item = DataItems[0]\n",
    "print('item[-1] shape:', item[-1].shape)    # last field in the item is the feature-sequence\n",
    "Feature_len = item[-1].shape[1]\n",
    "\n",
    "header_records = [\n",
    "    ('name', 'TIMIT-Test', 'Feature-Sequences extracted from the test dataset of TIMIT.'),\n",
    "    ('feature_length', Feature_len, 'The length of the feature. These features can be directly fed to the MLP12 model'),\n",
    "    ('accuracy', Accuracy, 'Accuracy of the LSTM250 model used to generate the \"predicted_index\" values.'),\n",
    "    \n",
    "    ('labels.table',  Label_table, 'Index to label mapping. The model predicts an index, which can be converted to the label using this table'),\n",
    "    ('dataset.table', Dataitem_table, 'This table serves as the (label, feature) list. The actual features are stored in a separate table.'), \n",
    "    ('dataitem.schema', '', 'There are 3 label-related fields in the dataset.table: \"label\" is the ground-truth, \"label_index\" is the index into the index-to-label mapping, \"predicted_index\" is the index predicted by the trained LSTM250 model'),\n",
    "    ('feature_sequence.table', FeatureSeq_table, 'Contains the actual feature-sequences for the model.'),\n",
    "]\n",
    "\n",
    "# Insert the header records\n",
    "deleteRows(DB_ds_path, 'Header')  # delete previous records\n",
    "insertRecordList(DB_ds_path, insertHeaderRecord, header_records)\n",
    "records = getRecords(DB_ds_path, 'Header')\n",
    "for r in records: print(r[:-1])   # print all but description field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efd7cd0",
   "metadata": {},
   "source": [
    "## Save Index-to-label mapping table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625637d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the labels table\n",
    "def createLableTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = f'''CREATE TABLE IF NOT EXISTS {Label_table} (\n",
    "                    label_index INTEGER PRIMARY KEY,\n",
    "                    label TEXT\n",
    "                )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createLableTable(DB_ds_path)\n",
    "table_names = getTableNames(DB_ds_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424a0701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts a record into the labels table\n",
    "def insertLabelRecord(cursor, record):\n",
    "    label_index, label = record  # this serves as a soft check for the record format\n",
    "    # Insert the record into the table\n",
    "    query = f\"INSERT INTO {Label_table} (label_index, label) VALUES (?, ?)\"\n",
    "    cursor.execute(query, (label_index, label))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "deleteRows(DB_ds_path, Label_table)\n",
    "insertRecordList(DB_ds_path, insertLabelRecord, [(-1, 'test')])\n",
    "getRecords(DB_ds_path, Label_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a963dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the label records\n",
    "label_records = [(label_index, str(label)) for label_index, label in Index_to_label.items()]\n",
    "print('label_records:', label_records)\n",
    "\n",
    "# Store them in the table\n",
    "deleteRows(DB_ds_path, Label_table)\n",
    "insertRecordList(DB_ds_path, insertLabelRecord, label_records)\n",
    "rec_list = getRecords(DB_ds_path, Label_table)\n",
    "print('rec_list:', rec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "del table_names, records, rec_list, r, label_records, item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da063a6b",
   "metadata": {},
   "source": [
    "## Separate Data-items and Feature Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2199eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataitems for DataItem table and Features table\n",
    "dataitem_records = []\n",
    "featureSeq_records = []\n",
    "\n",
    "item = DataItems[0]\n",
    "print('item[-1].type:', type(item[-1]))\n",
    "\n",
    "for item_index, item in enumerate(DataItems):\n",
    "    label, label_index, pred_index, seq_len, feat_seq = item   # parse the item\n",
    "    seq_id = item_index      # use the index in the dataset as the feature-sequence ID\n",
    "    item_rec = [label, label_index, pred_index, seq_len, seq_id]\n",
    "    feat_rec = (seq_id, feat_seq)    # feature-record: (feature-id, feature-sequence)\n",
    "    dataitem_records.append(item_rec)\n",
    "    featureSeq_records.append(feat_rec)\n",
    "\n",
    "# check the records\n",
    "print('')\n",
    "print('dataitem_records:', len(dataitem_records), len(dataitem_records[0]))\n",
    "print('featureSeq_records:', len(featureSeq_records), featureSeq_records[0][1].shape)\n",
    "\n",
    "# Check the feature-sequences\n",
    "check_index = 100\n",
    "item = DataItems[check_index]\n",
    "npmatch = (featureSeq_records[check_index][1] == item[-1])\n",
    "assert npmatch.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "del seq_id, seq_len, pred_index, npmatch, label_index, label, item_rec\n",
    "del item_index, item, header_records, feat_rec, feat_seq, check_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc76fe6",
   "metadata": {},
   "source": [
    "## Save Data Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataset table to save the data-items\n",
    "def createDataTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Define the table name and column names\n",
    "    table_name = Dataitem_table\n",
    "    columns = [\"id INTEGER PRIMARY KEY AUTOINCREMENT\",\n",
    "               \"label TEXT\",\n",
    "               \"label_index INTEGER\",\n",
    "               \"predicted_index INTEGER\",\n",
    "               \"sequence_len INTEGER\",\n",
    "               \"sequence_id INTEGER\"]\n",
    "\n",
    "    # Create the table\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(columns)})\"\n",
    "    cursor.execute(query)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table\n",
    "createDataTable(DB_ds_path)\n",
    "table_names = getTableNames(DB_ds_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b9322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts a record into the Dataset table\n",
    "def insertDataRecord(cursor, record):\n",
    "    label, label_index, predicted_index, seq_len, seq_id = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    query = f\"INSERT INTO {Dataitem_table} (label, label_index, predicted_index, sequence_len, sequence_id) VALUES (?, ?, ?, ?, ?)\"\n",
    "    cursor.execute(query, (label, label_index, predicted_index, seq_len, seq_id))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB_ds_path, insertDataRecord, [(\"Item 1\", 1, 2, 3, 4)])\n",
    "getRecords(DB_ds_path, Dataitem_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96c16eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert all dataset records\n",
    "deleteRows(DB_ds_path, Dataitem_table)   # delete old records\n",
    "insertRecordList(DB_ds_path, insertDataRecord, dataitem_records)\n",
    "rec_list = getRecords(DB_ds_path, Dataitem_table)\n",
    "print('rec_list:', len(rec_list), len(rec_list[0]))\n",
    "print('rec_list[0]', rec_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9551cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "del table_names, rec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c9938b",
   "metadata": {},
   "source": [
    "## Save Feature Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f522b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the feature-sequence table to save the feature_sequences\n",
    "# Columns: sequence-id, row-index, col_0, col_1, ..., col_n\n",
    "# row-index is the the index of the corresponding row in the sequence.\n",
    "def createFeatureTable(db_path, table_name, sequence_records, overwrite=False):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop the table if it exists and overwrite requested\n",
    "    table_exist = existTable(db_path, table_name)\n",
    "    if overwrite and table_exist:\n",
    "        print(f'WARN: Overwriting table {table_name}')\n",
    "        dropTable(db_path, table_name)\n",
    "        \n",
    "    # Create the table\n",
    "    feat_vec = sequence_records[0][1][0]  # get a feature vector\n",
    "    cols = len(feat_vec)                  # no. of columns to create in the table\n",
    "    column_names = \"sequence_id, row_index, \" + \", \".join([f\"col_{i}\" for i in range(cols)])  # Generate the column names string\n",
    "    cursor.execute(f\"CREATE TABLE {table_name} ({column_names})\")\n",
    "\n",
    "    # Insert the featurs into the table\n",
    "    for seq_item in tqdm(sequence_records):\n",
    "        seq_id, feat_seq = seq_item\n",
    "        for row_id, feat_vec in enumerate(feat_seq):\n",
    "            vals = f'{seq_id}, {row_id}, ' + ', '.join(map(str, feat_vec))\n",
    "            cursor.execute(f\"INSERT INTO {table_name} VALUES ({vals})\")\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table\n",
    "createFeatureTable(DB_ds_path, FeatureSeq_table, featureSeq_records, overwrite=True)\n",
    "table_names = getTableNames(DB_ds_path)\n",
    "print(table_names)\n",
    "\n",
    "rec_list = getRecords(DB_ds_path, FeatureSeq_table)\n",
    "print('rec_list:', len(rec_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_item = rec_list[0]\n",
    "print('feat_item:', len(feat_item))\n",
    "type_count = {}\n",
    "for d in feat_item: \n",
    "    t = type(d)\n",
    "    if t not in type_count: type_count[t] = 0\n",
    "    type_count[t] += 1\n",
    "print(type_count)\n",
    "\n",
    "\n",
    "# Delete names\n",
    "del feat_item, type_count, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a82fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra tables ---------------\n",
    "keep_tables = {'Header', 'sqlite_sequence', 'Labels_T', 'DataItems_T', 'FeatureSequences_T'}\n",
    "\n",
    "all_tables = getTableNames(DB_ds_path)\n",
    "cnt = 0\n",
    "for name in all_tables:\n",
    "    if name not in keep_tables:\n",
    "        dropTable(DB_ds_path, name)\n",
    "        print(f'WARN: Dropped table {name}')\n",
    "        cnt += 1\n",
    "print(f'INFO: {cnt} tables dropped')\n",
    "\n",
    "all_tables = getTableNames(DB_ds_path)\n",
    "ipd.display(all_tables)\n",
    "\n",
    "# Delete names\n",
    "del all_tables, cnt, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0ff5a",
   "metadata": {},
   "source": [
    "# Import Saved Dataset and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the meta tables\n",
    "rec_list = getRecords(DB_ds_path, 'Header')\n",
    "print('Header:')\n",
    "for r in rec_list: print(r[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with original features and dataitems\n",
    "def compare_dataset(db_items, org_items, tolerance):\n",
    "    range_iter = tqdm(range(len(db_items)))\n",
    "    for i in range_iter:\n",
    "        #print(i)\n",
    "        db_rec = db_items[i]\n",
    "        org_rec = org_items[i]\n",
    "        # Compare the labels and sequence lengths\n",
    "        db_labels = tuple(db_rec[:4])\n",
    "        org_labels = tuple(org_rec[:4])\n",
    "        #print('db_rec:', db_rec)\n",
    "        #print('org_rec:', org_rec)\n",
    "        assert db_labels==org_labels, f\"Labels mismatch:\\ndb_labels: {db_labels}\\norg_labels:{org_labels}\"\n",
    "        #if i==5: break\n",
    "        # Check features\n",
    "        org_feat = np.array(org_rec[-1])\n",
    "        db_feat = np.array(db_rec[-1])\n",
    "        assert np.allclose(org_feat, db_feat, tolerance), \"Feature vector mismatch\"\n",
    "    print(f'INFO: Compared {(i+1)} records')\n",
    "\n",
    "\n",
    "# merge the tables to make similar records as in DS_loaded['dataset']\n",
    "# build a feat_id: feat_vec map for merging.\n",
    "feat_records = getRecords(DB_ds_path, FeatureSeq_table)\n",
    "feat_records.sort()    # sort by (seq-id, row-index)\n",
    "seq_rec_map = {}\n",
    "for fitem in feat_records:    # fitem: (seq-id, row-index, col_0, col_1 ...)\n",
    "    seq_id = fitem[0]\n",
    "    feat_vec = fitem[2:]\n",
    "    if seq_id not in seq_rec_map:\n",
    "        seq_rec_map[seq_id] = []\n",
    "    seq_rec_map[seq_id].append(feat_vec)\n",
    "    \n",
    "\n",
    "# merge feature vectors with dataset items for comparison\n",
    "data_records = getRecords(DB_ds_path, Dataitem_table)\n",
    "db_items = []\n",
    "for drec in data_records:\n",
    "    seq_id = drec[-1]\n",
    "    feat_seq = np.array(seq_rec_map[seq_id])\n",
    "    merged_item = list(drec[1:5]) + [feat_seq]   # remove ID column and concat feature vector\n",
    "    db_items.append(merged_item)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10189338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with tolerance\n",
    "tolerance = 1e-6\n",
    "compare_dataset(db_items, DataItems, tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9887fbd1",
   "metadata": {},
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "Now you can use these databases to translate the Numpy model into C implementations. You can also run experiments on fixed-point precisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
