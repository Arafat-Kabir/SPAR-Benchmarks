{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d1405c",
   "metadata": {},
   "source": [
    "# Training LSTM-250 for Phoneme Recognition on TIMIT\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Loading feature datasets exported by previous notebook\n",
    "- Model design\n",
    "- Dataloader design\n",
    "- Training dashboard\n",
    "- Training\n",
    "- Validate trained model and export it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c2efb",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4663f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407a33f2",
   "metadata": {},
   "source": [
    "# Loading Feature Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829d9aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_path = './session/train-features.pt'\n",
    "test_ds_path = './session/test-features.pt'\n",
    "train_feat_dict = torch.load(train_ds_path)\n",
    "test_feat_dict  = torch.load(test_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_feat_dict.keys())\n",
    "print(train_feat_dict['note'])\n",
    "print(train_feat_dict['data-schema'])\n",
    "\n",
    "Train_feat = train_feat_dict['data']\n",
    "print('Train_feat:', len(Train_feat))\n",
    "print('Feature-len:', len(Train_feat[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_feat_dict.keys())\n",
    "print(test_feat_dict['note'])\n",
    "print('data-schema:', test_feat_dict['data-schema'])\n",
    "\n",
    "Test_feat = test_feat_dict['data']\n",
    "Feature_length = len(Test_feat[0][1])\n",
    "print('Train_feat:', len(Test_feat))\n",
    "print('Feature-len:', Feature_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242f7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete redundant names to avoid confusions\n",
    "del train_ds_path, test_ds_path, train_feat_dict, test_feat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30263a6e",
   "metadata": {},
   "source": [
    "## Show Some Data points\n",
    "\n",
    "**NOTE:**\n",
    "- Observe that the feature sequence layout is (feature-vector, sequence-point)\n",
    "- For LSTM training, it needs to be transposed to make it (sequence-point, feature-vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab93e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an item from Train/Test_feat, shows some information\n",
    "# feat_item: (phone, feature-sequence)\n",
    "def showFeatInfo(feat_item, end=''):    \n",
    "    print('item type:', type(feat_item))\n",
    "    phone, feat_seq = feat_item\n",
    "    print('phone:', phone)\n",
    "    print('feat-seq type:', type(feat_seq), feat_seq.dtype)\n",
    "    print('feat_seq:', feat_seq.shape, end)\n",
    "\n",
    "\n",
    "# show a few items\n",
    "showFeatInfo(Train_feat[0], end='\\n')\n",
    "showFeatInfo(Train_feat[3], end='\\n')\n",
    "\n",
    "showFeatInfo(Test_feat[0], end='\\n')\n",
    "showFeatInfo(Test_feat[3], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5625c7b",
   "metadata": {},
   "source": [
    "## Convert to float32\n",
    "\n",
    "Convert feature datatypes to float32. Some features are set as float64 in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c517396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a feature dataset, returns all dtypes of the feature-sequences\n",
    "def getAllDtype(feat_ds):\n",
    "    all_type = set()\n",
    "    for item in feat_ds:\n",
    "        _, feat_seq = item\n",
    "        all_type.add(feat_seq.dtype)\n",
    "    return all_type\n",
    "\n",
    "\n",
    "train_dtypes = getAllDtype(Train_feat)\n",
    "test_dtypes  = getAllDtype(Test_feat)\n",
    "print('train_dtypes:', train_dtypes)\n",
    "print('test_dtypes :', test_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01080cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a feature dataset, converts all feature-sequence into target datatype\n",
    "def changeDtype(feat_ds, target_dtype):\n",
    "    for item in feat_ds:\n",
    "        item[1] = item[1].astype(target_dtype)\n",
    "        \n",
    "        \n",
    "# Convert datatypes\n",
    "changeDtype(Train_feat, np.float32)\n",
    "changeDtype(Test_feat, np.float32)\n",
    "\n",
    "# Show after conversion\n",
    "train_dtypes = getAllDtype(Train_feat)\n",
    "test_dtypes  = getAllDtype(Test_feat)\n",
    "print('train_dtypes:', train_dtypes)\n",
    "print('test_dtypes :', test_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a986091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete names to avoid confusion\n",
    "del train_dtypes, test_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b3e0f",
   "metadata": {},
   "source": [
    "## Transpose Feature Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f14206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a feature dataset, transposes the feature sequences\n",
    "def transposeFeatSeq(feat_ds):\n",
    "    for item in feat_ds:\n",
    "        item[1] = item[1].T\n",
    "        \n",
    "        \n",
    "# Transpose the feature datasets\n",
    "transposeFeatSeq(Train_feat)\n",
    "transposeFeatSeq(Test_feat)\n",
    "\n",
    "# show a few item info\n",
    "showFeatInfo(Train_feat[0], end='\\n')\n",
    "showFeatInfo(Train_feat[3], end='\\n')\n",
    "\n",
    "showFeatInfo(Test_feat[0], end='\\n')\n",
    "showFeatInfo(Test_feat[3], end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306021ac",
   "metadata": {},
   "source": [
    "## Load the Label-to-Index Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7544da",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = './session/label-to-index.pt'\n",
    "Label_to_index = torch.load(label_path)\n",
    "Index_to_label = {index:label for label, index in Label_to_index.items()}\n",
    "\n",
    "print('Label_to_index:\\n', Label_to_index)\n",
    "print('Index_to_label:\\n', Index_to_label)\n",
    "\n",
    "\n",
    "# Delete names to avoid confusions in later sections\n",
    "del label_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476e6f3d",
   "metadata": {},
   "source": [
    "# Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a770a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "\n",
    "# LSTM model definition\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.debug = False    # Set it to true to print debug info\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    # Expects a padded_sequence of batched input and the lengths of the sequences\n",
    "    def forward(self, pad_seq, lengths):        \n",
    "        if self.debug: print('DEBUG START: LSTM model ---')\n",
    "\n",
    "        # Extract batch size for initialization of hidden state\n",
    "        batch_size = len(pad_seq)\n",
    "            \n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        # Convert padded sequence to variable length packed sequence for LSTM\n",
    "        packed_seq = rnn_utils.pack_padded_sequence(pad_seq, lengths, enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        # Forward propagate LSTM, returns a packed sequence\n",
    "        out_packed, _ = self.lstm(packed_seq, (h0, c0))\n",
    "        \n",
    "        # Extract final hidden states of each sequence for the output layer\n",
    "        out_pad, out_lens = rnn_utils.pad_packed_sequence(out_packed, batch_first=True)\n",
    "        out_indx = out_lens - 1   # indices of the last valid hidden state in the padded sequence\n",
    "        last_hidden = out_pad[range(batch_size), out_indx].contiguous()  # select the last valid state in each sequence, and make them contiguous for efficiency\n",
    "                \n",
    "        if self.debug:\n",
    "            print('last_hidden size:', last_hidden.size())\n",
    "            print('last_hidden:\\n', last_hidden)\n",
    "        \n",
    "        # Decode the hidden state of the last time step only (for whole batch)\n",
    "        out = self.fc(last_hidden)\n",
    "        if self.debug: print('DEBUG END: LSTM model ---')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the LSTM\n",
    "input_size  = Feature_length    # no. of mfcc coefficients\n",
    "hidden_size = 250               # no. of hiddent units in LSTM\n",
    "num_layers  = 3\n",
    "num_classes = len(Label_to_index)\n",
    "model_lstm250  = LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "model_lstm250.to(device)\n",
    "print(model_lstm250)\n",
    "\n",
    "\n",
    "# Save the model parameters\n",
    "Hparam = {\n",
    "    'input_size' : input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers' : num_layers,\n",
    "    'num_classes': num_classes, \n",
    "}\n",
    "\n",
    "\n",
    "# Delete temporary names\n",
    "del input_size, hidden_size, num_layers, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda3918",
   "metadata": {},
   "source": [
    "# Dataloader Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d737d2",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729aeda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use librosa.display.specshow to display 2D features\n",
    "def plotSpecshow(data, fig, axis):\n",
    "    img = librosa.display.specshow(data, ax=axis)\n",
    "    fig.colorbar(img, ax=axis)\n",
    "    return img\n",
    "\n",
    "\n",
    "# Given a feature-sequence, breaks down different parts then plots it\n",
    "def showFeatures(feature_sequence):    # feature_sequence: (sequence-point, feature-vector)\n",
    "    feat_seq = feature_sequence.T      # Transpose it to make it the way plotSpecshow() expects\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    mfcc = feat_seq[0:40, :]     # extract the mfcc coefficients only\n",
    "    ax_mfcc = fig.add_subplot(2, 2, 1)\n",
    "    ax_mfcc.set_title('MFCC')\n",
    "    plotSpecshow(mfcc, fig, ax_mfcc)\n",
    "\n",
    "    energy = feat_seq[40, :]     # extract energy only\n",
    "    ax_energy = fig.add_subplot(2, 2, 2)\n",
    "    ax_energy.set_title('Energy')\n",
    "    ax_energy.semilogy(energy)\n",
    "    \n",
    "    delta1 = feat_seq[41:82, :]  # extract delta1 only\n",
    "    ax_delta1 = fig.add_subplot(2, 2, 3)\n",
    "    ax_delta1.set_title('Delta-1')\n",
    "    plotSpecshow(delta1, fig, ax_delta1)\n",
    "\n",
    "    delta2 = feat_seq[82:, :]    # extract delta2 only\n",
    "    ax_delta2 = fig.add_subplot(2, 2, 4)\n",
    "    ax_delta2.set_title('Delta-2')\n",
    "    plotSpecshow(delta2, fig, ax_delta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2dcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TimitPhoneDataset(Dataset):\n",
    "    def __init__(self, feature_ds):\n",
    "        self.base_dataset = feature_ds    # list of (phoneme, feature-sequence)\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    " \n",
    "    # Returns: (phoneme, feature-sequence)\n",
    "    def __getitem__(self, index):\n",
    "        return self.base_dataset[index]\n",
    "         \n",
    "    \n",
    "# Instantiate the Dataset objects\n",
    "#Train_dataset = TimitPhoneDataset(Train_feat)\n",
    "Train_dataset = TimitPhoneDataset(Train_feat + Test_feat)\n",
    "Test_dataset  = TimitPhoneDataset(Test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if it works as expected\n",
    "phone, feat_seq = Test_dataset[23]\n",
    "print('phone:', phone)\n",
    "print('feat_seq:', feat_seq.shape)\n",
    "showFeatures(feat_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone, feat_seq = Train_dataset[1050]\n",
    "print('phone:', phone)\n",
    "print('feat_seq:', feat_seq.shape)\n",
    "showFeatures(feat_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0bd383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary names\n",
    "del phone, feat_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d527f2",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d4cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding is needed to make the batch <tensor> from <list> of variable length sequences\n",
    "# The padding values are not passed to the LSTM during trainig/testing\n",
    "def pad_sequence_lstm(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch\n",
    "\n",
    "# Gets the list of audio and labels as batch then\n",
    "# converts them into sequence of features for the model.\n",
    "# Adds padding to build the batch tensor\n",
    "def collate_fn_lstm(batch):\n",
    "    # A data tuple has the form: (phoneme, wave, sample_rate)\n",
    "    tensors, targets, lengths = [], [], []   # lengths is needed for pack_padded_sequence  in LSTM.forward()\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for item in batch:\n",
    "        label, feat_seq = item\n",
    "        feat_seq_tensor = torch.from_numpy(feat_seq)\n",
    "        tensors += [feat_seq_tensor]\n",
    "        targets += [Label_to_index[label]]\n",
    "        lengths.append(feat_seq_tensor.size()[0])\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence_lstm(tensors)\n",
    "    targets = torch.tensor(targets)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    return tensors, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad0389f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test padding function\n",
    "batch = [   torch.tensor([2,3,4,5]), \n",
    "            torch.tensor([2,3]),\n",
    "            torch.tensor([5,6,7])    ]\n",
    "pad_seq = pad_sequence_lstm(batch)\n",
    "print('pad_seq:\\n', pad_seq)\n",
    "\n",
    "\n",
    "# Test collate function\n",
    "batch = [Train_dataset[i*21] for i in range(5)]\n",
    "col_tensor, col_targets, col_lengths = collate_fn_lstm(batch)\n",
    "print('\\ncollate_out:\\n', col_tensor.size(), '\\n', col_targets, '\\n', col_lengths)\n",
    "\n",
    "\n",
    "# Manual features to compare\n",
    "print('')\n",
    "phone, feat_seq = batch[2]\n",
    "print('phone:', phone)\n",
    "print('feat_seq shape:', feat_seq.shape)\n",
    "\n",
    "\n",
    "# Delete names to avoid confusion later\n",
    "del batch, pad_seq\n",
    "del col_tensor, col_targets, col_lengths\n",
    "del phone, feat_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07520fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Dataloaders\n",
    "\n",
    "Batch_size_train = 64\n",
    "Batch_size_test = 256\n",
    "\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "    \n",
    "Train_loader = torch.utils.data.DataLoader(\n",
    "    Train_dataset,\n",
    "    batch_size=Batch_size_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_lstm,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "Test_loader = torch.utils.data.DataLoader(\n",
    "    Test_dataset,\n",
    "    batch_size=Batch_size_test,\n",
    "    shuffle=False,\n",
    "    #shuffle=True,        # make it True for fractional validation\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn_lstm,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bdeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataloaders\n",
    "for (sequence, labels, lengths) in Train_loader:\n",
    "    print('sequence:', sequence.size(), 'type:', sequence.type())\n",
    "    print('labels  :', labels.size(), 'type:', labels.type())\n",
    "    print('lengths :', lengths.size(), 'type:', lengths.type())\n",
    "    break\n",
    "    \n",
    "print('')\n",
    "for (sequence, labels, lengths) in Test_loader:\n",
    "    print('sequence:', sequence.size(), 'type:', sequence.type())\n",
    "    print('labels  :', labels.size(), 'type:', labels.type())\n",
    "    print('lengths :', lengths.size(), 'type:', lengths.type())\n",
    "    break\n",
    "    \n",
    "    \n",
    "# Delete names\n",
    "del num_workers, pin_memory, sequence, labels, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd986ca",
   "metadata": {},
   "source": [
    "# Setting up Training Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02438dc1",
   "metadata": {},
   "source": [
    "## Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0accc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "# The training method should not access global names to avoid confusions.\n",
    "# The only global name allowed is the Train_loader.\n",
    "# options and dashboard arguments are used to pass other global elements. \n",
    "def train_model(model, options, dashboard, debug=False):\n",
    "    # Get options and dashboard elements\n",
    "    criterion  = options['criterion']\n",
    "    optimizer  = options['optimizer']\n",
    "    log_interval = options['log_interval']\n",
    "    epoch_pbar   = dashboard['epoch_pbar']\n",
    "    \n",
    "    # Training initial setup\n",
    "    model.train()\n",
    "    model.debug = debug\n",
    "    epoch_pbar.reset()\n",
    "    \n",
    "    # Run the training loop\n",
    "    for batch_idx, (sequence, target, lengths) in enumerate(Train_loader):\n",
    "        sequence = sequence.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Run through model and compute loss\n",
    "        output = model(sequence, lengths)\n",
    "        loss = criterion(output, target)    # compute batch loss\n",
    "\n",
    "        # Update gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update dashboard and print at log intervals\n",
    "        epoch_pbar.update(1)   # update the epoch progress bar\n",
    "        is_log_step = (batch_idx % log_interval == 0)\n",
    "        if is_log_step: updateTrainingStat( options, dashboard, batch_idx, \n",
    "                                            len(sequence), len(Train_loader), loss )    \n",
    "        # DEBUG\n",
    "        if debug and batch_idx == 100: \n",
    "            print('DBG: Breaking prematurely')\n",
    "            break\n",
    "    model.debug = False\n",
    "    \n",
    "    \n",
    "# Update training status\n",
    "def updateTrainingStat(options, dashboard, batch_idx, sequence_len, Train_len, loss):\n",
    "    # Get options and dashboard elements\n",
    "    print_stat = options['print_stat']       # a true/false value\n",
    "    cur_epoch  = options['cur_epoch']        # current epoch number\n",
    "    train_loss = options['train_loss']       # a list to keep track of training loss, passed from top-level\n",
    "    plot_fig_ax = dashboard['plot_fig_ax']  # figure and axis handles for plotting\n",
    "    \n",
    "    # printing status (for short training sessions)\n",
    "    if print_stat:\n",
    "        printTrainingStat(cur_epoch, batch_idx, sequence_len, Train_len, loss)\n",
    "\n",
    "    # record and plot loss\n",
    "    train_loss.append(loss.item())\n",
    "    if plot_fig_ax:\n",
    "        fig, ax = plot_fig_ax\n",
    "        ax.plot(train_loss, color='b')\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        \n",
    "# Prints the training stats\n",
    "def printTrainingStat(cur_epoch, batch_idx, sequence_len, Train_len, loss):\n",
    "    print('Train Epoch: {} [{:6}/{} ({:2.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                      cur_epoch, batch_idx * sequence_len, Train_len,\n",
    "                      100. * batch_idx / Train_len, loss.data.item()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab9d3ee",
   "metadata": {},
   "source": [
    "## Testing Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66e6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "# Returns percent accuracy\n",
    "# User fraction: <1.0 to run validation on a fraction of the test data\n",
    "def test_model(model, options, debug=False):\n",
    "    # Get options\n",
    "    criterion = options['criterion']\n",
    "    fraction  = options['test_fraction']   # <= 1.0\n",
    "    print_stat = options['print_stat']     # true/false value\n",
    "    \n",
    "    # Testing initial setup\n",
    "    model.eval()\n",
    "    model.debug = debug\n",
    "    # Compute the stop batch no. based on fraction\n",
    "    total_batch = ceil(len(Test_dataset)/Batch_size_test)\n",
    "    stop_count = int(ceil(total_batch*fraction))\n",
    "    \n",
    "    # Run the test dataset through the model\n",
    "    loss, correct = 0, 0\n",
    "    tested_count = 0\n",
    "    for sequence, targets, lengths in Test_loader:\n",
    "        sequence = sequence.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(sequence, lengths)\n",
    "        loss += criterion(outputs, targets).data.item()\n",
    "        pred = get_likely_index(outputs) # get the index of the max log-probability\n",
    "        correct += number_of_correct(pred, targets)\n",
    "        tested_count += len(targets)  # increment the tested item counter\n",
    "        # Run only on the given fraction\n",
    "        stop_count -= 1\n",
    "        if stop_count < 0: break\n",
    "    model.debug = False\n",
    "    \n",
    "    # Print statistics\n",
    "    loss /= tested_count\n",
    "    test_loss.append(loss)\n",
    "    accuracy = (100.0 * correct) / tested_count\n",
    "    test_accuracy.append(accuracy)\n",
    "    \n",
    "    if print_stat:\n",
    "        print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                 loss, correct, tested_count, accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648887ee",
   "metadata": {},
   "source": [
    "## Training Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2264215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from ipywidgets.widgets import HTML\n",
    "import IPython.display as ipd\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Make a widget to show status text at the top of the cell\n",
    "status_text = HTML()\n",
    "status_init = \"<b>Status:</b> Start the training\"\n",
    "status_start = \"<b>Status:</b> Training started ...\"\n",
    "status_end   = \"<br><b style='color:green'>Training Done!</b>\"\n",
    "\n",
    "\n",
    "# Given the parameters, updates the status_text widget\n",
    "def updateStatusText(target_models, cur_accuracy, best_accuracy, learn_rate):\n",
    "    param_style = 'style=\"color:indianred\"'\n",
    "    text = f'<b>Status:</b> target-model #: <b {param_style}>{{}}</b> cur-acc: <b {param_style}>{{:.2f}}%</b>   best-acc: <b {param_style}>{{:.2f}}%</b> lr: <b {param_style}>{{}}</b>'\n",
    "    status_text.value = text.format(len(target_models), cur_accuracy, best_accuracy, learn_rate)\n",
    "\n",
    "\n",
    "# Call this function where you want to show the interactive plot\n",
    "fig_train = None\n",
    "ax_trainloss = None\n",
    "ax_testloss = None\n",
    "ax_testaccu = None\n",
    "  \n",
    "def showTrainPlot_2row():\n",
    "    global fig_train, ax_trainloss, ax_testloss, ax_testaccu\n",
    "    fig_height = 4.5\n",
    "    fig_train = plt.figure(\"Training Progress\", figsize=(2*fig_height, fig_height))\n",
    "    # Training loss axis\n",
    "    ax_trainloss = fig_train.add_subplot(2, 1, 1)\n",
    "    ax_trainloss.set_title(\"Training Loss\")\n",
    "    # Test (validation) loss axis\n",
    "    ax_testloss = fig_train.add_subplot(2, 2, 3)\n",
    "    ax_testloss.set_title(\"Test Loss\")\n",
    "    # Test (validation) accuracy axis\n",
    "    ax_testaccu = fig_train.add_subplot(2, 2, 4)\n",
    "    ax_testaccu.set_title(\"Test Accuracy\")\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig_train.tight_layout()\n",
    "\n",
    "    \n",
    "# Call this function to update the plot\n",
    "def updateTrainPlot(test_loss, test_accuracy):\n",
    "    ax_testloss.plot(test_loss, color='g')\n",
    "    ax_testaccu.plot(test_accuracy, color='g')\n",
    "    fig_train.canvas.draw()\n",
    "\n",
    "\n",
    "# Define the container to save the best models\n",
    "Saved_models = {-1:'Dummy'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9604b",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e3eb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion   = nn.CrossEntropyLoss()\n",
    "#optimizer   = optim.SGD(model_lstm250.parameters(), lr=0.001, momentum=0.5)\n",
    "optimizer   = optim.Adam(model_lstm250.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "lrScheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # reduce the learning after given steps by a factor of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c712c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop parameters\n",
    "n_epoch = 1\n",
    "target_accuracy = 75.0  # percent\n",
    "train_print_stat = False\n",
    "test_print_stat = False\n",
    "test_fraction = 1.0   # run validation on random fraction of the test dataset\n",
    "log_interval_percent = 20\n",
    "log_interval = (len(Train_dataset)//Batch_size_train) * log_interval_percent // 100\n",
    "\n",
    "\n",
    "# Tracking variables\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "target_models = {}\n",
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "\n",
    "# Set up training dashboard\n",
    "epoch_iter = tqdm(range(n_epoch), desc=\"Full Training\")\n",
    "total_train_batch = ceil(len(Train_dataset)/Batch_size_train)\n",
    "epoch_pbar = tqdm(total=total_train_batch, desc='This Epoch  ')\n",
    "status_text.value = status_init\n",
    "ipd.display(status_text)\n",
    "#ipd.display(substatus_text)\n",
    "showTrainPlot_2row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ae6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Prepare options and dashboard dictionaries\n",
    "options = {\n",
    "    'criterion'    : criterion,\n",
    "    'optimizer'    : optimizer,\n",
    "    'log_interval' : log_interval,\n",
    "    'print_stat'   : False,         # Set it to False to stop printing during training and validation\n",
    "    'cur_epoch'    : None,          # will be updated in the loop\n",
    "    'train_loss'   : train_loss,    # reference to a list\n",
    "    'test_fraction': test_fraction,\n",
    "}\n",
    "\n",
    "dashboard = {\n",
    "    'epoch_pbar'  : epoch_pbar,\n",
    "    'plot_fig_ax' : (fig_train, ax_trainloss),\n",
    "    'status_text' : status_text,\n",
    "}\n",
    "\n",
    "\n",
    "# Train the model and save the ones with accuracy >= target_accuracy\n",
    "status_text.value = status_start\n",
    "for epoch in epoch_iter:\n",
    "    # Training and testing\n",
    "    options['cur_epoch'] = epoch\n",
    "    train_model(model_lstm250, options, dashboard, debug=False)\n",
    "    status_text.value += \" -- Running Validation ... \"\n",
    "    accuracy = test_model(model_lstm250, options, debug=False)\n",
    "    lrScheduler.step()\n",
    "    status_text.value += \"done.\"\n",
    "    \n",
    "    # Save models achieving target accuracy\n",
    "    if accuracy >= target_accuracy:\n",
    "        accuracy = round(accuracy, 4)   # to reduce the key granularity\n",
    "        target_models[accuracy] = deepcopy(model_lstm250.state_dict())\n",
    "    \n",
    "    # Save the best model and update the status text\n",
    "    if accuracy > best_accuracy: \n",
    "        best_accuracy = accuracy\n",
    "        best_model = deepcopy(model_lstm250.state_dict())\n",
    "    learn_rate = optimizer.param_groups[0][\"lr\"]\n",
    "    updateStatusText(target_models, accuracy, best_accuracy, learn_rate)\n",
    "    updateTrainPlot(test_loss, test_accuracy)\n",
    "\n",
    "status_text.value += status_end\n",
    "\n",
    "\n",
    "# Print and save the best performing models, and show the training summary\n",
    "summary = []\n",
    "cnt = len(target_models)\n",
    "summary.append(f'Target met by: {cnt}')\n",
    "if cnt > 0: \n",
    "    summary.append('Saving target_models')\n",
    "    Saved_models.update(target_models)  # copy the target_models into the Saved_models\n",
    "summary.append(f'Saved_models#: {len(Saved_models)}')\n",
    "summary.append(f'Saved max acc: {max(Saved_models)}%')\n",
    "summary.append(f'Best in this iter: {best_accuracy:.2f}%')\n",
    "summary.append(f'Last learning rate: {optimizer.param_groups[0][\"lr\"]}')\n",
    "print('\\n'.join(summary))\n",
    "status_text.value += '<br>' + '<br>'.join(summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4977670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the best model\n",
    "saved_acc = max(Saved_models)\n",
    "if saved_acc > best_accuracy:\n",
    "    print(f'Loading from Saved models, accuracy: {saved_acc:.2f}%')\n",
    "    best_model_dict = Saved_models[saved_acc]\n",
    "else:\n",
    "    print(f'Loading from last training session, validation accuracy: {best_accuracy:.2f}%')\n",
    "    best_model_dict = best_model\n",
    "    \n",
    "model_lstm250.load_state_dict(best_model_dict)\n",
    "\n",
    "\n",
    "# Run the model on the entire test dataset\n",
    "options['test_fraction'] = 1\n",
    "accuracy = test_model(model_lstm250, options, debug=False)\n",
    "print(f'Accuracy on whole test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a160a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete names\n",
    "del criterion, optimizer, lrScheduler\n",
    "del options, dashboard\n",
    "del status_end, status_init, status_start, status_text, summary\n",
    "del epoch, n_epoch, epoch_iter, epoch_pbar\n",
    "del target_accuracy, target_models, test_accuracy, test_fraction, test_loss, test_print_stat\n",
    "del log_interval, log_interval_percent, \n",
    "del fig_train, ax_testaccu, ax_testloss, ax_trainloss\n",
    "del best_model, best_model_dict, best_accuracy, saved_acc\n",
    "del accuracy, number_of_correct, cnt\n",
    "del total_train_batch, train_loss, train_print_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab5de8",
   "metadata": {},
   "source": [
    "# Validate and Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46716de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the prediction\n",
    "def predict(feature_item):  # item: an item in the dataset\n",
    "    model_lstm250.eval()\n",
    "    # Extract features\n",
    "    batch = [feature_item]   # make a batch with single example\n",
    "    tensor, target, lengths = collate_fn_lstm(batch)\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model_lstm250(tensor, lengths)\n",
    "    pred = get_likely_index(output)[0]   # indexing to get the prediction from batch    \n",
    "    return pred\n",
    "\n",
    "\n",
    "# Run a prediction\n",
    "select_index = 104\n",
    "item = Test_dataset[select_index]\n",
    "pred_index = predict(item)\n",
    "pred_label  = Index_to_label[pred_index.item()]\n",
    "phone, feat_seq = item\n",
    "print(f\"Expected: {phone}. Predicted: {pred_label}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bbffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the entire test dataset\n",
    "correct_count = 0\n",
    "for item in tqdm(Test_feat):\n",
    "    pred_index = predict(item)\n",
    "    label, *_ = item\n",
    "    pred_label  = Index_to_label[pred_index.item()]\n",
    "    if pred_label==label: correct_count += 1\n",
    "\n",
    "accuracy = (100.0 * correct_count) / len(Test_dataset)\n",
    "print('correct_count:', correct_count)\n",
    "print(f'accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './session/trained-lstm250.pt'\n",
    "\n",
    "export_dict = {\n",
    "    'accuracy' : accuracy,\n",
    "    'correct_count' : correct_count,\n",
    "    'index_to_label' : Index_to_label,\n",
    "    'Hparam' : Hparam,\n",
    "    'state_dict' : model_lstm250.state_dict()\n",
    "}\n",
    "\n",
    "torch.save(export_dict, save_path)\n",
    "!ls -ltrh ./session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2d4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Test dataset\n",
    "save_path = './session/Test_feat.pt'\n",
    "torch.save(Test_feat, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
