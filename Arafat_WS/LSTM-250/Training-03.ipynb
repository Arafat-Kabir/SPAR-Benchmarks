{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64987c19",
   "metadata": {},
   "source": [
    "# Training LSTM-250 for Phoneme Recognition on TIMIT\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Loading feature datasets exported by previous notebook\n",
    "- Model design\n",
    "- Dataloader design\n",
    "- Training dashboard\n",
    "- Training\n",
    "- Validate trained model and export it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2638786d",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1931b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import librosa\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc311c",
   "metadata": {},
   "source": [
    "# Loading Feature Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_path = './session/train-norm-features-v1.pt'\n",
    "test_ds_path = './session/test-norm-features-v1.pt'\n",
    "\n",
    "train_feat_dict = torch.load(train_ds_path)\n",
    "test_feat_dict  = torch.load(test_ds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0d71bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_feat_dict.keys())\n",
    "print(train_feat_dict['note'])\n",
    "print(train_feat_dict['data-schema'])\n",
    "\n",
    "Train_feat = train_feat_dict['data']\n",
    "print('Train_feat:', len(Train_feat))\n",
    "print('Feature-len:', len(Train_feat[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1da487",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_feat_dict.keys())\n",
    "print(test_feat_dict['note'])\n",
    "print('data-schema:', test_feat_dict['data-schema'])\n",
    "\n",
    "Test_feat = test_feat_dict['data']\n",
    "Feature_length = len(Test_feat[0][1])\n",
    "print('Train_feat:', len(Test_feat))\n",
    "print('Feature-len:', Feature_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9fa231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load augmented dataset if needed\n",
    "Use_augmentation = False\n",
    "\n",
    "if Use_augmentation: \n",
    "    train_aug01_path = './session/train-norm-aug03-features.pt'\n",
    "    train_aug1_feat_dict = torch.load(train_aug01_path)\n",
    "    print(train_aug1_feat_dict.keys())\n",
    "    print(train_aug1_feat_dict['note'])\n",
    "    print(train_aug1_feat_dict['data-schema'])\n",
    "\n",
    "    Train_aug_feat = train_aug1_feat_dict['data']  #+ train_aug2_feat_dict['data']\n",
    "    print('Train_aug_feat:', len(Train_aug_feat))\n",
    "    print('Feature-len:', len(Train_aug_feat[0][1]))\n",
    "else:\n",
    "    print('Not loading augmented dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7bb5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete redundant names to avoid confusions\n",
    "del train_ds_path, test_ds_path, train_feat_dict, test_feat_dict\n",
    "if Use_augmentation: del train_aug01_path, train_aug1_feat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a1906a",
   "metadata": {},
   "source": [
    "## Show Some Data points\n",
    "\n",
    "**NOTE:**\n",
    "- Observe that the feature sequence layout is (feature-vector, sequence-point)\n",
    "- For LSTM training, it needs to be transposed to make it (sequence-point, feature-vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bff8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an item from Train/Test_feat, shows some information\n",
    "# feat_item: (phone, feature-sequence)\n",
    "def showFeatInfo(feat_item, end=''):    \n",
    "    print('item type:', type(feat_item))\n",
    "    phone, feat_seq = feat_item\n",
    "    print('phone:', phone)\n",
    "    print('feat-seq type:', type(feat_seq), feat_seq.dtype)\n",
    "    print('feat_seq:', feat_seq.shape, end)\n",
    "\n",
    "\n",
    "# show a few items\n",
    "showFeatInfo(Train_feat[0], end='\\n')\n",
    "showFeatInfo(Train_feat[3], end='\\n')\n",
    "\n",
    "showFeatInfo(Test_feat[0], end='\\n')\n",
    "showFeatInfo(Test_feat[3], end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ebb364",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Use_augmentation:\n",
    "    showFeatInfo(Train_aug_feat[0], end='\\n')\n",
    "    showFeatInfo(Train_aug_feat[3], end='\\n')\n",
    "else:\n",
    "    print('Not using augmeted dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696b4f6",
   "metadata": {},
   "source": [
    "## Convert to float32\n",
    "\n",
    "Convert feature datatypes to float32. Some features are set as float64 in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6846e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a feature dataset, returns all dtypes of the feature-sequences\n",
    "def getAllDtype(feat_ds):\n",
    "    all_type = set()\n",
    "    for item in feat_ds:\n",
    "        _, feat_seq = item\n",
    "        all_type.add(feat_seq.dtype)\n",
    "    return all_type\n",
    "\n",
    "\n",
    "train_dtypes = getAllDtype(Train_feat)\n",
    "test_dtypes  = getAllDtype(Test_feat)\n",
    "print('train_dtypes    :', train_dtypes)\n",
    "print('test_dtypes     :', test_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0948a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a feature dataset, converts all feature-sequence into target datatype\n",
    "def changeDtype(feat_ds, target_dtype):\n",
    "    for item in feat_ds:\n",
    "        item[1] = item[1].astype(target_dtype)\n",
    "        \n",
    "        \n",
    "# Convert datatypes\n",
    "changeDtype(Train_feat, np.float32)\n",
    "changeDtype(Test_feat, np.float32)\n",
    "\n",
    "# Show after conversion\n",
    "train_dtypes = getAllDtype(Train_feat)\n",
    "test_dtypes  = getAllDtype(Test_feat)\n",
    "print('train_dtypes:', train_dtypes)\n",
    "print('test_dtypes :', test_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3211d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert augmented dataset\n",
    "if Use_augmentation:\n",
    "    train_aug_dtypes = getAllDtype(Train_aug_feat)\n",
    "    print('train_aug_dtypes before:', train_aug_dtypes)\n",
    "    changeDtype(Train_aug_feat, np.float32)\n",
    "    train_aug_dtypes = getAllDtype(Train_aug_feat)\n",
    "    print('train_aug_dtypes after :', train_aug_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b24ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete names to avoid confusion\n",
    "del train_dtypes, test_dtypes\n",
    "if Use_augmentation: del train_aug_dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3fce77",
   "metadata": {},
   "source": [
    "## Transpose Feature Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a394b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a feature dataset, transposes the feature sequences\n",
    "def transposeFeatSeq(feat_ds):\n",
    "    for item in feat_ds:\n",
    "        item[1] = item[1].T\n",
    "        \n",
    "        \n",
    "# Transpose the feature datasets\n",
    "transposeFeatSeq(Train_feat)\n",
    "transposeFeatSeq(Test_feat)\n",
    "\n",
    "# show a few item info\n",
    "showFeatInfo(Train_feat[0], end='\\n')\n",
    "showFeatInfo(Train_feat[3], end='\\n')\n",
    "\n",
    "showFeatInfo(Test_feat[0], end='\\n')\n",
    "showFeatInfo(Test_feat[3], end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613d62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if Use_augmentation:\n",
    "    transposeFeatSeq(Train_aug_feat)\n",
    "    showFeatInfo(Train_aug_feat[0], end='\\n')\n",
    "    showFeatInfo(Train_aug_feat[3], end='\\n')\n",
    "else:\n",
    "    print(\"Not using augmeted dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b679a",
   "metadata": {},
   "source": [
    "## Load the Label-to-Index Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3b7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = './session/label-to-index-v1.pt'\n",
    "Label_to_index = torch.load(label_path)\n",
    "Index_to_label = {index:label for label, index in Label_to_index.items()}\n",
    "\n",
    "print('Label_to_index:\\n', Label_to_index)\n",
    "print('Index_to_label:\\n', Index_to_label)\n",
    "\n",
    "\n",
    "# Delete names to avoid confusions in later sections\n",
    "del label_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbda8f",
   "metadata": {},
   "source": [
    "# Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff961b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "\n",
    "# LSTM model definition\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.debug = False    # Set it to true to print debug info\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        #self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    # Expects a padded_sequence of batched input and the lengths of the sequences\n",
    "    def forward(self, pad_seq, lengths):        \n",
    "        if self.debug: print('DEBUG START: LSTM model ---')\n",
    "\n",
    "        # Extract batch size for initialization of hidden state\n",
    "        batch_size = len(pad_seq)\n",
    "            \n",
    "        # Set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        \n",
    "        # Convert padded sequence to variable length packed sequence for LSTM\n",
    "        packed_seq = rnn_utils.pack_padded_sequence(pad_seq, lengths, enforce_sorted=False, batch_first=True)\n",
    "        \n",
    "        # Forward propagate LSTM, returns a packed sequence\n",
    "        out_packed, _ = self.lstm(packed_seq, (h0, c0))\n",
    "        \n",
    "        # Extract final hidden states of each sequence for the output layer\n",
    "        out_pad, out_lens = rnn_utils.pad_packed_sequence(out_packed, batch_first=True)\n",
    "        out_indx = out_lens - 1   # indices of the last valid hidden state in the padded sequence\n",
    "        last_hidden = out_pad[range(batch_size), out_indx].contiguous()  # select the last valid state in each sequence, and make them contiguous for efficiency\n",
    "                \n",
    "        if self.debug:\n",
    "            print('last_hidden size:', last_hidden.size())\n",
    "            print('last_hidden:\\n', last_hidden)\n",
    "        \n",
    "        # Decode the hidden state of the last time step only (for whole batch)\n",
    "        out = self.fc(last_hidden)\n",
    "        if self.debug: print('DEBUG END: LSTM model ---')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control the random initialization\n",
    "Seed_model = 17    # seeds: 97, 17, 821\n",
    "random.seed(Seed_model)    \n",
    "\n",
    "\n",
    "# Instantiate the LSTM\n",
    "input_size  = Feature_length    # no. of mfcc coefficients\n",
    "hidden_size = 250               # no. of hiddent units in LSTM\n",
    "num_layers  = 3\n",
    "num_classes = len(Label_to_index)\n",
    "model_lstm250  = LSTM(input_size, hidden_size, num_layers, num_classes)\n",
    "model_lstm250.to(device)\n",
    "print(model_lstm250)\n",
    "\n",
    "\n",
    "# Save the model parameters\n",
    "Hparam = {\n",
    "    'input_size' : input_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_layers' : num_layers,\n",
    "    'num_classes': num_classes, \n",
    "}\n",
    "\n",
    "\n",
    "# Delete temporary names\n",
    "del input_size, hidden_size, num_layers, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252e137",
   "metadata": {},
   "source": [
    "# Dataloader Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da47a802",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d47105",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Use librosa.display.specshow to display 2D features\n",
    "def plotSpecshow(data, fig, axis):\n",
    "    img = librosa.display.specshow(data, ax=axis, vmin=-2, vmax=2, cmap='inferno')\n",
    "    fig.colorbar(img, ax=axis)\n",
    "    return img\n",
    "\n",
    "\n",
    "# Given a feature-sequence, breaks down different parts then plots it\n",
    "def showFeatures(feature_sequence):    # feature_sequence: (sequence-point, feature-vector)\n",
    "    feat_seq = feature_sequence.T      # Transpose it to make it the way plotSpecshow() expects\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    mel = feat_seq[0:40, :]     # extract the mfcc coefficients only\n",
    "    ax_mel = fig.add_subplot(2, 2, 1)\n",
    "    ax_mel.set_title('Mel Coefficients')\n",
    "    plotSpecshow(mel, fig, ax_mel)\n",
    "\n",
    "    energy = feat_seq[40, :]     # extract energy only\n",
    "    ax_energy = fig.add_subplot(2, 2, 2)\n",
    "    ax_energy.set_title('Energy')\n",
    "    ax_energy.plot(energy)\n",
    "    \n",
    "    delta1 = feat_seq[41:82, :]  # extract delta1 only\n",
    "    ax_delta1 = fig.add_subplot(2, 2, 3)\n",
    "    ax_delta1.set_title('Delta-1')\n",
    "    plotSpecshow(delta1, fig, ax_delta1)\n",
    "\n",
    "    delta2 = feat_seq[82:, :]    # extract delta2 only\n",
    "    ax_delta2 = fig.add_subplot(2, 2, 4)\n",
    "    ax_delta2.set_title('Delta-2')\n",
    "    plotSpecshow(delta2, fig, ax_delta2)\n",
    "    \n",
    "\n",
    "# Given a label and a dataset, returns the indices of the items with the given label\n",
    "def getIndicesByLabel(label, dataset):\n",
    "    indices = []\n",
    "    for i, item in enumerate(dataset):\n",
    "        if item[0]==label: indices.append(i)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f7fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TimitPhoneDataset(Dataset):\n",
    "    def __init__(self, feature_ds):\n",
    "        self.base_dataset = feature_ds    # list of (phoneme, feature-sequence)\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    " \n",
    "    # Returns: (phoneme, feature-sequence)\n",
    "    def __getitem__(self, index):\n",
    "        return self.base_dataset[index]\n",
    "         \n",
    "    \n",
    "# Instantiate the Dataset objects\n",
    "M_training_set = 'Train_dataset = TimitPhoneDataset(Train_feat + Test_feat)'\n",
    "#Train_dataset = TimitPhoneDataset(Train_feat)\n",
    "#Train_dataset = TimitPhoneDataset(Train_aug_feat)\n",
    "#Train_dataset = TimitPhoneDataset(Train_feat + Train_aug_feat)\n",
    "Train_dataset = TimitPhoneDataset(Train_feat + Test_feat)\n",
    "Test_dataset  = TimitPhoneDataset(Test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969e5461",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Label_to_index.keys())   # show all labels\n",
    "\n",
    "label = 'aa'\n",
    "print('label:', label)\n",
    "\n",
    "indices = getIndicesByLabel(label, Train_dataset)\n",
    "print('Train_dataset:\\n', indices[:100])\n",
    "indices = getIndicesByLabel(label, Test_dataset)\n",
    "print('Test_dataset:\\n', indices[:100])\n",
    "\n",
    "del label, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone, feat_seq = Train_dataset[3636]\n",
    "print('phone:', phone)\n",
    "print('feat_seq:', feat_seq.shape)\n",
    "showFeatures(feat_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad4a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if it works as expected\n",
    "phone, feat_seq = Test_dataset[3115]\n",
    "print('phone:', phone)\n",
    "print('feat_seq:', feat_seq.shape)\n",
    "showFeatures(feat_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete temporary names\n",
    "del phone, feat_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d2d850",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd553b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding is needed to make the batch <tensor> from <list> of variable length sequences\n",
    "# The padding values are not passed to the LSTM during trainig/testing\n",
    "def pad_sequence_lstm(batch):\n",
    "    # Make all tensor in a batch the same length by padding with zeros\n",
    "    batch = rnn_utils.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
    "    return batch\n",
    "\n",
    "# Gets the list of audio and labels as batch then\n",
    "# converts them into sequence of features for the model.\n",
    "# Adds padding to build the batch tensor\n",
    "def collate_fn_lstm(batch):\n",
    "    # A data tuple has the form: (phoneme, wave, sample_rate)\n",
    "    tensors, targets, lengths = [], [], []   # lengths is needed for pack_padded_sequence  in LSTM.forward()\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for item in batch:\n",
    "        label, feat_seq = item\n",
    "        feat_seq_tensor = torch.from_numpy(feat_seq)\n",
    "        tensors += [feat_seq_tensor]\n",
    "        targets += [Label_to_index[label]]\n",
    "        lengths.append(feat_seq_tensor.size()[0])\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = pad_sequence_lstm(tensors)\n",
    "    targets = torch.tensor(targets)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    return tensors, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575f31b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test padding function\n",
    "batch = [   torch.tensor([2,3,4,5]), \n",
    "            torch.tensor([2,3]),\n",
    "            torch.tensor([5,6,7])    ]\n",
    "pad_seq = pad_sequence_lstm(batch)\n",
    "print('pad_seq:\\n', pad_seq)\n",
    "\n",
    "\n",
    "# Test collate function\n",
    "batch = [Train_dataset[i*21] for i in range(5)]\n",
    "col_tensor, col_targets, col_lengths = collate_fn_lstm(batch)\n",
    "print('\\ncollate_out:\\n', col_tensor.size(), '\\n', col_targets, '\\n', col_lengths)\n",
    "\n",
    "\n",
    "# Manual features to compare\n",
    "print('')\n",
    "phone, feat_seq = batch[2]\n",
    "print('phone:', phone)\n",
    "print('feat_seq shape:', feat_seq.shape)\n",
    "\n",
    "\n",
    "# Delete names to avoid confusion later\n",
    "del batch, pad_seq\n",
    "del col_tensor, col_targets, col_lengths\n",
    "del phone, feat_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2a343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Dataloaders\n",
    "\n",
    "Batch_size_train = 128\n",
    "Batch_size_test = 256\n",
    "\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "    \n",
    "Train_loader = torch.utils.data.DataLoader(\n",
    "    Train_dataset,\n",
    "    batch_size=Batch_size_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn_lstm,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "Test_loader = torch.utils.data.DataLoader(\n",
    "    Test_dataset,\n",
    "    batch_size=Batch_size_test,\n",
    "    shuffle=False,\n",
    "    #shuffle=True,        # make it True for fractional validation\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn_lstm,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3213564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the dataloaders\n",
    "for (sequence, labels, lengths) in Train_loader:\n",
    "    print('sequence:', sequence.size(), 'type:', sequence.type())\n",
    "    print('labels  :', labels.size(), 'type:', labels.type())\n",
    "    print('lengths :', lengths.size(), 'type:', lengths.type())\n",
    "    break\n",
    "    \n",
    "print('')\n",
    "for (sequence, labels, lengths) in Test_loader:\n",
    "    print('sequence:', sequence.size(), 'type:', sequence.type())\n",
    "    print('labels  :', labels.size(), 'type:', labels.type())\n",
    "    print('lengths :', lengths.size(), 'type:', lengths.type())\n",
    "    break\n",
    "    \n",
    "    \n",
    "# Delete names\n",
    "del num_workers, pin_memory, sequence, labels, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2923ba",
   "metadata": {},
   "source": [
    "# Setting up Training Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaf091f",
   "metadata": {},
   "source": [
    "## Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff53b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "# The training method should not access global names to avoid confusions.\n",
    "# The only global name allowed is the Train_loader.\n",
    "# options and dashboard arguments are used to pass other global elements. \n",
    "def train_model(model, options, dashboard, debug=False):\n",
    "    # Get options and dashboard elements\n",
    "    criterion  = options['criterion']\n",
    "    optimizer  = options['optimizer']\n",
    "    log_interval = options['log_interval']\n",
    "    epoch_pbar   = dashboard['epoch_pbar']\n",
    "    \n",
    "    # Training initial setup\n",
    "    model.train()\n",
    "    model.debug = debug\n",
    "    epoch_pbar.reset()\n",
    "    accum_loss = 0\n",
    "    \n",
    "    # Run the training loop\n",
    "    for batch_idx, (sequence, target, lengths) in enumerate(Train_loader):\n",
    "        sequence = sequence.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Run through model and compute loss\n",
    "        output = model(sequence, lengths)\n",
    "        loss = criterion(output, target)    # compute batch loss\n",
    "\n",
    "        # Update gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update dashboard and print at log intervals\n",
    "        accum_loss += loss.item()  # accumulate loss for logging\n",
    "        epoch_pbar.update(1)       # update the epoch progress bar\n",
    "        is_log_step = (batch_idx!=0) and (batch_idx % log_interval == 0)\n",
    "        if is_log_step: \n",
    "            avg_loss = accum_loss / log_interval\n",
    "            updateTrainingStat( options, dashboard, batch_idx, \n",
    "                                len(sequence), len(Train_loader), avg_loss )    \n",
    "            accum_loss = 0   # reset for next log iteration\n",
    "            \n",
    "        # DEBUG\n",
    "        if debug and batch_idx == 100: \n",
    "            print('DBG: Breaking prematurely')\n",
    "            break\n",
    "    model.debug = False\n",
    "    \n",
    "    \n",
    "# Update training status\n",
    "def updateTrainingStat(options, dashboard, batch_idx, sequence_len, Train_len, avg_loss):\n",
    "    # Get options and dashboard elements\n",
    "    print_stat = options['print_stat']       # a true/false value\n",
    "    cur_epoch  = options['cur_epoch']        # current epoch number\n",
    "    train_loss = options['train_loss']       # a list to keep track of training loss, passed from top-level\n",
    "    plot_fig_ax = dashboard['plot_fig_ax']   # figure and axis handles for plotting\n",
    "    \n",
    "    # printing status (for short training sessions)\n",
    "    if print_stat:\n",
    "        printTrainingStat(cur_epoch, batch_idx, sequence_len, Train_len, avg_loss)\n",
    "\n",
    "    # record and plot loss\n",
    "    train_loss.append(avg_loss)\n",
    "    if plot_fig_ax:\n",
    "        fig, ax = plot_fig_ax\n",
    "        ax.plot(train_loss, color='b')\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        \n",
    "# Prints the training stats\n",
    "def printTrainingStat(cur_epoch, batch_idx, sequence_len, Train_len, loss):\n",
    "    print('Train Epoch: {} [{:6}/{} ({:2.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                      cur_epoch, batch_idx * sequence_len, Train_len,\n",
    "                      100. * batch_idx / Train_len, loss.data.item()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c22f6f",
   "metadata": {},
   "source": [
    "## Testing Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b68604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "# Returns percent accuracy\n",
    "# User fraction: <1.0 to run validation on a fraction of the test data\n",
    "def test_model(model, options, debug=False):\n",
    "    # Get options\n",
    "    criterion = options['criterion']\n",
    "    fraction  = options['test_fraction']   # <= 1.0\n",
    "    print_stat = options['print_stat']     # true/false value\n",
    "    \n",
    "    # Testing initial setup\n",
    "    model.eval()\n",
    "    model.debug = debug\n",
    "    # Compute the stop batch no. based on fraction\n",
    "    total_batch = ceil(len(Test_dataset)/Batch_size_test)\n",
    "    stop_count = int(ceil(total_batch*fraction))\n",
    "    \n",
    "    # Run the test dataset through the model\n",
    "    loss, correct = 0, 0\n",
    "    tested_count = 0\n",
    "    for sequence, targets, lengths in Test_loader:\n",
    "        sequence = sequence.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(sequence, lengths)\n",
    "        loss += criterion(outputs, targets).data.item()\n",
    "        pred = get_likely_index(outputs) # get the index of the max log-probability\n",
    "        correct += number_of_correct(pred, targets)\n",
    "        tested_count += len(targets)  # increment the tested item counter\n",
    "        # Run only on the given fraction\n",
    "        stop_count -= 1\n",
    "        if stop_count < 0: break\n",
    "    model.debug = False\n",
    "    \n",
    "    # Print statistics\n",
    "    loss /= tested_count\n",
    "    test_loss.append(loss)\n",
    "    accuracy = (100.0 * correct) / tested_count\n",
    "    test_accuracy.append(accuracy)\n",
    "    \n",
    "    if print_stat:\n",
    "        print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "                 loss, correct, tested_count, accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a582746",
   "metadata": {},
   "source": [
    "## Training Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b9395",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from ipywidgets.widgets import HTML\n",
    "import IPython.display as ipd\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Make a widget to show status text at the top of the cell\n",
    "status_text = HTML()\n",
    "status_init = \"<b>Status:</b> Start the training\"\n",
    "status_start = \"<b>Status:</b> Training started ...\"\n",
    "status_end   = \"<br><b style='color:green'>Training Done!</b>\"\n",
    "\n",
    "\n",
    "# Given the parameters, updates the status_text widget\n",
    "def updateStatusText(target_models, cur_accuracy, best_accuracy, learn_rate):\n",
    "    param_style = 'style=\"color:indianred\"'\n",
    "    text = f'<b>Status:</b> target-model #: <b {param_style}>{{}}</b> cur-acc: <b {param_style}>{{:.2f}}%</b>   best-acc: <b {param_style}>{{:.2f}}%</b> lr: <b {param_style}>{{}}</b>'\n",
    "    status_text.value = text.format(len(target_models), cur_accuracy, best_accuracy, learn_rate)\n",
    "\n",
    "\n",
    "# Call this function where you want to show the interactive plot\n",
    "fig_train = None\n",
    "ax_trainloss = None\n",
    "ax_testloss = None\n",
    "ax_testaccu = None\n",
    "  \n",
    "def showTrainPlot_2row():\n",
    "    global fig_train, ax_trainloss, ax_testloss, ax_testaccu\n",
    "    fig_height = 4.5\n",
    "    fig_train = plt.figure(\"Training Progress\", figsize=(2*fig_height, fig_height))\n",
    "    # Training loss axis\n",
    "    ax_trainloss = fig_train.add_subplot(2, 1, 1)\n",
    "    ax_trainloss.set_title(\"Training Loss\")\n",
    "    # Test (validation) loss axis\n",
    "    ax_testloss = fig_train.add_subplot(2, 2, 3)\n",
    "    ax_testloss.set_title(\"Test Loss\")\n",
    "    # Test (validation) accuracy axis\n",
    "    ax_testaccu = fig_train.add_subplot(2, 2, 4)\n",
    "    ax_testaccu.set_title(\"Test Accuracy\")\n",
    "    plt.subplots_adjust(hspace=0.2)\n",
    "    fig_train.tight_layout()\n",
    "\n",
    "    \n",
    "# Call this function to update the plot\n",
    "def updateTrainPlot(test_loss, test_accuracy):\n",
    "    ax_testloss.plot(test_loss, color='g')\n",
    "    ax_testaccu.plot(test_accuracy, color='g')\n",
    "    fig_train.canvas.draw()\n",
    "\n",
    "\n",
    "# Define the container to save the best models\n",
    "Saved_models = {-1:'Dummy'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb12f71",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4709e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control training randomization \n",
    "Seed_training = 17         # seeds: 97, 17\n",
    "random.seed(Seed_training)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9714ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    " \n",
    "\n",
    "# Define optimizer and loss function\n",
    "criterion   = nn.CrossEntropyLoss()\n",
    "#optimizer   = optim.SGD(model_lstm250.parameters(), lr=0.001, momentum=0.5)\n",
    "optimizer   = optim.Adam(model_lstm250.parameters(), lr=0.00001, weight_decay=0.0001)\n",
    "lrScheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)  # reduce the learning after given steps by a factor (gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop parameters\n",
    "n_epoch = 4\n",
    "target_accuracy = 72.0  # percent\n",
    "train_print_stat = False\n",
    "test_print_stat = False\n",
    "test_fraction = 1.0   # run validation on random fraction of the test dataset\n",
    "log_interval_percent = 20\n",
    "log_interval = (len(Train_dataset)//Batch_size_train) * log_interval_percent // 100\n",
    "\n",
    "\n",
    "# Tracking variables\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "target_models = {}\n",
    "best_accuracy = -1\n",
    "best_model = None\n",
    "\n",
    "\n",
    "# Set up training dashboard\n",
    "epoch_iter = tqdm(range(n_epoch), desc=\"Full Training\")\n",
    "total_train_batch = ceil(len(Train_dataset)/Batch_size_train)\n",
    "epoch_pbar = tqdm(total=total_train_batch, desc='This Epoch  ')\n",
    "status_text.value = status_init\n",
    "ipd.display(status_text)\n",
    "#ipd.display(substatus_text)\n",
    "showTrainPlot_2row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c9067",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# Prepare options and dashboard dictionaries\n",
    "options = {\n",
    "    'criterion'    : criterion,\n",
    "    'optimizer'    : optimizer,\n",
    "    'log_interval' : log_interval,\n",
    "    'print_stat'   : False,         # Set it to False to stop printing during training and validation\n",
    "    'cur_epoch'    : None,          # will be updated in the loop\n",
    "    'train_loss'   : train_loss,    # reference to a list\n",
    "    'test_fraction': test_fraction,\n",
    "}\n",
    "\n",
    "dashboard = {\n",
    "    'epoch_pbar'  : epoch_pbar,\n",
    "    'plot_fig_ax' : (fig_train, ax_trainloss),\n",
    "    'status_text' : status_text,\n",
    "}\n",
    "\n",
    "\n",
    "# Train the model and save the ones with accuracy >= target_accuracy\n",
    "status_text.value = status_start\n",
    "for epoch in epoch_iter:\n",
    "    # Training and testing\n",
    "    options['cur_epoch'] = epoch\n",
    "    train_model(model_lstm250, options, dashboard, debug=False)\n",
    "    status_text.value += \" -- Running Validation ... \"\n",
    "    accuracy = test_model(model_lstm250, options, debug=False)\n",
    "    lrScheduler.step()\n",
    "    status_text.value += \"done.\"\n",
    "    \n",
    "    # Save models achieving target accuracy\n",
    "    if accuracy >= target_accuracy:\n",
    "        accuracy = round(accuracy, 4)   # to reduce the key granularity\n",
    "        target_models[accuracy] = deepcopy(model_lstm250.state_dict())\n",
    "    \n",
    "    # Save the best model and update the status text\n",
    "    if accuracy > best_accuracy: \n",
    "        best_accuracy = accuracy\n",
    "        best_model = deepcopy(model_lstm250.state_dict())\n",
    "    learn_rate = optimizer.param_groups[0][\"lr\"]\n",
    "    updateStatusText(target_models, accuracy, best_accuracy, learn_rate)\n",
    "    updateTrainPlot(test_loss, test_accuracy)\n",
    "\n",
    "status_text.value += status_end\n",
    "\n",
    "\n",
    "# Print and save the best performing models, and show the training summary\n",
    "summary = []\n",
    "cnt = len(target_models)\n",
    "summary.append(f'Target met by: {cnt}')\n",
    "if cnt > 0: \n",
    "    summary.append('Saving target_models')\n",
    "    Saved_models.update(target_models)  # copy the target_models into the Saved_models\n",
    "summary.append(f'Saved_models#: {len(Saved_models)}')\n",
    "summary.append(f'Saved max acc: {max(Saved_models)}%')\n",
    "summary.append(f'Best in this iter: {best_accuracy:.2f}%')\n",
    "summary.append(f'Last learning rate: {optimizer.param_groups[0][\"lr\"]}')\n",
    "print('\\n'.join(summary))\n",
    "status_text.value += '<br>' + '<br>'.join(summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca134dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the best model\n",
    "saved_acc = max(Saved_models)\n",
    "if saved_acc > best_accuracy:\n",
    "    print(f'Loading from Saved models, accuracy: {saved_acc:.2f}%')\n",
    "    best_model_dict = Saved_models[saved_acc]\n",
    "else:\n",
    "    print(f'Loading from last training session, validation accuracy: {best_accuracy:.2f}%')\n",
    "    best_model_dict = best_model\n",
    "    \n",
    "model_lstm250.load_state_dict(best_model_dict)\n",
    "\n",
    "\n",
    "# Run the model on the entire test dataset\n",
    "options['test_fraction'] = 1\n",
    "accuracy = test_model(model_lstm250, options, debug=False)\n",
    "print(f'Accuracy on whole test dataset: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c30b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_session = False\n",
    "\n",
    "# Delete names\n",
    "if delete_session:\n",
    "    del criterion, optimizer, lrScheduler\n",
    "    del options, dashboard\n",
    "    del status_end, status_init, status_start, status_text, summary, learn_rate\n",
    "    del epoch, n_epoch, epoch_iter, epoch_pbar\n",
    "    del target_accuracy, target_models, test_accuracy, test_fraction, test_loss, test_print_stat\n",
    "    del log_interval, log_interval_percent, \n",
    "    del fig_train, ax_testaccu, ax_testloss, ax_trainloss\n",
    "    del best_model, best_model_dict, best_accuracy, saved_acc\n",
    "    del accuracy, number_of_correct, cnt\n",
    "    del total_train_batch, train_loss, train_print_stat\n",
    "else:\n",
    "    print('Not deleting session variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe90921",
   "metadata": {},
   "source": [
    "# Validate and Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the prediction\n",
    "def predict(feature_item):  # item: an item in the dataset\n",
    "    model_lstm250.eval()\n",
    "    # Extract features\n",
    "    batch = [feature_item]   # make a batch with single example\n",
    "    tensor, target, lengths = collate_fn_lstm(batch)\n",
    "    # Use the model to predict the label of the waveform\n",
    "    tensor = tensor.to(device)\n",
    "    target = target.to(device)\n",
    "    output = model_lstm250(tensor, lengths)\n",
    "    pred = get_likely_index(output)[0]   # indexing to get the prediction from batch    \n",
    "    return pred\n",
    "\n",
    "\n",
    "# Run a prediction\n",
    "select_index = 104\n",
    "item = Test_dataset[select_index]\n",
    "pred_index = predict(item)\n",
    "pred_label  = Index_to_label[pred_index.item()]\n",
    "phone, feat_seq = item\n",
    "print(f\"Expected: {phone}. Predicted: {pred_label}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf1876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the entire test dataset\n",
    "correct_count = 0\n",
    "for item in tqdm(Test_feat):\n",
    "    pred_index = predict(item)\n",
    "    label, *_ = item\n",
    "    pred_label  = Index_to_label[pred_index.item()]\n",
    "    if pred_label==label: correct_count += 1\n",
    "\n",
    "accuracy = (100.0 * correct_count) / len(Test_dataset)\n",
    "print('correct_count:', correct_count)\n",
    "print(f'accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d11780",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './session/trained-lstm250.pt'\n",
    "\n",
    "export_dict = {\n",
    "    'training_set:': M_training_set,      # how the training dataset was built\n",
    "    'seed_model' : Seed_model,\n",
    "    'seed_training' : Seed_training,\n",
    "    'accuracy' : accuracy,\n",
    "    'correct_count' : correct_count,\n",
    "    'index_to_label' : Index_to_label,\n",
    "    'Hparam' : Hparam,\n",
    "    'state_dict' : model_lstm250.state_dict()\n",
    "}\n",
    "\n",
    "\n",
    "# Show metadata of the exported mode\n",
    "print('Metadata:')\n",
    "for k, v in export_dict.items():\n",
    "    if k!='state_dict':\n",
    "        print(k,':',v)\n",
    "print('')\n",
    "\n",
    "\n",
    "# Save the model parameters\n",
    "torch.save(export_dict, save_path)\n",
    "!ls -ltrh ./session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b2eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del k, v\n",
    "del save_path, accuracy, correct_count, item, pred_index, label, pred_label\n",
    "del feat_seq, phone, select_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fddfa79",
   "metadata": {},
   "source": [
    "# Save the Test dataset for the model\n",
    "\n",
    "The test dataset and the prediction by the trained model is saved as the following dictionary:\n",
    "- label_dict: {label: index}\n",
    "- dataset_schema: structure of dataset items\n",
    "- dataset: list of items\n",
    "    - item: (label, label_index, predicted_index, sequence_length, feature_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb49fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a Test_feat record, makes a record to export the datset\n",
    "# export-item: (label, label_index, predicted-index, sequence-length, feature-sequence)\n",
    "def makeExportRecord(feature_item):\n",
    "    label, feat_seq = feature_item\n",
    "    pred_index = predict(feature_item)\n",
    "    pred_index = pred_index.item()     # get the number from the tensor\n",
    "    lbl_index = Label_to_index[label]\n",
    "    seq_len = len(feat_seq)\n",
    "    return (label, lbl_index, pred_index, seq_len, feat_seq)\n",
    "\n",
    "\n",
    "\n",
    "# Test above function\n",
    "item = Test_feat[0]\n",
    "exp_item = makeExportRecord(item)\n",
    "print('item:', item[:-1])\n",
    "print('exp_item:', exp_item[:-1])\n",
    "print('Label-index:', Label_to_index[item[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5851bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete names to avoid confusion\n",
    "del item, exp_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "correct_count = 0\n",
    "for item in tqdm(Test_feat):\n",
    "    exp_item = makeExportRecord(item)\n",
    "    dataset.append(exp_item)\n",
    "    if exp_item[1] == exp_item[2]:   # if label-index == predicted-index\n",
    "        correct_count += 1\n",
    "\n",
    "print('export_dataset:', len(export_dataset))\n",
    "print('correct_count :', correct_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3271ecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Test dataset\n",
    "save_path = './session/test-export-ds.pt'\n",
    "schema = \"(label, label_index, predicted_index, sequence_length, feature_sequence)\"\n",
    "export_ds = {\n",
    "    'label_dict': Label_to_index,\n",
    "    'dataset_schema': schema,\n",
    "    'dataset': dataset\n",
    "}\n",
    "\n",
    "torch.save(export_ds, save_path)\n",
    "!ls -ltrh ./session/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
