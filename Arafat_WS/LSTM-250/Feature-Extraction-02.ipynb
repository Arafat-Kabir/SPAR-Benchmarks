{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b9d6ee",
   "metadata": {},
   "source": [
    "# Feature Extraction for Phoneme Recognition on TIMIT\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Loading and testing the datasets exported by the previous notebook.\n",
    "- Feature design for the LSTM-250 network.\n",
    "- Feature extraction from the TIMIT dataset.\n",
    "- Exporting the features as standalone dataset for training the network.\n",
    "    - Feature dataset record: (phoneme, feature-sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed2982",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ee34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74cb192",
   "metadata": {},
   "source": [
    "# Loading and Testing the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b2c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = './session/curated-dataset.pt'\n",
    "ds_dict = torch.load(ds_path)\n",
    "print(ds_dict.keys())\n",
    "print(ds_dict['note'])\n",
    "\n",
    "Train_ds = ds_dict['train']\n",
    "Test_ds  = ds_dict['test']\n",
    "\n",
    "print('Train_ds:', len(Train_ds))\n",
    "ipd.display(Train_ds[:3])\n",
    "print('Test_ds:', len(Test_ds))\n",
    "ipd.display(Test_ds[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a42b647",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = Train_ds[1]\n",
    "phone, audio_path, start, end = rec\n",
    "wave, rate = librosa.load(audio_path, sr=None)\n",
    "phone_wave = wave[start:end]\n",
    "print(phone)\n",
    "ipd.display(ipd.Audio(phone_wave, rate=rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete redundant variables to avoid confusion\n",
    "del ds_path, ds_dict\n",
    "del rec, phone, audio_path, start, end, wave, rate, phone_wave\n",
    "print(dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a52f60",
   "metadata": {},
   "source": [
    "## Convert to Audio dataset\n",
    "\n",
    "Convert records from (phoneme, path-to-audio, start-index, end-index) to (phoneme, wave, rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e2faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a file path, returns the audio waveform and the sampling rate.\n",
    "# Mainly used for caching already loaded files.\n",
    "audio_cache = {}    # Caching loaded audio files for faster processing\n",
    "def getAudio(audio_path):\n",
    "    if audio_path not in audio_cache:\n",
    "        wave, rate = librosa.load(audio_path, sr=None)    \n",
    "        audio_cache[audio_path] = (wave, rate)\n",
    "    return audio_cache[audio_path]\n",
    "\n",
    "\n",
    "# Given file path, start, and end indices, returns the audio slice and the sampling rate\n",
    "def getAudioSlice(audio_path, start, end):\n",
    "    wave, rate = getAudio(audio_path)\n",
    "    return wave[start:end], rate\n",
    "\n",
    "\n",
    "# Without caching: 14 seconds\n",
    "# With caching: < 1sec\n",
    "# Test above function\n",
    "for rec in tqdm(Train_ds):    \n",
    "    _, audio_path, *_ = rec\n",
    "    getAudio(audio_path)\n",
    "    \n",
    "    \n",
    "# delete redundant variables to avoid confusion\n",
    "del rec, audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a phoneme record list, returns an audio record list: (phoneme, wave, rate)\n",
    "def makeAudioDS(list_phone_rec):\n",
    "    audio_ds = []\n",
    "    for phone_rec in list_phone_rec:\n",
    "        phone, audio_path, start, end = phone_rec\n",
    "        wave, rate = getAudioSlice(audio_path, start, end)\n",
    "        audio_rec = [phone, wave, rate]\n",
    "        audio_ds.append(audio_rec)\n",
    "    return audio_ds\n",
    "\n",
    "\n",
    "# Convert to audio datasets\n",
    "Train_audio_ds = makeAudioDS(Train_ds)\n",
    "Test_audio_ds  = makeAudioDS(Test_ds)\n",
    "        \n",
    "print('Train_audio_ds:', len(Train_audio_ds))\n",
    "print('Test_audio_ds :', len(Test_audio_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a90d389",
   "metadata": {},
   "source": [
    "# Feature Design\n",
    "\n",
    "**NOTE:**\n",
    "- Feature vector is designed following [[Speech-Recog paper]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638947)\n",
    "- Mel-spectrogram is used as the base feature, 40 of Mel bands are generated.\n",
    "- The energy term is computed using the mel-spectrogram.\n",
    "- First and second order derivatives of those terms are used.\n",
    "- Total length of the feature vector: 41 x 3 = 123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e4944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an audio waveform, get the MFCC coefficients\n",
    "# Params,\n",
    "#   n_mfcc    : no. of MFCC coefficient to return\n",
    "#   n_mels    : number of Mel bands to generate\n",
    "#   fft_window: length of the FFT window\n",
    "#   hop_len   : number of audio samples between adjacent STFT columns.\n",
    "# Note: Change fft_window and hop_len to play with the sequence lengths.\n",
    "#def getMFCC(wave, sample_rate, n_mfcc, fft_window, hop_length, n_mels):\n",
    "#    #mfcc = librosa.feature.mfcc(y=wave, sr=sample_rate, n_mfcc=n_mfcc, n_fft=fft_window, hop_length=hop_length, n_mels=n_mels)\n",
    "#    mel_spec = librosa.feature.melspectrogram(y=wave, sr=sample_rate, n_fft=fft_window, hop_length=hop_length, n_mels=n_mels)\n",
    "#    log_mel_spec = librosa.power_to_db(mel_spec)  # Convert to log-scale\n",
    "#    return log_mel_spec\n",
    "    #return mfcc\n",
    "\n",
    "\n",
    "# Given a sequence of mfcc coefficients, returns a sequence of corresponding energy terms\n",
    "#def getEnergy(mfcc_seq):\n",
    "#    energy_seq = np.sum(mfcc_seq**2, axis=0)\n",
    "#    return energy_seq\n",
    "\n",
    "\n",
    "# Given an audio waveform, get the mel-spectrogram\n",
    "# Params,\n",
    "#   n_mels     : number of Mel bands to generate\n",
    "#   fft_window : length of the FFT window\n",
    "#   hop_length : number of audio samples between adjacent STFT columns.\n",
    "# Note: Change fft_window and hop_len to play with the sequence lengths.\n",
    "def getMelSpec(wave, sample_rate, n_mels, fft_window, hop_length):\n",
    "    mel_spec = librosa.feature.melspectrogram(y=wave, sr=sample_rate, n_fft=fft_window, hop_length=hop_length, n_mels=n_mels)\n",
    "    log_mel_spec = librosa.power_to_db(mel_spec)  # Convert to log-scale\n",
    "    return log_mel_spec\n",
    "\n",
    "\n",
    "# Given a waveform, compute energy in each frame\n",
    "def getEnergy(wave, frame_length, hop_length):\n",
    "    energy = librosa.feature.rms(y=wave, frame_length=frame_length, hop_length=hop_length)\n",
    "    return energy\n",
    "\n",
    "\n",
    "# Given a sequence, computes the derivative of order=order.\n",
    "def getDelta(seq, order):\n",
    "    delta = librosa.feature.delta(seq, order=order)\n",
    "    return delta\n",
    "\n",
    "\n",
    "# Plotting utilities ---\n",
    "def plotAudio(wave, sample_rate, axis):\n",
    "    duration = len(wave) / sample_rate\n",
    "    time = np.linspace(0, duration, len(wave))\n",
    "    axis.plot(time, wave)\n",
    "    \n",
    "\n",
    "# Use librosa.display.specshow to display 2D features\n",
    "def plotSpecshow(data, fig, axis):\n",
    "    img = librosa.display.specshow(data, ax=axis)\n",
    "    fig.colorbar(img, ax=axis)\n",
    "    return img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833ea4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test above functions ---\n",
    "# Get an audio\n",
    "rec = Test_audio_ds[1200]\n",
    "phone, wave, rate = rec\n",
    "print('phone:', phone, '  wave:', len(wave), '  rate:', rate)\n",
    "\n",
    "#mfcc = getMFCC(wave, rate, n_mfcc=40, fft_window=64, hop_length=16, n_mels=40)\n",
    "#print('mfcc:', mfcc.shape)\n",
    "\n",
    "mel_spec = getMelSpec(wave, rate, n_mels=40, fft_window=64, hop_length=16)\n",
    "print('mel_spec:', mel_spec.shape)\n",
    "\n",
    "#energy = getEnergy(mfcc)\n",
    "energy = getEnergy(wave, frame_length=64, hop_length=16)\n",
    "print('energy:', energy.shape)\n",
    "\n",
    "#mfcc_eng = np.vstack([mfcc, energy])\n",
    "#print('mfcc_eng:', mfcc_eng.shape)\n",
    "\n",
    "mel_eng = np.vstack([mel_spec, energy])\n",
    "\n",
    "delta1 = getDelta(mel_eng, order=1)\n",
    "delta2 = getDelta(mel_eng, order=2)\n",
    "print('delta1:', delta1.shape)\n",
    "print('delta2:', delta2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846023f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the features\n",
    "print('phoneme:', phone)\n",
    "ipd.display(ipd.Audio(wave, rate=rate))\n",
    "\n",
    "fig = plt.figure(figsize=(3*4, 7))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "fig.tight_layout()\n",
    "\n",
    "ax_wave = fig.add_subplot(3, 1, 1)\n",
    "plotAudio(wave, rate, ax_wave)\n",
    "ax_wave.set_title('Audio')\n",
    "\n",
    "ax_mel = fig.add_subplot(3, 2, 3)\n",
    "ax_mel.set_title('Mel Spectrogram')\n",
    "plotSpecshow(mel_spec, fig, ax_mel)\n",
    "\n",
    "ax_energy = fig.add_subplot(3, 2, 4)\n",
    "ax_energy.set_title('Energy')\n",
    "ax_energy.plot(energy[0], color='r')\n",
    "\n",
    "ax_delta1 = fig.add_subplot(3, 2, 5)\n",
    "ax_delta1.set_title('Delta-1')\n",
    "plotSpecshow(delta1, fig, ax_delta1)\n",
    "\n",
    "ax_delta2 = fig.add_subplot(3, 2, 6)\n",
    "ax_delta2.set_title('Delta-2')\n",
    "plotSpecshow(delta2, fig, ax_delta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b07df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete redundant variables to avoid confusion\n",
    "del rec, phone, wave, rate\n",
    "del mel_spec, energy, mel_eng, delta1, delta2\n",
    "del ax_wave, ax_mel, ax_energy, ax_delta1, ax_delta2, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93bb85",
   "metadata": {},
   "source": [
    "## Check Audio Dataset Distribution\n",
    "\n",
    "Check the audio dataset to determine the parameters for the features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf as INF\n",
    "\n",
    "\n",
    "# Given a audio dataset, returns the record with min and max audio lengths\n",
    "def getMinMaxRec(ds_list):\n",
    "    min_len = INF\n",
    "    max_len = -INF\n",
    "    min_rec = None\n",
    "    max_rec = None\n",
    "    all_len = []\n",
    "    for audio_rec in ds_list:    # rec: (phoneme, wave, rate)\n",
    "        phone_len = len(audio_rec[1])\n",
    "        if phone_len < min_len:\n",
    "            min_len = phone_len\n",
    "            min_rec = audio_rec\n",
    "        if phone_len > max_len:\n",
    "            max_len = phone_len\n",
    "            max_rec = audio_rec\n",
    "        all_len.append(phone_len)\n",
    "    return min_len, max_len, min_rec, max_rec, all_len\n",
    "\n",
    "\n",
    "# Given an audio dataset, shows the audio length disribution\n",
    "def showAudioLenDisrib(audio_ds):\n",
    "    # Print min/max info\n",
    "    min_len, max_len, min_rec, max_rec, all_len = getMinMaxRec(audio_ds)\n",
    "    print('min_rec:', min_len, min_rec[0])\n",
    "    print('max_rec:', max_len, max_rec[0])\n",
    "    median = np.median(all_len)\n",
    "    print('median:', median)\n",
    "    \n",
    "    # show min-rec waveform\n",
    "    wave, rate = min_rec[1:]\n",
    "    print('min_rec waveform:')\n",
    "    plt.plot(wave)\n",
    "    plt.show()\n",
    "    \n",
    "    # Play the audio clips\n",
    "    ipd.display(ipd.Audio(wave, rate=rate))\n",
    "    wave, rate = max_rec[1:]\n",
    "    ipd.display(ipd.Audio(wave, rate=rate))\n",
    "    \n",
    "    # Show the histogram\n",
    "    plt.hist(all_len, bins='auto', rwidth=0.8)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of audio lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e66c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train_audio_ds ---')\n",
    "showAudioLenDisrib(Train_audio_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b2127",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test_audio_ds ---')\n",
    "showAudioLenDisrib(Test_audio_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec94a9",
   "metadata": {},
   "source": [
    "## Zero Padding of Short Audio\n",
    "\n",
    "**NOTE:**\n",
    "Based on the above observation\n",
    "- Because median is around 1024, pad zeros to make all audio length >= 1024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769ccc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an audio dataset, pad zeros to make all audio clips lengths >= min_len\n",
    "# audio_ds[i]: (phoneme, wave, rate)\n",
    "def padZeroAudio(audio_ds, min_len):\n",
    "    for audio_rec in audio_ds:\n",
    "        current_length = len(audio_rec[1])\n",
    "        if current_length < min_len:\n",
    "            num_zeros = min_len - current_length\n",
    "            padding = np.zeros(num_zeros)\n",
    "            padded_audio = np.concatenate((audio_rec[1], padding))\n",
    "            audio_rec[1] = padded_audio\n",
    "\n",
    "            \n",
    "# Pad zeros to audio\n",
    "F_min_audio_len = 1024\n",
    "padZeroAudio(Train_audio_ds, F_min_audio_len)\n",
    "padZeroAudio(Test_audio_ds, F_min_audio_len)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527d3141",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train_audio_ds ---')\n",
    "showAudioLenDisrib(Train_audio_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db933e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test_audio_ds ---')\n",
    "showAudioLenDisrib(Test_audio_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e9810",
   "metadata": {},
   "source": [
    "# Feature Extraction and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caed11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_fft_window = 256\n",
    "F_hop_length = 64\n",
    "F_n_mels = 40\n",
    "\n",
    "F_note = f'''\n",
    "F_n_mels    : {F_n_mels}\n",
    "F_fft_window: {F_fft_window}\n",
    "F_hop_length: {F_hop_length}\n",
    "'''\n",
    "\n",
    "# Given an audio record, returns a feature record: (phoneme, feature-sequence)\n",
    "def getFeatureRecord(audio_record):\n",
    "    # Feature extraction parameters\n",
    "    # Extract features from the audio-record\n",
    "    phone, wave, rate = audio_record\n",
    "    # compute mel-spectrogram\n",
    "    mel_spec = getMelSpec(wave, rate, n_mels=F_n_mels, fft_window=F_fft_window, hop_length=F_hop_length)\n",
    "    # Compute energy from mfcc then stack on top of mfcc for deta calculation\n",
    "    energy = getEnergy(wave, frame_length=F_fft_window, hop_length=F_hop_length)\n",
    "    mel_eng = np.vstack([mel_spec, energy])\n",
    "    # compute deltas\n",
    "    delta1 = getDelta(mel_eng, order=1)\n",
    "    delta2 = getDelta(mel_eng, order=2)\n",
    "    # stack all to make feature vector\n",
    "    feat_vec = np.vstack([mel_eng, delta1, delta2])\n",
    "    return [phone, feat_vec]    # make each record a list, not a tuple\n",
    "\n",
    "    \n",
    "# Test above function\n",
    "audio_rec = Train_audio_ds[4]\n",
    "phone, feat_vec = getFeatureRecord(audio_rec)\n",
    "print('phone:', phone)\n",
    "print('audio_rec[1]:', len(audio_rec[1]))\n",
    "print('feat_vec:', feat_vec.shape)\n",
    "F_feat_len = len(feat_vec)\n",
    "ipd.display(ipd.Audio(audio_rec[1], rate=audio_rec[2]))\n",
    "\n",
    "\n",
    "# delete redundant variables to avoid confusion\n",
    "del audio_rec, phone, feat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bb68e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given an audio dataset, returns a list of feature dataset\n",
    "# audio_ds[i]: (phone, wave, rate)\n",
    "# return[i]: (phone, feature-sequence)\n",
    "def makeFeatureDS(audio_ds):\n",
    "    feat_ds = []\n",
    "    for audio_rec in tqdm(audio_ds):\n",
    "        feat_rec = getFeatureRecord(audio_rec)\n",
    "        feat_ds.append(feat_rec)\n",
    "    return feat_ds\n",
    "\n",
    "\n",
    "# Given an audio-dataset, converts it into feature-dataset and saves in a file\n",
    "def saveFeatureDS(audio_ds, save_path, note):\n",
    "    feat_ds = makeFeatureDS(audio_ds)\n",
    "    assert len(feat_ds) == len(audio_ds), \"Test dataset length mismatch\"\n",
    "    export = {\n",
    "        'note' : note,\n",
    "        'data-schema' : '(phoneme, feature-sequence)',\n",
    "        'data' : feat_ds\n",
    "    }\n",
    "    torch.save(export, save_path)\n",
    "    print(f'INFO: Saved {save_path}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abae801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataset with necessary information for the next notebook\n",
    "note = f'''\n",
    "Notes:\n",
    "- Feature record: (phone, feature_sequence)\n",
    "- feature_seqence: list(feature_vector)\n",
    "- len(feature_vector): {F_feat_len}\n",
    "\n",
    "Features are extracted using following parameters''' + F_note\n",
    "\n",
    "print(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e66877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make test feature dataset then save\n",
    "saveFeatureDS(Test_audio_ds, './session/test-features.pt', note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b374fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveFeatureDS(Train_audio_ds, './session/train-features.pt', note)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
