{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64251b03",
   "metadata": {},
   "source": [
    "# Fixed Point Precision Study on MLP-100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32672482",
   "metadata": {},
   "source": [
    "This note book is the final stage of the model preparation for benchmarking. The next stage after this notebook is to simply implement the model in C using the knowledge gained from this notebook.  \n",
    "The fixed-point operations defined here tries to simulate the computations performed in SPAR. This might change over-time.\n",
    "\n",
    "**NOTE:**\n",
    "- The programs/code-snippets in this notebook follows C-like interfaces on purpose.\n",
    "- This is done so that, these code can be easily translated into C for the next stage of study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5286025",
   "metadata": {},
   "source": [
    "# SQLite3 Database Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd3e77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Returns a list of table names in the database file\n",
    "def getTableNames(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch the table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    table_names = cursor.fetchall()\n",
    "    table_names = [name[0] for name in table_names]  # make a list to return\n",
    "    conn.close()\n",
    "    return table_names\n",
    "\n",
    "\n",
    "# Returns a list of column names of the specified table\n",
    "def getColNames(database_filename, table_name):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(database_filename)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch the column names\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    results = cursor.fetchall()\n",
    "    column_names = [result[1] for result in results]  # Extract the column names from the query results\n",
    "\n",
    "    # Close the connection and return \n",
    "    conn.close()\n",
    "    return column_names\n",
    "\n",
    "    \n",
    "# returns all records of a given table\n",
    "def getRecords(db_path, table_name):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Fetch all records from the table\n",
    "    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    records = cursor.fetchall()\n",
    "    return records\n",
    "\n",
    "\n",
    "# Checks if a table exist\n",
    "def existTable(db_path, table_name):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Check if the table exists\n",
    "    cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'\")\n",
    "    result = cursor.fetchone()\n",
    "    if result is None: exist = False\n",
    "    else: exist = True\n",
    "    # Commit the changes and close the connection and return result\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456482ad",
   "metadata": {},
   "source": [
    "# Load Floating-Point Model Parameters and Dataset\n",
    "\n",
    "Here, the model and the dataset exported by the Numpy model extraction notebook is loaded and verified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea3cea",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09da043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_names: ['Header', 'sqlite_sequence', 'Labels_T', 'DataItems_T', 'Features_T']\n",
      "\n",
      "('name', 'MNIST-Test')\n",
      "('feature_length', 784)\n",
      "('accuracy', 98.36)\n",
      "('labels.table', 'Labels_T')\n",
      "('dataset.table', 'DataItems_T')\n",
      "('dataitem.schema', '')\n",
      "('features.table', 'Features_T')\n",
      "\n",
      "Data_table: DataItems_T\n",
      "Feature_table: Features_T\n",
      "Label_table: Labels_T\n"
     ]
    }
   ],
   "source": [
    "# Load and check the dataset table\n",
    "Dataset_path = './saved/mnist_test_data-98.36p.s3db'\n",
    "\n",
    "table_names = getTableNames(Dataset_path)\n",
    "print('table_names:', table_names)\n",
    "\n",
    "# Read the header table\n",
    "header_records =  getRecords(Dataset_path, 'Header')\n",
    "header_dict = {}\n",
    "print('')\n",
    "for r in header_records: \n",
    "    print(r[1:3])\n",
    "    header_dict[r[1]] = r[2]\n",
    "    \n",
    "# Get the table names\n",
    "Data_table = header_dict['dataset.table']\n",
    "Feature_table = header_dict['features.table']\n",
    "Label_table = header_dict['labels.table']\n",
    "print('')\n",
    "print('Data_table:', Data_table)\n",
    "print('Feature_table:', Feature_table)\n",
    "print('Label_table:', Label_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2942128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_records: [(0, '0'), (1, '1'), (2, '2'), (3, '3'), (4, '4'), (5, '5'), (6, '6'), (7, '7'), (8, '8'), (9, '9')]\n",
      "Label_to_index: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n",
      "Index_to_label: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9'}\n"
     ]
    }
   ],
   "source": [
    "# Read the label_to_index dictionary\n",
    "labels_records = getRecords(Dataset_path, Label_table)\n",
    "print('labels_records:', labels_records)\n",
    "\n",
    "Label_to_index = {label:index for (index, label) in labels_records}\n",
    "Index_to_label = {index:label for (index, label) in labels_records}\n",
    "print('Label_to_index:', Label_to_index)\n",
    "print('Index_to_label:', Index_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c0fec",
   "metadata": {},
   "source": [
    "### Build the Dataset array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b65fe35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dataclasses\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Dataset item class\n",
    "@dataclass\n",
    "class DataItem:\n",
    "    label: str\n",
    "    label_index: int\n",
    "    predicted_index: int\n",
    "    feature_vec: List[np.float32]\n",
    "        \n",
    "    def getItemSummary(self):\n",
    "        return str((self.label, self.label_index, self.predicted_index, self.feature_vec.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62c76c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_schema: ['id', 'label', 'label_index', 'predicted_index', 'feature_id']\n",
      "Dataset: 10000\n",
      "item: ('7', 7, 7, (784,))\n"
     ]
    }
   ],
   "source": [
    "# Read the features and data-item records then merge them\n",
    "# Make the feature_id:feature_vec map\n",
    "feat_records = getRecords(Dataset_path, Feature_table)\n",
    "featid_map = {}\n",
    "for r in feat_records:\n",
    "    feat_id = r[0]       # first column is the feature ID\n",
    "    feat_vec = r[1:]\n",
    "    featid_map[feat_id]  = feat_vec\n",
    "\n",
    "# Read the data-items and put them in DataItem array\n",
    "Dataset = []\n",
    "data_records = getRecords(Dataset_path, Data_table)\n",
    "data_schema = getColNames(Dataset_path, Data_table)\n",
    "print('data_schema:', data_schema)\n",
    "\n",
    "for r in data_records:\n",
    "    label = r[1]\n",
    "    label_index = r[2]\n",
    "    pred_index = r[3]\n",
    "    feat_id = r[4]\n",
    "    feat_vec = np.array(featid_map[feat_id], dtype=np.float32)\n",
    "    item = DataItem(label, label_index, pred_index, feat_vec)\n",
    "    Dataset.append(item)\n",
    "\n",
    "item = Dataset[0]\n",
    "print('Dataset:', len(Dataset))\n",
    "print('item:', item.getItemSummary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0236da",
   "metadata": {},
   "source": [
    "## Load the Trained Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e3cee27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table_names: ['Header', 'sqlite_sequence', 'Hparam_T', 'FC1_Weight_T', 'FC2_Weight_T', 'FC3_Weight_T', 'FC1_Bias_T', 'FC2_Bias_T', 'FC3_Bias_T']\n",
      "\n",
      "('name', 'MLP-100')\n",
      "('architecture', '784-FC:100-FC:100-10')\n",
      "('accuracy', 98.36)\n",
      "('correct_count', 9836)\n",
      "('Hparam.table', 'Hparam_T')\n",
      "('fc1.weight.table', 'FC1_Weight_T')\n",
      "('fc1.bias.table', 'FC1_Bias_T')\n",
      "('fc2.weight.table', 'FC2_Weight_T')\n",
      "('fc2.bias.table', 'FC2_Bias_T')\n",
      "('fc3.weight.table', 'FC3_Weight_T')\n",
      "('fc3.bias.table', 'FC3_Bias_T')\n"
     ]
    }
   ],
   "source": [
    "# Load and check the dataset table\n",
    "Model_path = './saved/trained_mlp100-98.36p.s3db'\n",
    "\n",
    "table_names = getTableNames(Model_path)\n",
    "print('table_names:', table_names)\n",
    "\n",
    "# Read the header table\n",
    "header_records =  getRecords(Model_path, 'Header')\n",
    "Header_dict = {}\n",
    "print('')\n",
    "for r in header_records: \n",
    "    print(r[1:3])\n",
    "    Header_dict[r[1]] = r[2]\n",
    "\n",
    "\n",
    "\n",
    "# Get the table names for later use\n",
    "Fc1w_table = Header_dict['fc1.weight.table']\n",
    "Fc1b_table = Header_dict['fc1.bias.table']\n",
    "Fc2w_table = Header_dict['fc2.weight.table']\n",
    "Fc2b_table = Header_dict['fc2.bias.table']\n",
    "Fc3w_table = Header_dict['fc3.weight.table']\n",
    "Fc3b_table = Header_dict['fc3.bias.table']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "712b62a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a table saved using createMatrixTable as a list of tuples\n",
    "def readMatrixTable(db_path, table_name):\n",
    "    # read the records\n",
    "    rec_list = getRecords(db_path, table_name)\n",
    "    # build the matrix\n",
    "    rec_list.sort()         # sort by row_no (first column)\n",
    "    matrix = []\n",
    "    for rec in rec_list:\n",
    "        matrix.append(rec[1:])  # stripe off the row_no columns\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# test this functions\n",
    "mat1 = np.array(readMatrixTable(Model_path, Fc1w_table))\n",
    "mat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a2dd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParamTable_names: ['FC1_Weight_T', 'FC1_Bias_T', 'FC2_Weight_T', 'FC2_Bias_T', 'FC3_Weight_T', 'FC3_Bias_T']\n",
      "FC1_Weight_T: (100, 784) float64\n",
      "FC1_Bias_T: (100,) float64\n",
      "FC2_Weight_T: (100, 100) float64\n",
      "FC2_Bias_T: (100,) float64\n",
      "FC3_Weight_T: (10, 100) float64\n",
      "FC3_Bias_T: (10,) float64\n"
     ]
    }
   ],
   "source": [
    "# Returns the weights and biases as a dictionary\n",
    "def readModelParam(db_path, table_names):\n",
    "    model_params = {}\n",
    "    for name in table_names:\n",
    "        # read the matrix as a list of tuples\n",
    "        mat = readMatrixTable(db_path, name)\n",
    "        # Check if it is a matrix or a vector\n",
    "        if len(mat)==1: is_vector = True\n",
    "        else: is_vector = False\n",
    "        # convert to numpy array\n",
    "        if is_vector: mat = np.array(mat[0])    # make a 1D array for vectors\n",
    "        else: mat = np.array(mat)\n",
    "        # save it for returning\n",
    "        model_params[name] = mat\n",
    "    return model_params\n",
    "\n",
    "# List of table names for running loop\n",
    "ParamTable_names = [\n",
    "    Fc1w_table,\n",
    "    Fc1b_table,\n",
    "    Fc2w_table,\n",
    "    Fc2b_table,\n",
    "    Fc3w_table,\n",
    "    Fc3b_table\n",
    "]\n",
    "print('ParamTable_names:', ParamTable_names)\n",
    "\n",
    "# Read the model parameters as numpy matrix/vectors\n",
    "model_params = readModelParam(Model_path, ParamTable_names)\n",
    "for k, v in model_params.items():\n",
    "    print(f'{k}:', v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71155ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_weight: (100, 784) float32\n",
      "fc2_weight: (100, 100) float32\n",
      "fc3_weight: (10, 100) float32\n",
      "fc1_bias: (100,) float32\n",
      "fc2_bias: (100,) float32\n",
      "fc3_bias: (10,) float32\n"
     ]
    }
   ],
   "source": [
    "# Model Parameters class\n",
    "@dataclass\n",
    "class mlp100_Params:\n",
    "    fc1_weight: np.ndarray\n",
    "    fc2_weight: np.ndarray\n",
    "    fc3_weight: np.ndarray\n",
    "    fc1_bias: np.ndarray\n",
    "    fc2_bias: np.ndarray\n",
    "    fc3_bias: np.ndarray\n",
    "        \n",
    "\n",
    "# Instantiate the model parameter class with float32 datatype\n",
    "Model_params = mlp100_Params(\n",
    "    model_params[Fc1w_table].astype(np.float32),\n",
    "    model_params[Fc2w_table].astype(np.float32),\n",
    "    model_params[Fc3w_table].astype(np.float32),\n",
    "    model_params[Fc1b_table].astype(np.float32),\n",
    "    model_params[Fc2b_table].astype(np.float32),\n",
    "    model_params[Fc3b_table].astype(np.float32)\n",
    ")\n",
    "\n",
    "# Show the parameter info\n",
    "for field in dataclasses.fields(Model_params):\n",
    "    field_value = getattr(Model_params, field.name)\n",
    "    print(field.name+':', field_value.shape, field_value.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb58293",
   "metadata": {},
   "source": [
    "# Verify the Model on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "434a7de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7\n",
      "('7', 7, 7, (784,))\n"
     ]
    }
   ],
   "source": [
    "# Relu on numpy array\n",
    "def npReLU(np_arr):\n",
    "    return np.maximum(0, np_arr)\n",
    "\n",
    "\n",
    "# Performs the forward inference on the mlp100 model\n",
    "# params: model parameters, an instance of mlp100_Params\n",
    "def mlp100_forward(params, feature_vec):\n",
    "    x1 = params.fc1_weight @ feature_vec + params.fc1_bias\n",
    "    fc1_out = npReLU(x1)\n",
    "    x2 = params.fc2_weight @ fc1_out + params.fc2_bias\n",
    "    fc2_out = npReLU(x2)\n",
    "    fc3_out = params.fc3_weight @ fc2_out + params.fc3_bias\n",
    "    return fc3_out\n",
    "\n",
    "# Uses the forward pass and converts the result into predicted_index\n",
    "def mlp100_predict(params, feature_vec):\n",
    "    out_vec = mlp100_forward(params, feature_vec)\n",
    "    return np.argmax(out_vec)   # return the index of the highest probable class\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = Dataset[0]\n",
    "pred = mlp100_predict(Model_params, item.feature_vec)\n",
    "print('pred:', pred)\n",
    "print(item.getItemSummary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "317d4bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5e8f35f4fc470897b14258e1fbe115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 98.36%   correct_count: 9836   expected-miss: 0   total_count: 10000\n",
      "Expected:  accuracy: 98.36%   correct_count: 9836\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# Validate the Given model on the whole dataset\n",
    "# model_params: instance of mlp100_Params\n",
    "# model_predict: a function that takes model_params and a feacture_vec to compute predicted_index\n",
    "def validateModel(model_params, model_predict):\n",
    "    expect_miss = 0      # keeps track of no. of mismatche between prediction in dataset vs model prediction\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for item in tqdm(Dataset):\n",
    "        pred_index = model_predict(model_params, item.feature_vec)\n",
    "        if pred_index != item.predicted_index: expect_miss += 1    # prediction does not match prediction in dataset\n",
    "        if pred_index == item.label_index: correct_count += 1   # prediction matched the actual label-index\n",
    "        total_count += 1\n",
    "    # Compute and print statistics\n",
    "    accuracy = (100.0 * correct_count) / total_count\n",
    "    print(f'Validation accuracy: {accuracy:.2f}%   correct_count: {correct_count}   expected-miss: {expect_miss}   total_count: {total_count}')\n",
    "    return accuracy, correct_count, expect_miss, total_count\n",
    "\n",
    "\n",
    "validateModel(Model_params, mlp100_predict)\n",
    "print(f'Expected:  accuracy: {Header_dict[\"accuracy\"]}%', '  correct_count:', Header_dict['correct_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513e1458",
   "metadata": {},
   "source": [
    "# Define Fixed-Point Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d3b17",
   "metadata": {},
   "source": [
    "## Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60c355db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fxp_ctor() tests:\r\n",
      "[8, 9, 4, 2, 31, -3, -9]\r\n",
      "[-8. -7.  4.  2. -1. -3.  7.]\r\n",
      "None\r\n",
      "\r\n",
      "None\r\n",
      "[16, 18, -4, -32, 32, -10]\r\n",
      "[-16. -14.  -4.   0.   0. -10.]\r\n",
      "\r\n",
      "\r\n",
      "fxp_getSignBits() tests:\r\n",
      "None\r\n",
      "[1, 1, 1, 0, 0, 0]\r\n",
      "[1 1 1 0 0 0]\r\n",
      "\r\n",
      "None\r\n",
      "[1, 0, 1, 0, 1, 0]\r\n",
      "[1 0 1 0 1 0]\r\n",
      "\r\n",
      "\r\n",
      "fxp_computeStatus tests:\r\n",
      "None\r\n",
      "[ 128  144   64   32  496  -48 -144]\r\n",
      "[-128 -112   64   32  -16  -48  112]\r\n",
      "fxp_Status(overflow=True, overflow_count=4, max_vali=496, min_vali=-144, max_valf=31.0, min_valf=-9.0)\r\n",
      "\r\n",
      "None\r\n",
      "[  512   576  -128 -1024  1024  -320 -2048]\r\n",
      "[-512 -448 -128    0    0 -320    0]\r\n",
      "fxp_Status(overflow=True, overflow_count=5, max_vali=1024, min_vali=-2048, max_valf=64.0, min_valf=-128.0)\r\n",
      "\r\n",
      "\r\n",
      "fxp from matrix tests:\r\n",
      "None\r\n",
      "[[1. 2. 3. 4.]\r\n",
      " [2. 5. 7. 2.]\r\n",
      " [9. 3. 5. 0.]]\r\n"
     ]
    }
   ],
   "source": [
    "# Delete the cache before importing\n",
    "!rm -rf __pycache__/\n",
    "from AK_FixedPoint import *\n",
    "\n",
    "# Run Unit tests to make sure everything is okay\n",
    "!python3 unittest_fxp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa041f2",
   "metadata": {},
   "source": [
    "## Math operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d49bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing fxp_add():\r\n",
      "fxp_FixedPoint:\r\n",
      "  total_width: 10\r\n",
      "  frac_width: 5\r\n",
      "  scale_fact: 32\r\n",
      "  data: type<class 'numpy.ndarray'>  base-type: int64  shape: (6,)\r\n",
      "[ 352  448    0  192  384 -384]\r\n",
      "[ 11.  14.   0.   6.  12. -12.]\r\n",
      "fxp_Status(overflow=True, overflow_count=2, max_vali=640, min_vali=-640, max_valf=20.0, min_valf=-20.0)\r\n",
      "\r\n",
      "\r\n",
      "Testing fxp_sub():\r\n",
      "fxp_FixedPoint:\r\n",
      "  total_width: 10\r\n",
      "  frac_width: 5\r\n",
      "  scale_fact: 32\r\n",
      "  data: type<class 'numpy.ndarray'>  base-type: int64  shape: (8,)\r\n",
      "[ 288  320 -192 -448    0 -384 -512  480]\r\n",
      "[  9.  10.  -6. -14.   0. -12. -16.  15.]\r\n",
      "fxp_Status(overflow=True, overflow_count=3, max_vali=640, min_vali=-544, max_valf=20.0, min_valf=-17.0)\r\n",
      "\r\n",
      "\r\n",
      "Testing Mult:\r\n",
      "fxp_FixedPoint:\r\n",
      "  total_width: 10\r\n",
      "  frac_width: 5\r\n",
      "  scale_fact: 32\r\n",
      "  data: type<class 'numpy.ndarray'>  base-type: int64  shape: (6,)\r\n",
      "[  76  224 -297 -384 -512 -512]\r\n",
      "[  2.375     7.       -9.28125 -12.      -16.      -16.     ]\r\n",
      "['1001100', '11100000', '-100101001', '-110000000', '-1000000000', '-1000000000']\r\n",
      "fxp_Status(overflow=True, overflow_count=1, max_vali=512, min_vali=-512, max_valf=16.0, min_valf=-16.0)\r\n",
      "\r\n",
      "\r\n",
      "Testing wrapping behavior:\r\n",
      "1.0\r\n",
      "2.0\r\n",
      "3.0\r\n",
      "4.0\r\n",
      "5.0\r\n",
      "6.0\r\n",
      "7.0\r\n",
      "-8.0\r\n",
      "-7.0\r\n",
      "-6.0\r\n",
      "-5.0\r\n",
      "-4.0\r\n",
      "-3.0\r\n",
      "-2.0\r\n",
      "-1.0\r\n",
      "0.0\r\n",
      "\r\n",
      "\r\n",
      "Testing fraction precision:\r\n",
      "[9]\r\n",
      "[1.125]\r\n",
      "['1001']\r\n",
      "\r\n",
      "[10]\r\n",
      "[1.25]\r\n",
      "['1010']\r\n",
      "\r\n",
      "[11]\r\n",
      "[1.375]\r\n",
      "['1011']\r\n",
      "\r\n",
      "[12]\r\n",
      "[1.5]\r\n",
      "['1100']\r\n",
      "\r\n",
      "[13]\r\n",
      "[1.625]\r\n",
      "['1101']\r\n",
      "\r\n",
      "[14]\r\n",
      "[1.75]\r\n",
      "['1110']\r\n",
      "\r\n",
      "[15]\r\n",
      "[1.875]\r\n",
      "['1111']\r\n",
      "\r\n",
      "[16]\r\n",
      "[2.]\r\n",
      "['10000']\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "#Run unit tests to make sure everything is okay\n",
    "!python3 unittest_fxp_math.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0ecf08",
   "metadata": {},
   "source": [
    "## Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "633d07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import inf as INF\n",
    "\n",
    "\n",
    "# Performs matrix-vector multiplication and keeps track of error.\n",
    "# status_obj: instance of fxp_Status to get status back\n",
    "# Returns the output vector and a tupel with intermediate results for debugging: resutl, (...)\n",
    "def fxp_matmul_mv(fxp_mat, fxp_vec, status_obj=None, debug=False):\n",
    "    # Make sure all assumptions are met\n",
    "    assert len(fxp_mat._data.shape) == 2, \"fxp_mat must be built from a 2D Numpy array\"\n",
    "    assert len(fxp_vec._data.shape) == 1, \"fxp_mat must be built from a 1D Numpy array\"\n",
    "    assert fxp_mat._data.shape[1] == fxp_vec._data.shape[0], \"Matrix column count not equal vector length\"\n",
    "    \n",
    "    # Get the data-type parameters\n",
    "    t_width = fxp_vec._total_width\n",
    "    f_width = fxp_vec._frac_width\n",
    "    compute_status = True if status_obj != None else False\n",
    "    \n",
    "    # multiply row-wise\n",
    "    prod_np = (fxp_mat._data * fxp_vec._data)   # multiplying raw values\n",
    "    # compute error status for multiplying into 2x wider result (less likely to have errors in this step)\n",
    "    fxp_prod = fxp_makeWider(fxp_mat, 2)  # build 2x wider fxp object\n",
    "    fxp_prod._data = prod_np              # copy the raw product values\n",
    "    prod_stat = fxp_fitData(fxp_prod, compute_status)   # now fit within this precision\n",
    "    if compute_status: \n",
    "        if debug: print('prod_stat:', prod_stat)\n",
    "        fxp_accumulateStatus(status_obj, prod_stat)  # record the multiplication errors\n",
    "        \n",
    "    # Now scale down to original precision before accumulation; record error status\n",
    "    prod_np_down = prod_np >> f_width       # discard lower fraction bits\n",
    "    fxp_prod_down = fxp_makeSame(fxp_mat)   # fxp object with original precision\n",
    "    fxp_prod_down._data = prod_np_down\n",
    "    prod_down_stat = fxp_fitData(fxp_prod_down, compute_status)\n",
    "    if compute_status: \n",
    "        if debug: print('prod_down_stat:', prod_down_stat)\n",
    "        fxp_accumulateStatus(status_obj, prod_down_stat)  # accumulate the scaling errors\n",
    "        \n",
    "    # accumulate along rows; record error status\n",
    "    accum_np = np.sum(fxp_prod_down._data, axis=1)\n",
    "    fxp_accum = fxp_makeSame(fxp_vec)\n",
    "    fxp_accum._data = accum_np\n",
    "    accum_stat = fxp_fitData(fxp_accum, compute_status)\n",
    "    if compute_status: \n",
    "        if debug: print('accum_stat:', accum_stat)\n",
    "        fxp_accumulateStatus(status_obj, accum_stat)  # accumulate the scaling errors\n",
    "    return fxp_accum, (prod_np, fxp_prod, prod_np_down, fxp_prod_down, accum_np, fxp_accum)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "322fe196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31 59 65]\n",
      "prod_stat: fxp_Status(overflow=False, overflow_count=0, max_vali=10240, min_vali=0, max_valf=40.0, min_valf=0.0)\n",
      "prod_down_stat: fxp_Status(overflow=True, overflow_count=2, max_vali=640, min_vali=0, max_valf=40.0, min_valf=0.0)\n",
      "accum_stat: fxp_Status(overflow=False, overflow_count=0, max_vali=496, min_vali=-80, max_valf=31.0, min_valf=-5.0)\n",
      "Overall status: fxp_Status(overflow=True, overflow_count=2, max_vali=10240, min_vali=-80, max_valf=40.0, min_valf=-5.0)\n",
      "\n",
      "fxp_FixedPoint:\n",
      "  total_width: 10\n",
      "  frac_width: 4\n",
      "  scale_fact: 16\n",
      "  data: type<class 'numpy.ndarray'>  base-type: int64  shape: (3,)\n",
      "[31. -5.  1.]\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Test\n",
    "mat_inp = [\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 5, 7, 2],\n",
    "    [9, 3, 5, 0],\n",
    "]\n",
    "vec_inp = [4, 8, 1, 2]\n",
    "mat_np = np.array(mat_inp)\n",
    "vec_np = np.array(vec_inp)\n",
    "res_np = mat_np @ vec_np\n",
    "print(res_np)\n",
    "\n",
    "total_width = 10\n",
    "frac_width = 4\n",
    "stat = fxp_Status(False, 0, -INF, INF, -INF, INF)\n",
    "fxp_mat_inp, _ = fxp_ctor(total_width, frac_width, mat_np)\n",
    "fxp_vec_inp, _ = fxp_ctor(total_width, frac_width, vec_np)\n",
    "fxp_result, dbg = fxp_matmul_mv(fxp_mat_inp, fxp_vec_inp, stat, debug=True)\n",
    "\n",
    "\n",
    "print('Overall status:', stat)\n",
    "\n",
    "print('')\n",
    "fxp_printInfo(fxp_result)\n",
    "fxp_printValue(fxp_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29b219fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4 16  3  8]\n",
      " [ 8 40  7  4]\n",
      " [36 24  5  0]]\n",
      "\n",
      "[[ 4 16  3  8]\n",
      " [ 8 40  7  4]\n",
      " [36 24  5  0]]\n",
      "\n",
      "fxp_FixedPoint:\n",
      "  total_width: 20\n",
      "  frac_width: 8\n",
      "  scale_fact: 256\n",
      "  data: type<class 'numpy.ndarray'>  base-type: int64  shape: (3, 4)\n",
      "[[ 4. 16.  3.  8.]\n",
      " [ 8. 40.  7.  4.]\n",
      " [36. 24.  5.  0.]]\n",
      "\n",
      "fxp_FixedPoint:\n",
      "  total_width: 10\n",
      "  frac_width: 4\n",
      "  scale_fact: 16\n",
      "  data: type<class 'numpy.ndarray'>  base-type: int64  shape: (3, 4)\n",
      "[[  4.  16.   3.   8.]\n",
      " [  8. -24.   7.   4.]\n",
      " [-28.  24.   5.   0.]]\n",
      "\n",
      "fxp_FixedPoint:\n",
      "  total_width: 10\n",
      "  frac_width: 4\n",
      "  scale_fact: 16\n",
      "  data: type<class 'numpy.ndarray'>  base-type: int64  shape: (3,)\n",
      "[31. -5.  1.]\n"
     ]
    }
   ],
   "source": [
    "# Check the intermediate results\n",
    "prod_np, fxp_prod, prod_np_down, fxp_prod_down, accum_np, fxp_accum = dbg\n",
    "print(mat_np * vec_np)\n",
    "print('')\n",
    "\n",
    "print(prod_np >> (2*frac_width))\n",
    "\n",
    "print('')\n",
    "fxp_printInfo(fxp_prod)\n",
    "fxp_printValue(fxp_prod)\n",
    "\n",
    "print('')\n",
    "fxp_printInfo(fxp_prod_down)\n",
    "fxp_printValue(fxp_prod_down)\n",
    "\n",
    "\n",
    "print('')\n",
    "fxp_printInfo(fxp_accum)\n",
    "fxp_printValue(fxp_accum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb215f",
   "metadata": {},
   "source": [
    "# Implement Model in Fixed-Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43614cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Model Parameters to fixed point\n",
    "Fxp_total_width = 32\n",
    "Fxp_frac_width = 16\n",
    "\n",
    "Fxp_model_params = {}\n",
    "for k, param in Model_params.items():\n",
    "    fxp_param, stat = fxp_ctor(Fxp_total_width, Fxp_frac_width, param)\n",
    "    if stat.overflow:\n",
    "        print(stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu on numpy array\n",
    "def fxpReLU(fxp_num):\n",
    "    return np.maximum(0, np_arr)\n",
    "\n",
    "\n",
    "# Performs the forward inference on the mlp100 model\n",
    "# params: model parameters, an instance of mlp100_Params\n",
    "def mlp100_forward_fxp(params, feature_vec):\n",
    "    x1 = params.fc1_weight @ feature_vec + params.fc1_bias\n",
    "    fc1_out = npReLU(x1)\n",
    "    x2 = params.fc2_weight @ fc1_out + params.fc2_bias\n",
    "    fc2_out = npReLU(x2)\n",
    "    fc3_out = params.fc3_weight @ fc2_out + params.fc3_bias\n",
    "    return fc3_out\n",
    "\n",
    "# Uses the forward pass and converts the result into predicted_index\n",
    "def mlp100_predict_fxp(params, feature_vec):\n",
    "    out_vec = mlp100_forward(params, feature_vec)\n",
    "    return np.argmax(out_vec)   # return the index of the highest probable class\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = Dataset[0]\n",
    "pred = mlp100_predict(Model_params, item.feature_vec)\n",
    "print('pred:', pred)\n",
    "print(item.getItemSummary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbac1c",
   "metadata": {},
   "source": [
    "# Experiments for Fixed-Point Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47e441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c647a0b7",
   "metadata": {},
   "source": [
    "# Export Model and Dataset as SQLite3 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39833208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
