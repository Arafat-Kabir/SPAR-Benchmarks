{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e71b6053",
   "metadata": {},
   "source": [
    "# MLP-100 Numpy Model Extraction\n",
    "\n",
    "Loads the Dataset and a Model saved from training notebook then translates into Numpy implementation. The dataset is modified with the accuracy from Numpy model (which should match the original model). Then the model and the dataset is exported as sqlite3 databases for implementation in C.  \n",
    "\n",
    "**NOTE:** The dataset exported by the training notebook may have incorrect predicted index due to several iterations of model training and not updating the dataset. We'll re-run the predictions here and update the predicted index in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d54159",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dccb04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# We don't need GPU for this, not training\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device('cuda')\n",
    "#else:\n",
    "#    device = torch.device('cpu')\n",
    "\n",
    "#print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
    "print('Using PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689a7655",
   "metadata": {},
   "source": [
    "# Load and Validate torch.nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a358c13",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "**NOTE:** Always copy the following cell from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fd90970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP with single hiddend layer with 12 units and ReLU activation.\n",
    "class MLP100(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP100, self).__init__()\n",
    "        # Save parameters\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.debug = False    # can be used to activate debugging features\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, 100)   # 100 hidden units\n",
    "        self.fc1_drop = nn.Dropout(0.2)         # drop-out for faster training, has no effect on inference\n",
    "        self.fc2 = nn.Linear(100, 100)          # 100 hidden units\n",
    "        self.fc2_drop = nn.Dropout(0.2)         # drop-out for faster training, has no effect on inference\n",
    "        self.fc3 = nn.Linear(100, num_classes)  # output layer\n",
    "\n",
    "    # Expects a batch of 1-D tensor\n",
    "    # Dimension of x: (batch-size, input_size)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   # pass through the first hidden layer\n",
    "        x = self.fc1_drop(x)      \n",
    "        x = F.relu(self.fc2(x))   # pass through the second hidden layer\n",
    "        x = self.fc2_drop(x)      \n",
    "        x = self.fc3(x)           # pass through the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f392492e",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92c60bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 244852\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 20 17:42 trained_mlp100-98.11p.pt\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 20 18:07 trained_mlp100-98.34p.pt\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 21 13:55 trained_mlp100.pt\n",
      "-rw-rw-r-- 1 makabir makabir 70809849 Jun 21 13:55 test_dataset.pt\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 21 13:57 trained_mlp100-98.36p.pt\n",
      "-rw-r--r-- 1 makabir makabir    16384 Jun 21 15:02 model.s3db\n",
      "-rw-r--r-- 1 makabir makabir  1818624 Jun 21 16:12 model-mlp100.s3db\n",
      "-rw-r--r-- 1 makabir makabir  1818624 Jun 21 19:52 trained_mlp100-98.36p.s3db\n",
      "-rw-r--r-- 1 makabir makabir 82194432 Jun 21 19:59 mnist_test_data-98.36p.s3db\n",
      "-rw-rw-r-- 1 makabir makabir  8575008 Jun 21 20:11 trained-bak1.zip\n",
      "-rw-r--r-- 1 makabir makabir  1818624 Jun 22 12:24 trained_mlp100.s3db\n",
      "-rw-r--r-- 1 makabir makabir 82198528 Jun 22 12:24 mnist_test_data.s3db\n",
      "\n",
      "dict_keys(['accuracy', 'correct_count', 'Hparam', 'state_dict'])\n",
      "Hparam: {'input_size': 784, 'num_classes': 10}\n",
      "MLP100(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc1_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc2_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr ./saved/\n",
    "print('')\n",
    "\n",
    "# Load saved model dictionary\n",
    "model_path = './saved/trained_mlp100-98.36p.pt'\n",
    "model_dict = torch.load(model_path)\n",
    "print(model_dict.keys())\n",
    "\n",
    "# Parse the values for easier use\n",
    "Accuracy = model_dict['accuracy']\n",
    "Correct_count = model_dict['correct_count']\n",
    "Hparam = model_dict['Hparam']\n",
    "Model_state_dict = model_dict['state_dict']\n",
    "Model_perf = f'Model Performance:   accuracy: {Accuracy:.2f}%   correct_count: {Correct_count}'  # to be used later\n",
    "print('Hparam:', Hparam)\n",
    "\n",
    "# move all weights to cpu\n",
    "for key in Model_state_dict: \n",
    "    Model_state_dict[key] = Model_state_dict[key].to('cpu')\n",
    "\n",
    "# Instantiate the model\n",
    "model_pt = MLP100(Hparam['input_size'], Hparam['num_classes'])\n",
    "model_pt.load_state_dict(Model_state_dict)\n",
    "print(model_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e31252",
   "metadata": {},
   "source": [
    "## Load Saved Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd05bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_dict: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
      "dataset_schema: (label, label_index, predicted_index, feature_length, feature_vector)\n",
      "item-> label: 7, label_index: 7, predicted_index: 7, feature_length: 784, feature_vector size: 784\n"
     ]
    }
   ],
   "source": [
    "# Prints a dataset item\n",
    "def print_dataitem(item):\n",
    "    mstr = f\"label: {item[0]}, label_index: {item[1]}, predicted_index: {item[2]}, feature_length: {item[3]},\"\n",
    "    mstr2 = f\"feature_vector size: {len(item[4])}\"\n",
    "    print(mstr, mstr2)\n",
    "\n",
    "    \n",
    "# Load the test dataset\n",
    "ds_path = './saved/test_dataset.pt'\n",
    "DS_loaded = torch.load(ds_path)\n",
    "for key in DS_loaded:\n",
    "    if key != 'dataset':\n",
    "        print(f'{key}:', DS_loaded[key])\n",
    "\n",
    "# Show an item summary\n",
    "item = DS_loaded['dataset'][0]\n",
    "print('item-> ', end='')\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95abb1c0",
   "metadata": {},
   "source": [
    "## Validate The Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eaa3642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7\n",
      "label: 7, label_index: 7, predicted_index: 7, feature_length: 784, feature_vector size: 784\n"
     ]
    }
   ],
   "source": [
    "# find most likely label index for each element\n",
    "def get_likely_index(tensor):\n",
    "    # convert to tensor from numpy if needed\n",
    "    if not torch.is_tensor(tensor):\n",
    "        tensor = torch.from_numpy(tensor)\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# Given an item form the test_dataset, returns an example for predict() function\n",
    "# numpytype: set it to True to return numpy nd-array\n",
    "def make_example(data_item, numpytype=False):\n",
    "    feature = torch.tensor(data_item[4])\n",
    "    if numpytype: feature = feature.detach().numpy()\n",
    "    return feature\n",
    "\n",
    "\n",
    "# test prediction from dataset item.\n",
    "# ptmodel: set it to True for the PyTorch model\n",
    "def predict(example, model=None, ptmodel=False):  # example: feature_vector\n",
    "    if ptmodel: model.eval()    # set the pytorch model to evaluation mode\n",
    "    # Use the model to predict the label of the image\n",
    "    feature = example\n",
    "    if ptmodel: feature = feature.unsqueeze(0)    # add the batch dimension for the pytorch model\n",
    "    output = model(feature)\n",
    "    pred = get_likely_index(output)\n",
    "    if ptmodel: pred = pred[0]    # removing batch index\n",
    "    return pred.item()\n",
    "\n",
    "\n",
    "# Test predict()\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item)\n",
    "pred = predict(example, model=model_pt, ptmodel=True)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20b2a672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc42f3ec72c947d688485014bd77b274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 98.36%   correct_count: 9836   expected-miss: 0   total_count: 10000\n",
      "Expected Model Performance:   accuracy: 98.36%   correct_count: 9836\n"
     ]
    }
   ],
   "source": [
    "# Validate the Given model on the whole dataset\n",
    "# ptmodel: set it to True for the PyTorch model\n",
    "def validateModel(model=None, ptmodel=False, numpytype=False):\n",
    "    dataset = DS_loaded['dataset']\n",
    "    expect_miss = 0      # keeps track of no. of mismatche between prediction in dataset vs model prediction\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for item in tqdm(dataset):\n",
    "        lbl, lbl_index, pred_index, *_ = item\n",
    "        example = make_example(item, numpytype=numpytype)\n",
    "        pred = predict(example, model=model, ptmodel=ptmodel)\n",
    "        if pred != pred_index: expect_miss += 1    # prediction does not match prediction in dataset\n",
    "        if pred == lbl_index: correct_count += 1   # prediction matched the actual label-index\n",
    "        total_count += 1\n",
    "    # Compute and print statistics\n",
    "    accuracy = (100.0 * correct_count) / total_count\n",
    "    print(f'Validation accuracy: {accuracy:.2f}%   correct_count: {correct_count}   expected-miss: {expect_miss}   total_count: {total_count}')\n",
    "    return accuracy, correct_count, expect_miss, total_count\n",
    "\n",
    "            \n",
    "# Validate the loaded model\n",
    "validateModel(model_pt, ptmodel=True)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53c232f",
   "metadata": {},
   "source": [
    "# Implementation Using torch.tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "243a7470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight: torch.Size([100, 784])\n",
      "fc1.bias  : torch.Size([100])\n",
      "fc2.weight: torch.Size([100, 100])\n",
      "fc2.bias  : torch.Size([100])\n",
      "fc3.weight: torch.Size([10, 100])\n",
      "fc3.bias  : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Extract the weights as torch.tensors\n",
    "for key in Model_state_dict:\n",
    "    print(f'{key:10}:', Model_state_dict[key].size())\n",
    "\n",
    "fc1_weight_pt = Model_state_dict['fc1.weight']\n",
    "fc1_bias_pt = Model_state_dict['fc1.bias']\n",
    "fc2_weight_pt = Model_state_dict['fc2.weight']\n",
    "fc2_bias_pt = Model_state_dict['fc2.bias']\n",
    "fc3_weight_pt = Model_state_dict['fc3.weight']\n",
    "fc3_bias_pt = Model_state_dict['fc3.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c18616ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7\n",
      "label: 7, label_index: 7, predicted_index: 7, feature_length: 784, feature_vector size: 784\n"
     ]
    }
   ],
   "source": [
    "# Define the model using pytorch tensor operations.\n",
    "# Input interface is the same as the \n",
    "def tensorModel(features):\n",
    "    x1 = fc1_weight_pt @ features + fc1_bias_pt\n",
    "    fc1_out = F.relu(x1)\n",
    "    x2 = fc2_weight_pt @ fc1_out + fc2_bias_pt\n",
    "    fc2_out = F.relu(x2)\n",
    "    fc3_out = fc3_weight_pt @ fc2_out + fc3_bias_pt\n",
    "    return fc3_out\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item)\n",
    "pred = predict(example, model=tensorModel, ptmodel=False)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b27a9601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18d60a27de34fdaa86ae7ae5d6ba052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 98.36%   correct_count: 9836   expected-miss: 0   total_count: 10000\n",
      "Expected Model Performance:   accuracy: 98.36%   correct_count: 9836\n"
     ]
    }
   ],
   "source": [
    "# Validate the tensor operation based model\n",
    "validateModel(tensorModel, ptmodel=False)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11aaaa",
   "metadata": {},
   "source": [
    "# Implement Using Numpy Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44d40dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_weight_np: (100, 784)\n"
     ]
    }
   ],
   "source": [
    "# Copy weights as numpy ndarray\n",
    "fc1_weight_np = fc1_weight_pt.detach().numpy()\n",
    "fc1_bias_np   = fc1_bias_pt.detach().numpy()\n",
    "fc2_weight_np = fc2_weight_pt.detach().numpy()\n",
    "fc2_bias_np   = fc2_bias_pt.detach().numpy()\n",
    "fc3_weight_np = fc3_weight_pt.detach().numpy()\n",
    "fc3_bias_np   = fc3_bias_pt.detach().numpy()\n",
    "\n",
    "print('fc1_weight_np:', fc1_weight_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6c908ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7\n",
      "label: 7, label_index: 7, predicted_index: 7, feature_length: 784, feature_vector size: 784\n"
     ]
    }
   ],
   "source": [
    "# Relu on numpy array\n",
    "def npReLU(np_arr):\n",
    "    return np.maximum(0, np_arr)\n",
    "\n",
    "\n",
    "# Define the model using numpy matrix operations\n",
    "# Input interface is the same as the \n",
    "def numpyModel(features):\n",
    "    x1 = fc1_weight_np @ features + fc1_bias_np\n",
    "    fc1_out = npReLU(x1)\n",
    "    x2 = fc2_weight_np @ fc1_out + fc2_bias_np\n",
    "    fc2_out = npReLU(x2)\n",
    "    fc3_out = fc3_weight_np @ fc2_out + fc3_bias_np\n",
    "    return fc3_out\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item, numpytype=True)\n",
    "pred = predict(example, model=numpyModel, ptmodel=False)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "690966c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f408a03083a453aaee52d8844fcb2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 98.36%   correct_count: 9836   expected-miss: 0   total_count: 10000\n",
      "Expected Model Performance:   accuracy: 98.36%   correct_count: 9836\n"
     ]
    }
   ],
   "source": [
    "# Validate the tensor operation based model\n",
    "validateModel(numpyModel, ptmodel=False, numpytype=True)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4729844",
   "metadata": {},
   "source": [
    "# Update the dataset with the Numpy Model Predicted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4393f0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfac7a5d321049009a70f419bb2306c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Fixed 0 predicted_index in the dataset\n"
     ]
    }
   ],
   "source": [
    "enum_iter = tqdm( enumerate(DS_loaded['dataset']), total=len(DS_loaded['dataset']) )\n",
    "fix_count = 0\n",
    "for index, item in enum_iter:\n",
    "    # Make prediction using Numpy model\n",
    "    example = make_example(item, numpytype=True)\n",
    "    pred = predict(example, model=numpyModel, ptmodel=False)\n",
    "    # Check and fix the predicted_index in the dataset\n",
    "    if pred!=item[2]:\n",
    "        DS_loaded['dataset'][index][2] = pred\n",
    "        fix_count += 1\n",
    "\n",
    "print(f'INFO: Fixed {fix_count} predicted_index in the dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82de4be",
   "metadata": {},
   "source": [
    "# Export Numpy Model as sqlite3 DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b938f766",
   "metadata": {},
   "source": [
    "## Create the database file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7909e874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Overwriting existing file ./saved/trained_mlp100.s3db\n",
      "total 243072\r\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 20 17:42 trained_mlp100-98.11p.pt\r\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 20 18:07 trained_mlp100-98.34p.pt\r\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 21 13:55 trained_mlp100.pt\r\n",
      "-rw-rw-r-- 1 makabir makabir 70809849 Jun 21 13:55 test_dataset.pt\r\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 21 13:57 trained_mlp100-98.36p.pt\r\n",
      "-rw-r--r-- 1 makabir makabir    16384 Jun 21 15:02 model.s3db\r\n",
      "-rw-r--r-- 1 makabir makabir  1818624 Jun 21 16:12 model-mlp100.s3db\r\n",
      "-rw-r--r-- 1 makabir makabir  1818624 Jun 21 19:52 trained_mlp100-98.36p.s3db\r\n",
      "-rw-r--r-- 1 makabir makabir 82194432 Jun 21 19:59 mnist_test_data-98.36p.s3db\r\n",
      "-rw-rw-r-- 1 makabir makabir  8575008 Jun 21 20:11 trained-bak1.zip\r\n",
      "-rw-r--r-- 1 makabir makabir 82198528 Jun 22 12:24 mnist_test_data.s3db\r\n",
      "-rw-r--r-- 1 makabir makabir        0 Jun 22 12:25 trained_mlp100.s3db\r\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "\n",
    "# Create the sqlite3 database file, overwrite if exist\n",
    "# Check if the database file already exists\n",
    "def createDB(db_path, overwrite=False):\n",
    "    # Check overwrite conditions\n",
    "    file_exist = os.path.exists(db_path)\n",
    "    if file_exist and overwrite==False:\n",
    "        print(f'ERROR: {db_path} exist, try overwrite=True')\n",
    "        return False\n",
    "    # remove old file if exist\n",
    "    if file_exist:\n",
    "        print('WARN: Overwriting existing file', db_path)\n",
    "        os.remove(db_path)\n",
    "    # Connect to the SQLite database file with overwrite option\n",
    "    conn = sqlite3.connect(db_path, isolation_level=None)\n",
    "    conn.close()\n",
    "    return True\n",
    "\n",
    "\n",
    "DB_path = './saved/trained_mlp100.s3db'\n",
    "createDB(DB_path, overwrite=True)\n",
    "!ls -ltr saved/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164009c7",
   "metadata": {},
   "source": [
    "## Database Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8eb05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of table names in the database file\n",
    "def getTableNames(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch the table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    table_names = cursor.fetchall()\n",
    "    table_names = [name[0] for name in table_names]  # make a list to return\n",
    "    conn.close()\n",
    "    return table_names\n",
    "\n",
    "\n",
    "# Returns a list of column names of the specified table\n",
    "def getColNames(database_filename, table_name):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(database_filename)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Fetch the column names\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    results = cursor.fetchall()\n",
    "    column_names = [result[1] for result in results]  # Extract the column names from the query results\n",
    "\n",
    "    # Close the connection and return \n",
    "    conn.close()\n",
    "    return column_names\n",
    "\n",
    "    \n",
    "# returns all records of a given table\n",
    "def getRecords(db_path, table_name):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Fetch all records from the table\n",
    "    cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    records = cursor.fetchall()\n",
    "    return records\n",
    "\n",
    "\n",
    "# Deletes a table from the database\n",
    "def dropTable(db_path, table_name):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Drop the table if it exists\n",
    "    cursor.execute(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "\n",
    "# Deletes the rows of the given table\n",
    "def deleteRows(db_path, table_name):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Delete all rows from the table\n",
    "    print(f'WARN: Deleting all rows in {table_name}')\n",
    "    cursor.execute(f\"DELETE FROM {table_name}\")\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Checks if a table exist\n",
    "def existTable(db_path, table_name):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Check if the table exists\n",
    "    cursor.execute(f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'\")\n",
    "    result = cursor.fetchone()\n",
    "    if result is None: exist = False\n",
    "    else: exist = True\n",
    "    # Commit the changes and close the connection and return result\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return exist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a1b33f",
   "metadata": {},
   "source": [
    "## Write the header table\n",
    "\n",
    "The header table contains information about the rest of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7bcf511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Header', 'sqlite_sequence']\n"
     ]
    }
   ],
   "source": [
    "# Creates the header table\n",
    "def createHeaderTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = '''CREATE TABLE IF NOT EXISTS Header (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        key TEXT,\n",
    "                        value NUMERIC,\n",
    "                        description TEXT\n",
    "                    )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createHeaderTable(DB_path)\n",
    "table_names = getTableNames(DB_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72e53dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropTable(DB_path, 'Header')\n",
    "#print(getTableNames(DB_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccb24ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'example_key', 'example_value', 'example_description')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserts a record into the Header table\n",
    "def insertHeaderRecord(cursor, record):\n",
    "    key, value, description = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    cursor.execute('''INSERT INTO Header (key, value, description)\n",
    "                      VALUES (?, ?, ?)''', (key, value, description))\n",
    "\n",
    "\n",
    "# Inserts a list of records into the Header table\n",
    "# recordFunc: A function that takes a cursor and a record and inserts into the table (insertHeaderRecord)\n",
    "# record_list: list of tuples (key, target, description)\n",
    "def insertRecordList(db_path, recordFunc, record_list):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Insert the records\n",
    "    for record in record_list:\n",
    "        recordFunc(cursor, record)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB_path, insertHeaderRecord, [('example_key', 'example_value', 'example_description')])\n",
    "getRecords(DB_path, 'Header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08f559d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Deleting all rows in Header\n",
      "(2, 'name', 'MLP-100')\n",
      "(3, 'architecture', '784-FC:100-FC:100-10')\n",
      "(4, 'accuracy', 98.36)\n",
      "(5, 'correct_count', 9836)\n",
      "(6, 'Hparam.table', 'Hparam_T')\n",
      "(7, 'fc1.weight.table', 'FC1_Weight_T')\n",
      "(8, 'fc1.bias.table', 'FC1_Bias_T')\n",
      "(9, 'fc2.weight.table', 'FC2_Weight_T')\n",
      "(10, 'fc2.bias.table', 'FC2_Bias_T')\n",
      "(11, 'fc3.weight.table', 'FC3_Weight_T')\n",
      "(12, 'fc3.bias.table', 'FC3_Bias_T')\n"
     ]
    }
   ],
   "source": [
    "# Define the records as a list\n",
    "Fc1w_table = 'FC1_Weight_T'\n",
    "Fc1b_table = 'FC1_Bias_T'\n",
    "Fc2w_table = 'FC2_Weight_T'\n",
    "Fc2b_table = 'FC2_Bias_T'\n",
    "Fc3w_table = 'FC3_Weight_T'\n",
    "Fc3b_table = 'FC3_Bias_T'\n",
    "Hparam_table = 'Hparam_T'\n",
    "\n",
    "header_records = [\n",
    "    ('name', 'MLP-100', ''),\n",
    "    ('architecture', '784-FC:100-FC:100-10', 'It is an MLP with 2 hidden layer with 100 units in each layer with ReLU activation. Trained on MNIST dataset (output layer with 10 units).'),\n",
    "    ('accuracy', Accuracy, 'Accuracy% of the trained model on the test dataset.'),\n",
    "    ('correct_count', Correct_count, 'Number of correct predictions by the trained model on the test dataset.'),\n",
    "    \n",
    "    ('Hparam.table',   Hparam_table, 'This is the name of the table that contains different parameters of the model.'),\n",
    "    ('fc1.weight.table', Fc1w_table, 'Name of the table containing the fc1.weight matrix'),\n",
    "    ('fc1.bias.table',   Fc1b_table, 'Name of the table containing the fc1.bias vector'),\n",
    "    ('fc2.weight.table', Fc2w_table, 'Name of the table containing the fc2.weight matrix'),\n",
    "    ('fc2.bias.table',   Fc2b_table, 'Name of the table containing the fc2.bias vector'),\n",
    "    ('fc3.weight.table', Fc3w_table, 'Name of the table containing the fc3.weight matrix'),\n",
    "    ('fc3.bias.table',   Fc3b_table, 'Name of the table containing the fc3.bias vector'),\n",
    "]\n",
    "\n",
    "# Insert the header records\n",
    "deleteRows(DB_path, 'Header')  # delete previous records\n",
    "insertRecordList(DB_path, insertHeaderRecord, header_records)\n",
    "records = getRecords(DB_path, 'Header')\n",
    "for r in records: print(r[:-1])   # print all but description field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545b859",
   "metadata": {},
   "source": [
    "## Write the Hparam table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5c8d107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Header', 'sqlite_sequence', 'Hparam_T']\n"
     ]
    }
   ],
   "source": [
    "# Creates the Hparam table\n",
    "def createHparamTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = f'''CREATE TABLE IF NOT EXISTS {Hparam_table} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        key TEXT,\n",
    "                        value NUMERIC,\n",
    "                        description TEXT\n",
    "                    )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createHparamTable(DB_path)\n",
    "table_names = getTableNames(DB_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64d21426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'example_key', 123, 'example_description')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserts a record into the Hparam table\n",
    "def insertHparamRecord(cursor, record):\n",
    "    key, value, description = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    cursor.execute(f'''INSERT INTO {Hparam_table} (key, value, description)\n",
    "                      VALUES (?, ?, ?)''', (key, value, description))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB_path, insertHparamRecord, [('example_key', 123, 'example_description')])\n",
    "getRecords(DB_path, Hparam_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8066bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hparam from pytorch: {'input_size': 784, 'num_classes': 10}\n",
      "WARN: Deleting all rows in Hparam_T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 'input_size', 784, 'Input size of the MLP'),\n",
       " (3, 'num_classes', 10, 'Output size of the MLP'),\n",
       " (4, 'fc1.weight.row', 100, 'No. of rows in fc1.weight'),\n",
       " (5, 'fc1.weight.col', 784, 'No. of columns in fc1.weight'),\n",
       " (6, 'fc1.bias.len', 100, 'Lengths of the fc1.bias vector'),\n",
       " (7, 'fc2.weight.row', 100, 'No. of rows in fc2.weight'),\n",
       " (8, 'fc2.weight.col', 100, 'No. of columns in fc2.weight'),\n",
       " (9, 'fc2.bias.len', 100, 'Lengths of the fc2.bias vector'),\n",
       " (10, 'fc3.weight.row', 10, 'No. of rows in fc3.weight'),\n",
       " (11, 'fc3.weight.col', 100, 'No. of columns in fc3.weight'),\n",
       " (12, 'fc3.bias.len', 10, 'Lengths of the fc3.bias vector')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert the Hparam records\n",
    "print('Hparam from pytorch:', Hparam)\n",
    "hparam_records = [\n",
    "    ('input_size', Hparam['input_size'], 'Input size of the MLP'),\n",
    "    ('num_classes', Hparam['num_classes'], 'Output size of the MLP'),\n",
    "    \n",
    "    ('fc1.weight.row', len(fc1_weight_np), 'No. of rows in fc1.weight'),\n",
    "    ('fc1.weight.col', len(fc1_weight_np[0]), 'No. of columns in fc1.weight'),\n",
    "    ('fc1.bias.len', len(fc1_bias_np), 'Lengths of the fc1.bias vector'),\n",
    "    \n",
    "    ('fc2.weight.row', len(fc2_weight_np), 'No. of rows in fc2.weight'),\n",
    "    ('fc2.weight.col', len(fc2_weight_np[0]), 'No. of columns in fc2.weight'),\n",
    "    ('fc2.bias.len', len(fc2_bias_np), 'Lengths of the fc2.bias vector'),\n",
    "    \n",
    "    ('fc3.weight.row', len(fc3_weight_np), 'No. of rows in fc3.weight'),\n",
    "    ('fc3.weight.col', len(fc3_weight_np[0]), 'No. of columns in fc3.weight'),\n",
    "    ('fc3.bias.len', len(fc3_bias_np), 'Lengths of the fc3.bias vector'),\n",
    "]\n",
    "\n",
    "\n",
    "deleteRows(DB_path, Hparam_table)\n",
    "insertRecordList(DB_path, insertHparamRecord, hparam_records)\n",
    "getRecords(DB_path, Hparam_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99581057",
   "metadata": {},
   "source": [
    "## Write the weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b5d84ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col_names: ['row_no', 'col_0', 'col_1', 'col_2', 'col_3'] ... ['col_779', 'col_780', 'col_781', 'col_782', 'col_783']\n",
      "records[i]: (2, -3.39176e-40, 1.06084e-40, 3.82982e-40, 1.488e-40) ...\n"
     ]
    }
   ],
   "source": [
    "# Saves a numpy 2D array as a table in the database.\n",
    "# Columns: ID, row_no, col_0, col_1, ..., col_n\n",
    "def createMatrixTable(db_path, table_name, nparray, overwrite=False):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop the table if it exists and overwrite requested\n",
    "    table_exist = existTable(db_path, table_name)\n",
    "    if overwrite and table_exist:\n",
    "        print(f'WARN: Overwriting table {table_name}')\n",
    "        dropTable(db_path, table_name)\n",
    "        \n",
    "    # Create the table\n",
    "    rows, cols = nparray.shape\n",
    "    column_names = \"row_no, \" + \", \".join([f\"col_{i}\" for i in range(cols)])  # Generate the column names string\n",
    "    cursor.execute(f\"CREATE TABLE {table_name} ({column_names})\")\n",
    "\n",
    "    # Insert the array rows into the table\n",
    "    for i in range(rows):\n",
    "        vals = f'{i}, ' + ', '.join(map(str, nparray[i]))\n",
    "        cursor.execute(f\"INSERT INTO {table_name} VALUES ({vals})\")\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# test createMatrixTable()\n",
    "createMatrixTable(DB_path, 'test', fc1_weight_np, overwrite=True)\n",
    "col_names = getColNames(DB_path, 'test')\n",
    "records = getRecords(DB_path, 'test')\n",
    "print('col_names:', col_names[:5], '...', col_names[-5:])\n",
    "print('records[i]:', records[2][:5], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e494dcfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1b shape: (1, 100)\n"
     ]
    }
   ],
   "source": [
    "# Save the weights\n",
    "createMatrixTable(DB_path, Fc1w_table, fc1_weight_np, overwrite=True)\n",
    "createMatrixTable(DB_path, Fc2w_table, fc2_weight_np, overwrite=True)\n",
    "createMatrixTable(DB_path, Fc3w_table, fc3_weight_np, overwrite=True)\n",
    "\n",
    "# Convert vectors into 2D array for the table\n",
    "fc1b = np.expand_dims(fc1_bias_np, axis=0)\n",
    "fc2b = np.expand_dims(fc2_bias_np, axis=0)\n",
    "fc3b = np.expand_dims(fc3_bias_np, axis=0)\n",
    "print('fc1b shape:', fc1b.shape)\n",
    "\n",
    "createMatrixTable(DB_path, Fc1b_table, fc1b, overwrite=True)\n",
    "createMatrixTable(DB_path, Fc2b_table, fc2b, overwrite=True)\n",
    "createMatrixTable(DB_path, Fc3b_table, fc3b, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6221a66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Dropped table test\n",
      "INFO: 1 tables dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Header',\n",
       " 'sqlite_sequence',\n",
       " 'Hparam_T',\n",
       " 'FC1_Weight_T',\n",
       " 'FC2_Weight_T',\n",
       " 'FC3_Weight_T',\n",
       " 'FC1_Bias_T',\n",
       " 'FC2_Bias_T',\n",
       " 'FC3_Bias_T']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop extra tables\n",
    "keep_tables = {'sqlite_sequence', 'Header', 'Hparam_T', \n",
    "               'FC1_Weight_T', 'FC2_Weight_T', 'FC3_Weight_T', \n",
    "               'FC1_Bias_T', 'FC2_Bias_T', 'FC3_Bias_T'}\n",
    "\n",
    "all_tables = getTableNames(DB_path)\n",
    "cnt = 0\n",
    "for name in all_tables:\n",
    "    if name not in keep_tables:\n",
    "        dropTable(DB_path, name)\n",
    "        print(f'WARN: Dropped table {name}')\n",
    "        cnt += 1\n",
    "print(f'INFO: {cnt} tables dropped')\n",
    "\n",
    "all_tables = getTableNames(DB_path)\n",
    "ipd.display(all_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4a63f",
   "metadata": {},
   "source": [
    "## Import Saved Model and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41e68b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header:\n",
      "('name', 'MLP-100')\n",
      "('architecture', '784-FC:100-FC:100-10')\n",
      "('accuracy', 98.36)\n",
      "('correct_count', 9836)\n",
      "('Hparam.table', 'Hparam_T')\n",
      "('fc1.weight.table', 'FC1_Weight_T')\n",
      "('fc1.bias.table', 'FC1_Bias_T')\n",
      "('fc2.weight.table', 'FC2_Weight_T')\n",
      "('fc2.bias.table', 'FC2_Bias_T')\n",
      "('fc3.weight.table', 'FC3_Weight_T')\n",
      "('fc3.bias.table', 'FC3_Bias_T')\n",
      "\n",
      "Hparam:\n",
      "('input_size', 784)\n",
      "('num_classes', 10)\n",
      "('fc1.weight.row', 100)\n",
      "('fc1.weight.col', 784)\n",
      "('fc1.bias.len', 100)\n",
      "('fc2.weight.row', 100)\n",
      "('fc2.weight.col', 100)\n",
      "('fc2.bias.len', 100)\n",
      "('fc3.weight.row', 10)\n",
      "('fc3.weight.col', 100)\n",
      "('fc3.bias.len', 10)\n"
     ]
    }
   ],
   "source": [
    "# Check the meta tables\n",
    "rec_list = getRecords(DB_path, 'Header')\n",
    "print('Header:')\n",
    "for r in rec_list: print(r[1:-1])\n",
    "\n",
    "print('')\n",
    "rec_list = getRecords(DB_path, 'Hparam_T')\n",
    "print('Hparam:')\n",
    "for r in rec_list: print(r[1:-1])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b60ca94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 784)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a table saved using createMatrixTable as a list of tuples\n",
    "def readMatrixTable(db_path, table_name):\n",
    "    # read the records\n",
    "    rec_list = getRecords(db_path, table_name)\n",
    "    # build the matrix\n",
    "    rec_list.sort()         # sort by row_no (first column)\n",
    "    matrix = []\n",
    "    for rec in rec_list:\n",
    "        matrix.append(rec[1:])  # stripe off the row_no columns\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# test this functions\n",
    "mat1 = np.array(readMatrixTable(DB_path, Fc1w_table))\n",
    "mat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e10317ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FC1_Weight_T: (100, 784) float64\n",
      "FC2_Weight_T: (100, 100) float64\n",
      "FC3_Weight_T: (10, 100) float64\n",
      "FC1_Bias_T: (100,) float64\n",
      "FC2_Bias_T: (100,) float64\n",
      "FC3_Bias_T: (10,) float64\n"
     ]
    }
   ],
   "source": [
    "# Returns the weights and biases as a dictionary\n",
    "def readModelParam(db_path, table_names):\n",
    "    model_params = {}\n",
    "    for name in table_names:\n",
    "        # read the matrix as a list of tuples\n",
    "        mat = readMatrixTable(db_path, name)\n",
    "        # Check if it is a matrix or a vector\n",
    "        if len(mat)==1: is_vector = True\n",
    "        else: is_vector = False\n",
    "        # convert to numpy array\n",
    "        if is_vector: mat = np.array(mat[0])    # make a 1D array for vectors\n",
    "        else: mat = np.array(mat)\n",
    "        # save it for returning\n",
    "        model_params[name] = mat\n",
    "    return model_params\n",
    "        \n",
    "\n",
    "# test this function\n",
    "param_tables = [\n",
    "    'FC1_Weight_T',\n",
    "    'FC2_Weight_T',\n",
    "    'FC3_Weight_T',\n",
    "    'FC1_Bias_T',\n",
    "    'FC2_Bias_T',\n",
    "    'FC3_Bias_T'\n",
    "]\n",
    "\n",
    "model_params = readModelParam(DB_path, param_tables)\n",
    "for k, v in model_params.items():\n",
    "    print(f'{k}:', v.shape, v.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcdda57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing: FC1_Weight_T\n",
      "min: -1.2509781   max: 0.66414034\n",
      "diff: 5.6259155334359434e-08\n",
      "\n",
      "Comparing: FC2_Weight_T\n",
      "min: -0.6132809   max: 0.705184\n",
      "diff: 2.8821563691217023e-08\n",
      "\n",
      "Comparing: FC3_Weight_T\n",
      "min: -0.55377436   max: 0.40687668\n",
      "diff: 1.629867552033204e-08\n",
      "\n",
      "Comparing: FC1_Bias_T\n",
      "min: -0.8386414   max: 0.53778327\n",
      "diff: 2.9688262959126632e-08\n",
      "\n",
      "Comparing: FC2_Bias_T\n",
      "min: -0.5823776   max: 1.2394397\n",
      "diff: 4.467773440097744e-08\n",
      "\n",
      "Comparing: FC3_Bias_T\n",
      "min: -0.41325337   max: 0.74696374\n",
      "diff: 6.000137331430011e-09\n"
     ]
    }
   ],
   "source": [
    "# Compare with original weights\n",
    "org_params = {\n",
    "    'FC1_Weight_T': fc1_weight_np,\n",
    "    'FC2_Weight_T': fc2_weight_np,\n",
    "    'FC3_Weight_T': fc3_weight_np,\n",
    "    'FC1_Bias_T': fc1_bias_np,\n",
    "    'FC2_Bias_T': fc2_bias_np,\n",
    "    'FC3_Bias_T': fc3_bias_np\n",
    "}\n",
    "\n",
    "def compare_model_params(model_params, org_params, tolerance):\n",
    "    for k in model_params:\n",
    "        print('\\nComparing:', k)\n",
    "        db_val = model_params[k]\n",
    "        org_val = org_params[k]\n",
    "        dmin = np.min(org_val)\n",
    "        dmax = np.max(org_val)\n",
    "        print('min:', dmin, '  max:', dmax)\n",
    "        diff_val = np.max(np.abs(db_val - org_val))   # get the maximum difference\n",
    "        print('diff:', diff_val)\n",
    "        assert diff_val <= tolerance   # use manual check\n",
    "        assert np.allclose(db_val, org_val, rtol=tolerance)  # use numpy built-in check\n",
    "\n",
    "\n",
    "# compare with tolerance\n",
    "tolerance = 1e-6\n",
    "compare_model_params(model_params, org_params, tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac9bbf9",
   "metadata": {},
   "source": [
    "# Export the Dataset as sqlite3 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43bdb45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Overwriting existing file ./saved/mnist_test_data.s3db\n",
      "total 164572\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 20 17:42 trained_mlp100-98.11p.pt\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 20 18:07 trained_mlp100-98.34p.pt\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 21 13:55 trained_mlp100.pt\n",
      "-rw-rw-r-- 1 makabir makabir 70809849 Jun 21 13:55 test_dataset.pt\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 21 13:57 trained_mlp100-98.36p.pt\n",
      "-rw-r--r-- 1 makabir makabir    16384 Jun 21 15:02 model.s3db\n",
      "-rw-r--r-- 1 makabir makabir  1818624 Jun 21 16:12 model-mlp100.s3db\n",
      "-rw-r--r-- 1 makabir makabir  1818624 Jun 21 19:52 trained_mlp100-98.36p.s3db\n",
      "-rw-r--r-- 1 makabir makabir 82194432 Jun 21 19:59 mnist_test_data-98.36p.s3db\n",
      "-rw-rw-r-- 1 makabir makabir  8575008 Jun 21 20:11 trained-bak1.zip\n",
      "-rw-r--r-- 1 makabir makabir  1818624 Jun 22 12:25 trained_mlp100.s3db\n",
      "-rw-r--r-- 1 makabir makabir        0 Jun 22 12:25 mnist_test_data.s3db\n"
     ]
    }
   ],
   "source": [
    "# Create the database file\n",
    "DB2_path = './saved/mnist_test_data.s3db'\n",
    "createDB(DB2_path, overwrite=True)\n",
    "!ls -ltr saved/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f70d1",
   "metadata": {},
   "source": [
    "## Save the Header table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c315a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Header', 'sqlite_sequence']\n"
     ]
    }
   ],
   "source": [
    "# Create the table and check \n",
    "createHeaderTable(DB2_path)\n",
    "table_names = getTableNames(DB2_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e0099a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Deleting all rows in Header\n",
      "(1, 'name', 'MNIST-Test')\n",
      "(2, 'feature_length', 784)\n",
      "(3, 'accuracy', 98.36)\n",
      "(4, 'labels.table', 'Labels_T')\n",
      "(5, 'dataset.table', 'DataItems_T')\n",
      "(6, 'dataitem.schema', '')\n",
      "(7, 'features.table', 'Features_T')\n"
     ]
    }
   ],
   "source": [
    "# Define the records as a list\n",
    "Label_table = 'Labels_T'\n",
    "Dataitem_table = 'DataItems_T'\n",
    "Feature_table = 'Features_T'\n",
    "\n",
    "item = DS_loaded['dataset'][0]\n",
    "Feature_len = len(item[-1])    # last field in the item is the feature_vector\n",
    "\n",
    "header_records = [\n",
    "    ('name', 'MNIST-Test', 'Features (flattened images) extracted from the test dataset of MNIST.'),\n",
    "    ('feature_length', Feature_len, 'The length of the feature. These features can be directly fed to the MLP100 model'),\n",
    "    ('accuracy', Accuracy, 'Accuracy of the MLP100 model used to generate the \"predicted_index\" values.'),\n",
    "    \n",
    "    ('labels.table',   Label_table, 'Index to label mapping. The model predicts an index, which can be converted to the label using this table'),\n",
    "    ('dataset.table', Dataitem_table, 'This table serves as the (label, feature) list. The actual features are stored in a separate table.'), \n",
    "    ('dataitem.schema', '', 'There are 3 label-related fields in the dataset.table: \"label\" is the ground-truth, \"label_index\" is the index into the index-to-label mapping, \"predicted_index\" is the index predicted by the trained MLP100 model'),\n",
    "    ('features.table', Feature_table, 'Contains the actual features for the model.'),\n",
    "]\n",
    "\n",
    "# Insert the header records\n",
    "deleteRows(DB2_path, 'Header')  # delete previous records\n",
    "insertRecordList(DB2_path, insertHeaderRecord, header_records)\n",
    "records = getRecords(DB2_path, 'Header')\n",
    "for r in records: print(r[:-1])   # print all but description field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7109fb4",
   "metadata": {},
   "source": [
    "## Save the Index-to-label mapping table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c53a162f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Header', 'sqlite_sequence', 'Labels_T']\n"
     ]
    }
   ],
   "source": [
    "# Creates the labels table\n",
    "def createLableTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = f'''CREATE TABLE IF NOT EXISTS {Label_table} (\n",
    "                    label_index INTEGER PRIMARY KEY,\n",
    "                    label TEXT\n",
    "                )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createLableTable(DB2_path)\n",
    "table_names = getTableNames(DB2_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b7ae8891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Deleting all rows in Labels_T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-1, 'test')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserts a record into the labels table\n",
    "def insertLabelRecord(cursor, record):\n",
    "    label_index, label = record  # this serves as a soft check for the record format\n",
    "    # Insert the record into the table\n",
    "    query = f\"INSERT INTO {Label_table} (label_index, label) VALUES (?, ?)\"\n",
    "    cursor.execute(query, (label_index, label))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "deleteRows(DB2_path, Label_table)\n",
    "insertRecordList(DB2_path, insertLabelRecord, [(-1, 'test')])\n",
    "getRecords(DB2_path, Label_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b1b70fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_records: [(0, '0'), (1, '1'), (2, '2'), (3, '3'), (4, '4'), (5, '5'), (6, '6'), (7, '7'), (8, '8'), (9, '9')]\n",
      "WARN: Deleting all rows in Labels_T\n",
      "rec_list: [(0, '0'), (1, '1'), (2, '2'), (3, '3'), (4, '4'), (5, '5'), (6, '6'), (7, '7'), (8, '8'), (9, '9')]\n"
     ]
    }
   ],
   "source": [
    "# Build the label records\n",
    "labels_dict = DS_loaded['label_dict']\n",
    "label_records = [(label_index, str(label)) for label, label_index in labels_dict.items()]\n",
    "print('label_records:', label_records)\n",
    "\n",
    "# Store them in the table\n",
    "deleteRows(DB2_path, Label_table)\n",
    "insertRecordList(DB2_path, insertLabelRecord, label_records)\n",
    "rec_list = getRecords(DB2_path, Label_table)\n",
    "print('rec_list:', rec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0873cc2c",
   "metadata": {},
   "source": [
    "## Save the data-items and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2bf4dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item[-1].type: <class 'list'>\n",
      "\n",
      "dataitem_records: 10000 4\n",
      "feature_records: 10000 785\n"
     ]
    }
   ],
   "source": [
    "# Split the dataitems for DataItem table and Features table\n",
    "dataitem_records = []\n",
    "feature_records = []\n",
    "\n",
    "item = DS_loaded['dataset'][0]\n",
    "print('item[-1].type:', type(item[-1]))\n",
    "\n",
    "for item_index, item in enumerate(DS_loaded['dataset']):\n",
    "    label, label_index, pred_index, feat_len, feat_vec = item   # parse the item\n",
    "    feat_id = item_index      # use the index in the dataset as the feature ID\n",
    "    item_rec = [label, label_index, pred_index, feat_id]\n",
    "    feat_rec = [feat_id] + feat_vec    # feature-record: (feature-id, col_0, col_1, ...)\n",
    "    dataitem_records.append(item_rec)\n",
    "    feature_records.append(feat_rec)\n",
    "\n",
    "# check the records\n",
    "print('')\n",
    "print('dataitem_records:', len(dataitem_records), len(dataitem_records[0]))\n",
    "print('feature_records:', len(feature_records), len(feature_records[0]))\n",
    "\n",
    "check_index = 100\n",
    "item = DS_loaded['dataset'][check_index]\n",
    "assert feature_records[check_index][1:] == item[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a8288",
   "metadata": {},
   "source": [
    "### Save Data Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed5ce99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Header', 'sqlite_sequence', 'Labels_T', 'DataItems_T']\n"
     ]
    }
   ],
   "source": [
    "# Creates the dataset table to save the data-items\n",
    "def createDataTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Define the table name and column names\n",
    "    table_name = Dataitem_table\n",
    "    columns = [\"id INTEGER PRIMARY KEY AUTOINCREMENT\",\n",
    "               \"label TEXT\",\n",
    "               \"label_index INTEGER\",\n",
    "               \"predicted_index INTEGER\",\n",
    "               \"feature_id INTEGER\"]\n",
    "\n",
    "    # CCreate the table\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(columns)})\"\n",
    "    cursor.execute(query)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table\n",
    "createDataTable(DB2_path)\n",
    "table_names = getTableNames(DB2_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88c175c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'Item 1', 1, 2, 3)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inserts a record into the Dataset table\n",
    "def insertDataRecord(cursor, record):\n",
    "    label, label_index, predicted_index, feature_id = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    query = f\"INSERT INTO {Dataitem_table} (label, label_index, predicted_index, feature_id) VALUES (?, ?, ?, ?)\"\n",
    "    cursor.execute(query, (label, label_index, predicted_index, feature_id))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB2_path, insertDataRecord, [(\"Item 1\", 1, 2, 3)])\n",
    "getRecords(DB2_path, Dataitem_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f0b2ab05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: Deleting all rows in DataItems_T\n",
      "rec_list: 10000 5\n",
      "rec_list[0] (2, '7', 7, 7, 0)\n"
     ]
    }
   ],
   "source": [
    "# insert all dataset records\n",
    "deleteRows(DB2_path, Dataitem_table)   # delete old records\n",
    "insertRecordList(DB2_path, insertDataRecord, dataitem_records)\n",
    "rec_list = getRecords(DB2_path, Dataitem_table)\n",
    "print('rec_list:', len(rec_list), len(rec_list[0]))\n",
    "print('rec_list[0]', rec_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9253cc4a",
   "metadata": {},
   "source": [
    "### Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a9be216c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ec894ee14c444a950abaab6c886dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Header', 'sqlite_sequence', 'Labels_T', 'DataItems_T', 'Features_T']\n",
      "rec_list: 10000\n"
     ]
    }
   ],
   "source": [
    "# Creates the features table to save the feature_vectors\n",
    "# Columns: feature_id, col_0, col_1, ..., col_n\n",
    "def createFeatureTable(db_path, table_name, feature_list, overwrite=False):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop the table if it exists and overwrite requested\n",
    "    table_exist = existTable(db_path, table_name)\n",
    "    if overwrite and table_exist:\n",
    "        print(f'WARN: Overwriting table {table_name}')\n",
    "        dropTable(db_path, table_name)\n",
    "        \n",
    "    # Create the table\n",
    "    cols = len(feature_list[0]) - 1    # ommitting the feature-id column from count\n",
    "    column_names = \"feature_id, \" + \", \".join([f\"col_{i}\" for i in range(cols)])  # Generate the column names string\n",
    "    cursor.execute(f\"CREATE TABLE {table_name} ({column_names})\")\n",
    "\n",
    "    # Insert the featurs into the table\n",
    "    for feat_item in tqdm(feature_list):\n",
    "        feat_id = feat_item[0]\n",
    "        feat_vec = feat_item[1:]\n",
    "        vals = f'{feat_id}, ' + ', '.join(map(str, feat_vec))\n",
    "        cursor.execute(f\"INSERT INTO {table_name} VALUES ({vals})\")\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table\n",
    "createFeatureTable(DB2_path, Feature_table, feature_records, overwrite=True)\n",
    "table_names = getTableNames(DB2_path)\n",
    "print(table_names)\n",
    "\n",
    "rec_list = getRecords(DB2_path, Feature_table)\n",
    "print('rec_list:', len(rec_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ada5c80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feat_item: 785\n",
      "{<class 'int'>: 1, <class 'float'>: 784}\n"
     ]
    }
   ],
   "source": [
    "feat_item = rec_list[0]\n",
    "print('feat_item:', len(feat_item))\n",
    "type_count = {}\n",
    "for d in feat_item: \n",
    "    t = type(d)\n",
    "    if t not in type_count: type_count[t] = 0\n",
    "    type_count[t] += 1\n",
    "print(type_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2dc18a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 0 tables dropped\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Header', 'sqlite_sequence', 'Labels_T', 'DataItems_T', 'Features_T']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop extra tables ---------------\n",
    "keep_tables = {'Header', 'sqlite_sequence', 'Labels_T', 'DataItems_T', 'Features_T'}\n",
    "\n",
    "all_tables = getTableNames(DB2_path)\n",
    "cnt = 0\n",
    "for name in all_tables:\n",
    "    if name not in keep_tables:\n",
    "        dropTable(DB2_path, name)\n",
    "        print(f'WARN: Dropped table {name}')\n",
    "        cnt += 1\n",
    "print(f'INFO: {cnt} tables dropped')\n",
    "\n",
    "all_tables = getTableNames(DB2_path)\n",
    "ipd.display(all_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337228d5",
   "metadata": {},
   "source": [
    "## Import Saved Dataset and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdba792c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header:\n",
      "('name', 'MNIST-Test')\n",
      "('feature_length', 784)\n",
      "('accuracy', 98.36)\n",
      "('labels.table', 'Labels_T')\n",
      "('dataset.table', 'DataItems_T')\n",
      "('dataitem.schema', '')\n",
      "('features.table', 'Features_T')\n"
     ]
    }
   ],
   "source": [
    "# Check the meta tables\n",
    "rec_list = getRecords(DB2_path, 'Header')\n",
    "print('Header:')\n",
    "for r in rec_list: print(r[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65ebc7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8fdb6038a3488289527e9d20cb05ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Compared 10000 records\n"
     ]
    }
   ],
   "source": [
    "# Compare with original features and dataitems\n",
    "def compare_dataset(db_items, org_items, tolerance):\n",
    "    range_iter = tqdm(range(len(db_items)))\n",
    "    for i in range_iter:\n",
    "        #print(i)\n",
    "        db_rec = db_items[i]\n",
    "        org_rec = org_items[i]\n",
    "        # Compare the labels\n",
    "        db_labels = db_rec[:3]\n",
    "        org_labels = org_rec[:3]\n",
    "        #print('db_rec:', db_rec)\n",
    "        #print('org_rec:', org_rec)\n",
    "        assert db_labels==org_labels, \"Labels mismatch\"\n",
    "        #if i==5: break\n",
    "        # Check features\n",
    "        org_feat = np.array(org_rec[-1])\n",
    "        db_feat = np.array(db_rec[-1])\n",
    "        assert np.allclose(org_feat, db_feat, tolerance), \"Feature vector mismatch\"\n",
    "    print(f'INFO: Compared {(i+1)} records')\n",
    "\n",
    "\n",
    "# merge the tables to make similar records as in DS_loaded['dataset']\n",
    "# build a feat_id: feat_vec map for merging.\n",
    "feat_records = getRecords(DB2_path, Feature_table)\n",
    "feat_rec_map = {}\n",
    "for fitem in feat_records:\n",
    "    feat_id = fitem[0]\n",
    "    feat_vec = fitem[1:]\n",
    "    feat_rec_map[feat_id] = feat_vec\n",
    "\n",
    "# merge feature vectors with dataset items for comparison\n",
    "data_records = getRecords(DB2_path, Dataitem_table)\n",
    "db_items = []\n",
    "for drec in data_records:\n",
    "    feat_id = drec[-1]\n",
    "    feat_vec = feat_rec_map[feat_id]\n",
    "    merged_item = list(drec[1:4]) + [feat_vec]\n",
    "    merged_item[0] = int(merged_item[0])\n",
    "    db_items.append(merged_item)\n",
    "        \n",
    "# compare with tolerance\n",
    "tolerance = 1e-6\n",
    "compare_dataset(db_items, DS_loaded['dataset'], tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c6d95",
   "metadata": {},
   "source": [
    "# Concluding Remarks\n",
    "\n",
    "Now you can use these databases to translate the Numpy model into C implementations. You can also run experiments on fixed-point precisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
