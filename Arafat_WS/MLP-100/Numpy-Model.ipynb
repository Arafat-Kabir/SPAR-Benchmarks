{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22e3bdd",
   "metadata": {},
   "source": [
    "# MLP-100 Numpy Model Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839cf4c",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d292ece2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.0.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# We don't need GPU for this, not training\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device('cuda')\n",
    "#else:\n",
    "#    device = torch.device('cpu')\n",
    "\n",
    "#print('Using PyTorch version:', torch.__version__, ' Device:', device)\n",
    "print('Using PyTorch version:', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a7f98",
   "metadata": {},
   "source": [
    "# Load and Validate torch.nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24383f",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "**NOTE:** Always copy the following cell from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "415b90f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP with single hiddend layer with 12 units and ReLU activation.\n",
    "class MLP100(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP100, self).__init__()\n",
    "        # Save parameters\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.debug = False    # can be used to activate debugging features\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, 100)   # 100 hidden units\n",
    "        self.fc1_drop = nn.Dropout(0.2)         # drop-out for faster training, has no effect on inference\n",
    "        self.fc2 = nn.Linear(100, 100)          # 100 hidden units\n",
    "        self.fc2_drop = nn.Dropout(0.2)         # drop-out for faster training, has no effect on inference\n",
    "        self.fc3 = nn.Linear(100, num_classes)  # output layer\n",
    "\n",
    "    # Expects a batch of 1-D tensor\n",
    "    # Dimension of x: (batch-size, input_size)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   # pass through the first hidden layer\n",
    "        x = self.fc1_drop(x)      \n",
    "        x = F.relu(self.fc2(x))   # pass through the second hidden layer\n",
    "        x = self.fc2_drop(x)      \n",
    "        x = self.fc3(x)           # pass through the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf3b3f2",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa86db5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 69864\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 20 17:41 trained_mlp100.pt\n",
      "-rw-rw-r-- 1 makabir makabir 70809849 Jun 20 17:41 test_dataset.pt\n",
      "-rw-rw-r-- 1 makabir makabir   360783 Jun 20 17:42 trained_mlp100-98.11p.pt\n",
      "\n",
      "dict_keys(['accuracy', 'correct_count', 'Hparam', 'state_dict'])\n",
      "Hparam: {'input_size': 784, 'num_classes': 10}\n",
      "MLP100(\n",
      "  (fc1): Linear(in_features=784, out_features=100, bias=True)\n",
      "  (fc1_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc2_drop): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "!ls -ltr ./saved/\n",
    "print('')\n",
    "\n",
    "# Load saved model dictionary\n",
    "model_path = './saved/trained_mlp100-98.11p.pt'\n",
    "model_dict = torch.load(model_path)\n",
    "print(model_dict.keys())\n",
    "\n",
    "# Parse the values for easier use\n",
    "Accuracy = model_dict['accuracy']\n",
    "Correct_count = model_dict['correct_count']\n",
    "Hparam = model_dict['Hparam']\n",
    "Model_state_dict = model_dict['state_dict']\n",
    "Model_perf = f'Model Performance:   accuracy: {Accuracy:.2f}%   correct_count: {Correct_count}'  # to be used later\n",
    "print('Hparam:', Hparam)\n",
    "\n",
    "# move all weights to cpu\n",
    "for key in Model_state_dict: \n",
    "    Model_state_dict[key] = Model_state_dict[key].to('cpu')\n",
    "\n",
    "# Instantiate the model\n",
    "model_pt = MLP100(Hparam['input_size'], Hparam['num_classes'])\n",
    "model_pt.load_state_dict(Model_state_dict)\n",
    "print(model_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4edd238",
   "metadata": {},
   "source": [
    "## Load Saved Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c82c070b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_dict: {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
      "dataset_schema: (label, label_index, predicted_index, feature_length, feature_vector)\n",
      "item-> label: 7, label_index: 7, predicted_index: 7, feature_length: 784, feature_vector size: 784\n"
     ]
    }
   ],
   "source": [
    "# Prints a dataset item\n",
    "def print_dataitem(item):\n",
    "    mstr = f\"label: {item[0]}, label_index: {item[1]}, predicted_index: {item[2]}, feature_length: {item[3]},\"\n",
    "    mstr2 = f\"feature_vector size: {len(item[4])}\"\n",
    "    print(mstr, mstr2)\n",
    "\n",
    "    \n",
    "# Load the test dataset\n",
    "ds_path = './saved/test_dataset.pt'\n",
    "DS_loaded = torch.load(ds_path)\n",
    "for key in DS_loaded:\n",
    "    if key != 'dataset':\n",
    "        print(f'{key}:', DS_loaded[key])\n",
    "\n",
    "# Show an item summary\n",
    "item = DS_loaded['dataset'][0]\n",
    "print('item-> ', end='')\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283dcdbb",
   "metadata": {},
   "source": [
    "## Validate The Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc648f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7\n",
      "label: 7, label_index: 7, predicted_index: 7, feature_length: 784, feature_vector size: 784\n"
     ]
    }
   ],
   "source": [
    "# find most likely label index for each element\n",
    "def get_likely_index(tensor):\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# Given an item form the test_dataset, returns an example for predict() function\n",
    "# numpytype: set it to True to return numpy nd-array\n",
    "def make_example(data_item, numpytype=False):\n",
    "    feature = torch.tensor(data_item[4])\n",
    "    if numpytype: feature = feature.detach().numpy()\n",
    "    return feature\n",
    "\n",
    "\n",
    "# test prediction from dataset item.\n",
    "# ptmodel: set it to True for the PyTorch model\n",
    "def predict(example, model=None, ptmodel=False):  # example: feature_vector\n",
    "    if ptmodel: model.eval()    # set the pytorch model to evaluation mode\n",
    "    # Use the model to predict the label of the image\n",
    "    feature = example\n",
    "    if ptmodel: feature = feature.unsqueeze(0)    # add the batch dimension for the pytorch model\n",
    "    output = model(feature)\n",
    "    pred = get_likely_index(output)\n",
    "    if ptmodel: pred = pred[0]    # removing batch index\n",
    "    return pred.item()\n",
    "\n",
    "\n",
    "# Test predict()\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item)\n",
    "pred = predict(example, model=model_pt, ptmodel=True)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb5eb7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7a2cca71954154af0a544d3a3ba8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 98.11%   correct_count: 9811   expected-miss: 0   total_count: 10000\n",
      "Expected Model Performance:   accuracy: 98.11%   correct_count: 9811\n"
     ]
    }
   ],
   "source": [
    "# Validate the Given model on the whole dataset\n",
    "# ptmodel: set it to True for the PyTorch model\n",
    "def validateMode(model=None, ptmodel=False, numpytype=False):\n",
    "    dataset = DS_loaded['dataset']\n",
    "    expect_miss = 0      # keeps track of no. of mismatche between prediction in dataset vs model prediction\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for item in tqdm(dataset):\n",
    "        lbl, lbl_index, pred_index, *_ = item\n",
    "        example = make_example(item, numpytype=numpytype)\n",
    "        pred = predict(example, model=model, ptmodel=ptmodel)\n",
    "        if pred != pred_index: expect_miss += 1    # prediction does not match prediction in dataset\n",
    "        if pred == lbl_index: correct_count += 1   # prediction matched the actual label-index\n",
    "        total_count += 1\n",
    "    # Compute and print statistics\n",
    "    accuracy = (100.0 * correct_count) / total_count\n",
    "    print(f'Validation accuracy: {accuracy:.2f}%   correct_count: {correct_count}   expected-miss: {expect_miss}   total_count: {total_count}')\n",
    "    return accuracy, correct_count, expect_miss, total_count\n",
    "\n",
    "            \n",
    "# Validate the loaded model\n",
    "validateMode(model_pt, ptmodel=True)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6674852",
   "metadata": {},
   "source": [
    "# Implementation Using torch.tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f876161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight: torch.Size([100, 784])\n",
      "fc1.bias  : torch.Size([100])\n",
      "fc2.weight: torch.Size([100, 100])\n",
      "fc2.bias  : torch.Size([100])\n",
      "fc3.weight: torch.Size([10, 100])\n",
      "fc3.bias  : torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Extract the weights as torch.tensors\n",
    "for key in Model_state_dict:\n",
    "    print(f'{key:10}:', Model_state_dict[key].size())\n",
    "\n",
    "fc1_weight_pt = Model_state_dict['fc1.weight']\n",
    "fc1_bias_pt = Model_state_dict['fc1.bias']\n",
    "fc2_weight_pt = Model_state_dict['fc2.weight']\n",
    "fc2_bias_pt = Model_state_dict['fc2.bias']\n",
    "fc3_weight_pt = Model_state_dict['fc3.weight']\n",
    "fc3_bias_pt = Model_state_dict['fc3.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355d25ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7\n",
      "label: 7, label_index: 7, predicted_index: 7, feature_length: 784, feature_vector size: 784\n"
     ]
    }
   ],
   "source": [
    "# Define the model using pytorch tensor operations.\n",
    "# Input interface is the same as the \n",
    "def tensorModel(features):\n",
    "    x1 = fc1_weight_pt @ features + fc1_bias_pt\n",
    "    fc1_out = F.relu(x1)\n",
    "    x2 = fc2_weight_pt @ fc1_out + fc2_bias_pt\n",
    "    fc2_out = F.relu(x2)\n",
    "    fc3_out = fc3_weight_pt @ fc2_out + fc3_bias_pt\n",
    "    return fc3_out\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item)\n",
    "pred = predict(example, model=tensorModel, ptmodel=False)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0ed12fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d9db372182450b9266a0e354e37832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 98.11%   correct_count: 9811   expected-miss: 0   total_count: 10000\n",
      "Expected Model Performance:   accuracy: 98.11%   correct_count: 9811\n"
     ]
    }
   ],
   "source": [
    "# Validate the tensor operation based model\n",
    "validateMode(tensorModel, ptmodel=False)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c073cb",
   "metadata": {},
   "source": [
    "# Implement Using Numpy Matrixing Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20f5d482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1_weight_np: (100, 784)\n"
     ]
    }
   ],
   "source": [
    "# Copy weights as numpy ndarray\n",
    "fc1_weight_np = fc1_weight_pt.detach().numpy()\n",
    "fc1_bias_np   = fc1_bias_pt.detach().numpy()\n",
    "fc2_weight_np = fc2_weight_pt.detach().numpy()\n",
    "fc2_bias_np   = fc2_bias_pt.detach().numpy()\n",
    "fc3_weight_np = fc3_weight_pt.detach().numpy()\n",
    "fc3_bias_np   = fc3_bias_pt.detach().numpy()\n",
    "\n",
    "print('fc1_weight_np:', fc1_weight_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fbd3a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7\n",
      "label: 7, label_index: 7, predicted_index: 7, feature_length: 784, feature_vector size: 784\n"
     ]
    }
   ],
   "source": [
    "# Relu on numpy array\n",
    "def npReLU(np_arr):\n",
    "    return max(0, np_arr)\n",
    "\n",
    "\n",
    "# Define the model using numpy matrix operations\n",
    "# Input interface is the same as the \n",
    "def numpyModel(features):\n",
    "    x1 = fc1_weight_np @ features + fc1_bias_np\n",
    "    fc1_out = npReLU(x1)\n",
    "    x2 = fc2_weight_np @ fc1_out + fc2_bias_np\n",
    "    fc2_out = npReLU(x2)\n",
    "    fc3_out = fc3_weight_np @ fc2_out + fc3_bias_np\n",
    "    return fc3_out\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item, numpytype=True)\n",
    "pred = predict(example, model=tensorModel, ptmodel=False)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e1c4bb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bc78a1bbac4550b2af2fef48e659f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 98.11%   correct_count: 9811   expected-miss: 0   total_count: 10000\n",
      "Expected Model Performance:   accuracy: 98.11%   correct_count: 9811\n"
     ]
    }
   ],
   "source": [
    "# Validate the tensor operation based model\n",
    "validateMode(tensorModel, ptmodel=False, numpytype=True)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70a276",
   "metadata": {},
   "source": [
    "# Export Numpy Model & Dataset as sqlite3 DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a79ea",
   "metadata": {},
   "source": [
    "# Import Saved Model and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ffe3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
