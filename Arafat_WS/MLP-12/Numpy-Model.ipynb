{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13d09f12",
   "metadata": {},
   "source": [
    "# MLP-12 Numpy Model Extraction\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "- Load Dataset and Model, then verify.\n",
    "- Extract the weights.\n",
    "- Describe the model using tensor operations, then validate.\n",
    "- Describe the model using numpy matrix operations, then validate.\n",
    "- Export numpy model as sqlite3 database for implementation in C.\n",
    "- Export test dataset as sqlite3 database for implementation in C.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset exported by the training notebook may have incorrect predicted index due to several iterations of model training and not updating the dataset. We'll re-run the predictions here and update the predicted index in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74528974",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc475987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# We don't need GPU for this, not training\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device('cuda')\n",
    "#else:\n",
    "#    device = torch.device('cpu')\n",
    "\n",
    "device = 'cpu'\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb4d795",
   "metadata": {},
   "source": [
    "# Load and Validate torch.nn.Module Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ca5cb",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "**NOTE:** Always copy the following cell from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0d2b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP with single hiddend layer with 12 units and ReLU activation.\n",
    "class MLP12(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP12, self).__init__()\n",
    "        # Save parameters\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.debug = False    # can be used to activate debugging features\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, 12)   # 12 hidden units\n",
    "        self.fc1_drop = nn.Dropout(0.2)        # drop-out for faster training, has no effect on inference\n",
    "        self.fc2 = nn.Linear(12, num_classes)  # output layer\n",
    "\n",
    "    # Expects a batch of 1-D tensor\n",
    "    # Dimension of x: (batch-size, input_size)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   # pass through the hidden layer\n",
    "        x = self.fc1_drop(x)      \n",
    "        x = self.fc2(x)           # pass through the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8a0083",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltr ./saved/\n",
    "print('')\n",
    "\n",
    "# Loaded saved model dictionary\n",
    "model_path = './saved/trained_mlp12-94.0p.pt'\n",
    "model_dict = torch.load(model_path)\n",
    "print(model_dict.keys())\n",
    "for k,v in model_dict.items():\n",
    "    if k!='state_dict': print(k,':',v)\n",
    "        \n",
    "        \n",
    "# Parse the values for easier use\n",
    "Accuracy = model_dict['accuracy']\n",
    "Correct_count = model_dict['correct_count']\n",
    "Hparam = model_dict['Hparam']\n",
    "Model_state_dict = model_dict['state_dict']\n",
    "Model_perf = f'Model Performance:   accuracy: {Accuracy:.2f}%   correct_count: {Correct_count}'  # to be used later\n",
    "print('Hparam:', Hparam)\n",
    "print('Model_perf:', Model_perf)\n",
    "\n",
    "\n",
    "# move all weights to cpu\n",
    "for key in Model_state_dict: \n",
    "    Model_state_dict[key] = Model_state_dict[key].to('cpu')\n",
    "    \n",
    "\n",
    "# Instantiate the model\n",
    "model_pt = MLP12(Hparam['input_size'], Hparam['num_classes'])\n",
    "model_pt.load_state_dict(Model_state_dict)\n",
    "model_pt.to('cpu')\n",
    "model_pt.eval()     # we are always evaluating here\n",
    "print(model_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_path, model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d50214",
   "metadata": {},
   "source": [
    "## Load Saved Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa81cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints a dataset item\n",
    "def print_dataitem(item):\n",
    "    mstr = f\"label: {item[0]}, label_index: {item[1]}, predicted_index: {item[2]}, feature_length: {item[3]},\"\n",
    "    mstr2 = f\"feature_vector size: {len(item[4])}\"\n",
    "    print(mstr, mstr2)\n",
    "\n",
    "    \n",
    "# Load the test dataset\n",
    "ds_path = './saved/test_dataset.pt'\n",
    "DS_loaded = torch.load(ds_path)\n",
    "for key in DS_loaded:\n",
    "    if key != 'dataset':\n",
    "        print(f'{key}:', DS_loaded[key])\n",
    "\n",
    "        \n",
    "# Show an item summary\n",
    "item = DS_loaded['dataset'][0]\n",
    "print('item-> ', end='')\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1075c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds_path, item, key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6ead95",
   "metadata": {},
   "source": [
    "## Validate The Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6adc89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most likely label index for each element\n",
    "def get_likely_index(tensor):\n",
    "    # convert to tensor from numpy if needed\n",
    "    if not torch.is_tensor(tensor):\n",
    "        tensor = torch.from_numpy(tensor)\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# Given an item form the test_dataset, returns an example for predict() function\n",
    "# numpytype: set it to True to return numpy nd-array\n",
    "def make_example(data_item, numpytype=False):\n",
    "    feature = torch.tensor(data_item[4])\n",
    "    if numpytype: feature = feature.detach().numpy()\n",
    "    return feature\n",
    "\n",
    "\n",
    "# test prediction from dataset item.\n",
    "# ptmodel: set it to True for the PyTorch model\n",
    "def predict(example, model=None, ptmodel=False):  # example: feature_vector\n",
    "    if ptmodel: model.eval()    # set the pytorch model to evaluation mode\n",
    "    # Use the model to predict the label of the image\n",
    "    feature = example\n",
    "    if ptmodel: feature = feature.unsqueeze(0)    # add the batch dimension for the pytorch model\n",
    "    output = model(feature)\n",
    "    pred = get_likely_index(output)\n",
    "    if ptmodel: pred = pred[0]    # removing batch index\n",
    "    return pred.item()\n",
    "\n",
    "\n",
    "# Test predict()\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item)\n",
    "pred = predict(example, model=model_pt, ptmodel=True)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)\n",
    "\n",
    "# Delete names\n",
    "del item, example, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07777766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Given model on the whole dataset\n",
    "# ptmodel: set it to True for the PyTorch model\n",
    "def validateModel(model=None, ptmodel=False, numpytype=False):\n",
    "    dataset = DS_loaded['dataset']\n",
    "    expect_miss = 0      # keeps track of no. of mismatche between prediction in dataset vs model prediction\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for item in tqdm(dataset):\n",
    "        lbl, lbl_index, pred_index, *_ = item\n",
    "        example = make_example(item, numpytype=numpytype)\n",
    "        pred = predict(example, model=model, ptmodel=ptmodel)\n",
    "        if pred != pred_index: expect_miss += 1    # prediction does not match prediction in dataset\n",
    "        if pred == lbl_index: correct_count += 1   # prediction matched the actual label-index\n",
    "        total_count += 1\n",
    "    # Compute and print statistics\n",
    "    accuracy = (100.0 * correct_count) / total_count\n",
    "    print(f'Validation accuracy: {accuracy:.2f}%   correct_count: {correct_count}   expected-miss: {expect_miss}   total_count: {total_count}')\n",
    "    return accuracy, correct_count, expect_miss, total_count\n",
    "\n",
    "            \n",
    "# Validate the loaded model\n",
    "validateModel(model_pt, ptmodel=True)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd700bb",
   "metadata": {},
   "source": [
    "# Implementation Using torch.tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba0d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights as torch.tensors\n",
    "for key in Model_state_dict:\n",
    "    print(f'{key:10}:', Model_state_dict[key].size())\n",
    "\n",
    "fc1_weight_pt = Model_state_dict['fc1.weight']\n",
    "fc1_bias_pt = Model_state_dict['fc1.bias']\n",
    "fc2_weight_pt = Model_state_dict['fc2.weight']\n",
    "fc2_bias_pt = Model_state_dict['fc2.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using pytorch tensor operations.\n",
    "# Input interface is the same as the \n",
    "def tensorModel(features):\n",
    "    x1 = fc1_weight_pt @ features + fc1_bias_pt\n",
    "    fc1_out = F.relu(x1)\n",
    "    fc2_out = fc2_weight_pt @ fc1_out + fc2_bias_pt\n",
    "    return fc2_out\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item)\n",
    "pred = predict(example, model=tensorModel, ptmodel=False)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)\n",
    "\n",
    "# Delete names\n",
    "del item, example, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the tensor operation based model\n",
    "validateModel(tensorModel, ptmodel=False)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33696d9f",
   "metadata": {},
   "source": [
    "# Implement Using Numpy Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac1eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy weights as numpy ndarray\n",
    "fc1_weight_np = fc1_weight_pt.detach().numpy()\n",
    "fc1_bias_np   = fc1_bias_pt.detach().numpy()\n",
    "fc2_weight_np = fc2_weight_pt.detach().numpy()\n",
    "fc2_bias_np   = fc2_bias_pt.detach().numpy()\n",
    "\n",
    "print('fc1_weight_np:', fc1_weight_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514f5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu on numpy array\n",
    "def npReLU(np_arr):\n",
    "    return np.maximum(0, np_arr)\n",
    "\n",
    "\n",
    "# Define the model using numpy matrix operations\n",
    "# Input interface is the same as the \n",
    "def numpyModel(features):\n",
    "    x1 = fc1_weight_np @ features + fc1_bias_np\n",
    "    fc1_out = npReLU(x1)\n",
    "    fc2_out = fc2_weight_np @ fc1_out + fc2_bias_np\n",
    "    return fc2_out\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item, numpytype=True)\n",
    "pred = predict(example, model=numpyModel, ptmodel=False)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)\n",
    "\n",
    "# Delete names\n",
    "del item, example, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the tensor operation based model\n",
    "validateModel(numpyModel, ptmodel=False, numpytype=True)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e465b97a",
   "metadata": {},
   "source": [
    "# Update the dataset with the Numpy Model Predicted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec400b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_iter = tqdm( enumerate(DS_loaded['dataset']), total=len(DS_loaded['dataset']) )\n",
    "fix_count = 0\n",
    "for index, item in enum_iter:\n",
    "    # Make prediction using Numpy model\n",
    "    example = make_example(item, numpytype=True)\n",
    "    pred = predict(example, model=numpyModel, ptmodel=False)\n",
    "    # Check and fix the predicted_index in the dataset\n",
    "    if pred!=item[2]:\n",
    "        DS_loaded['dataset'][index][2] = pred\n",
    "        fix_count += 1\n",
    "\n",
    "print(f'INFO: Fixed {fix_count} predicted_index in the dataset')\n",
    "\n",
    "# Delete names\n",
    "del enum_iter, fix_count, index, item, example, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68db203",
   "metadata": {},
   "source": [
    "# Export Numpy Model as sqlite3 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe4cd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the cache and import sqlite3 utilities\n",
    "!rm -rf __pycache__/\n",
    "from utilsqlite3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b030e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database file\n",
    "DB_path = './saved/trained-mlp12.s3db'\n",
    "createDB(DB_path, overwrite=True)\n",
    "!ls -ltrh ./saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd401b1c",
   "metadata": {},
   "source": [
    "## Write the header table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8803fc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the header table\n",
    "def createHeaderTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = '''CREATE TABLE IF NOT EXISTS Header (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        key TEXT,\n",
    "                        value NUMERIC,\n",
    "                        description TEXT\n",
    "                    )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createHeaderTable(DB_path)\n",
    "table_names = getTableNames(DB_path)\n",
    "print(table_names)\n",
    "del table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts a record into the Header table (will be called by insertRecordList() utility function)\n",
    "def insertHeaderRecord(cursor, record):\n",
    "    key, value, description = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    cursor.execute('''INSERT INTO Header (key, value, description)\n",
    "                      VALUES (?, ?, ?)''', (key, value, description))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB_path, insertHeaderRecord, [('example_key', 'example_value', 'example_description')])\n",
    "getRecords(DB_path, 'Header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the records as a list\n",
    "Fc1w_table = 'FC1_Weight_T'\n",
    "Fc1b_table = 'FC1_Bias_T'\n",
    "Fc2w_table = 'FC2_Weight_T'\n",
    "Fc2b_table = 'FC2_Bias_T'\n",
    "Hparam_table = 'Hparam_T'\n",
    "\n",
    "header_records = [\n",
    "    ('name', 'MLP-12', ''),\n",
    "    ('architecture', '784-FC:12-10', 'It is an MLP with 1 hidden layer with 12 units with ReLU activation. Trained on MNIST dataset (output layer with 10 units).'),\n",
    "    ('accuracy', Accuracy, 'Accuracy% of the trained model on the test dataset.'),\n",
    "    ('correct_count', Correct_count, 'Number of correct predictions by the trained model on the test dataset.'),\n",
    "    \n",
    "    ('Hparam.table',   Hparam_table, 'This is the name of the table that contains different parameters of the model.'),\n",
    "    ('fc1.weight.table', Fc1w_table, 'Name of the table containing the fc1.weight matrix'),\n",
    "    ('fc1.bias.table',   Fc1b_table, 'Name of the table containing the fc1.bias vector'),\n",
    "    ('fc2.weight.table', Fc2w_table, 'Name of the table containing the fc2.weight matrix'),\n",
    "    ('fc2.bias.table',   Fc2b_table, 'Name of the table containing the fc2.bias vector'),\n",
    "]\n",
    "\n",
    "# Insert the header records\n",
    "deleteRows(DB_path, 'Header')  # delete previous records\n",
    "insertRecordList(DB_path, insertHeaderRecord, header_records)\n",
    "records = getRecords(DB_path, 'Header')\n",
    "for r in records: print(r[:-1])   # print all but description field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bf42c",
   "metadata": {},
   "source": [
    "# Export the Dataset as sqlite3 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0c004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e8e9c0",
   "metadata": {},
   "source": [
    "# Concluding Remarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0fa41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
