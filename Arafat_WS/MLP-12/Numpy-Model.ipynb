{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1439af4f",
   "metadata": {},
   "source": [
    "# MLP-12 Numpy Model Extraction\n",
    "\n",
    "**Goals:**\n",
    "\n",
    "- Load Dataset and Model, then verify.\n",
    "- Extract the weights.\n",
    "- Describe the model using tensor operations, then validate.\n",
    "- Describe the model using numpy matrix operations, then validate.\n",
    "- Export numpy model as sqlite3 database for implementation in C.\n",
    "- Export test dataset as sqlite3 database for implementation in C.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset exported by the training notebook may have incorrect predicted index due to several iterations of model training and not updating the dataset. We'll re-run the predictions here and update the predicted index in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff3d5f",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b39d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# We don't need GPU for this, not training\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device('cuda')\n",
    "#else:\n",
    "#    device = torch.device('cpu')\n",
    "\n",
    "device = 'cpu'\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00d783",
   "metadata": {},
   "source": [
    "# Load and Validate torch.nn.Module Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03440cc0",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "\n",
    "**NOTE:** Always copy the following cell from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f348a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP with single hiddend layer with 12 units and ReLU activation.\n",
    "class MLP12(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(MLP12, self).__init__()\n",
    "        # Save parameters\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.debug = False    # can be used to activate debugging features\n",
    "        # Define layers\n",
    "        self.fc1 = nn.Linear(input_size, 12)   # 12 hidden units\n",
    "        self.fc1_drop = nn.Dropout(0.2)        # drop-out for faster training, has no effect on inference\n",
    "        self.fc2 = nn.Linear(12, num_classes)  # output layer\n",
    "\n",
    "    # Expects a batch of 1-D tensor\n",
    "    # Dimension of x: (batch-size, input_size)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   # pass through the hidden layer\n",
    "        x = self.fc1_drop(x)      \n",
    "        x = self.fc2(x)           # pass through the output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc3579",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084e0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -ltr ./saved/\n",
    "print('')\n",
    "\n",
    "# Loaded saved model dictionary\n",
    "model_path = './saved/trained_mlp12-94.0p.pt'\n",
    "model_dict = torch.load(model_path)\n",
    "print(model_dict.keys())\n",
    "for k,v in model_dict.items():\n",
    "    if k!='state_dict': print(k,':',v)\n",
    "        \n",
    "        \n",
    "# Parse the values for easier use\n",
    "Accuracy = model_dict['accuracy']\n",
    "Correct_count = model_dict['correct_count']\n",
    "Hparam = model_dict['Hparam']\n",
    "Model_state_dict = model_dict['state_dict']\n",
    "Model_perf = f'Model Performance:   accuracy: {Accuracy:.2f}%   correct_count: {Correct_count}'  # to be used later\n",
    "print('Hparam:', Hparam)\n",
    "print('Model_perf:', Model_perf)\n",
    "\n",
    "\n",
    "# move all weights to cpu\n",
    "for key in Model_state_dict: \n",
    "    Model_state_dict[key] = Model_state_dict[key].to('cpu')\n",
    "    \n",
    "\n",
    "# Instantiate the model\n",
    "model_pt = MLP12(Hparam['input_size'], Hparam['num_classes'])\n",
    "model_pt.load_state_dict(Model_state_dict)\n",
    "model_pt.to('cpu')\n",
    "model_pt.eval()     # we are always evaluating here\n",
    "print(model_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_path, model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c777c15c",
   "metadata": {},
   "source": [
    "## Load Saved Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413bb2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints a dataset item\n",
    "def print_dataitem(item):\n",
    "    mstr = f\"label: {item[0]}, label_index: {item[1]}, predicted_index: {item[2]}, feature_length: {item[3]},\"\n",
    "    mstr2 = f\"feature_vector size: {len(item[4])}\"\n",
    "    print(mstr, mstr2)\n",
    "\n",
    "    \n",
    "# Load the test dataset\n",
    "ds_path = './saved/test_dataset.pt'\n",
    "DS_loaded = torch.load(ds_path)\n",
    "for key in DS_loaded:\n",
    "    if key != 'dataset':\n",
    "        print(f'{key}:', DS_loaded[key])\n",
    "\n",
    "        \n",
    "# Show an item summary\n",
    "item = DS_loaded['dataset'][0]\n",
    "print('item-> ', end='')\n",
    "print_dataitem(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7593195",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ds_path, item, key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7659cc52",
   "metadata": {},
   "source": [
    "## Validate The Loaded Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf1ef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most likely label index for each element\n",
    "def get_likely_index(tensor):\n",
    "    # convert to tensor from numpy if needed\n",
    "    if not torch.is_tensor(tensor):\n",
    "        tensor = torch.from_numpy(tensor)\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "# Given an item form the test_dataset, returns an example for predict() function\n",
    "# numpytype: set it to True to return numpy nd-array\n",
    "def make_example(data_item, numpytype=False):\n",
    "    feature = torch.tensor(data_item[4])\n",
    "    if numpytype: feature = feature.detach().numpy()\n",
    "    return feature\n",
    "\n",
    "\n",
    "# test prediction from dataset item.\n",
    "# ptmodel: set it to True for the PyTorch model\n",
    "def predict(example, model=None, ptmodel=False):  # example: feature_vector\n",
    "    if ptmodel: model.eval()    # set the pytorch model to evaluation mode\n",
    "    # Use the model to predict the label of the image\n",
    "    feature = example\n",
    "    if ptmodel: feature = feature.unsqueeze(0)    # add the batch dimension for the pytorch model\n",
    "    output = model(feature)\n",
    "    pred = get_likely_index(output)\n",
    "    if ptmodel: pred = pred[0]    # removing batch index\n",
    "    return pred.item()\n",
    "\n",
    "\n",
    "# Test predict()\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item)\n",
    "pred = predict(example, model=model_pt, ptmodel=True)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)\n",
    "\n",
    "# Delete names\n",
    "del item, example, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fae90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the Given model on the whole dataset\n",
    "# ptmodel: set it to True for the PyTorch model\n",
    "def validateModel(model=None, ptmodel=False, numpytype=False):\n",
    "    dataset = DS_loaded['dataset']\n",
    "    expect_miss = 0      # keeps track of no. of mismatche between prediction in dataset vs model prediction\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for item in tqdm(dataset):\n",
    "        lbl, lbl_index, pred_index, *_ = item\n",
    "        example = make_example(item, numpytype=numpytype)\n",
    "        pred = predict(example, model=model, ptmodel=ptmodel)\n",
    "        if pred != pred_index: expect_miss += 1    # prediction does not match prediction in dataset\n",
    "        if pred == lbl_index: correct_count += 1   # prediction matched the actual label-index\n",
    "        total_count += 1\n",
    "    # Compute and print statistics\n",
    "    accuracy = (100.0 * correct_count) / total_count\n",
    "    print(f'Validation accuracy: {accuracy:.2f}%   correct_count: {correct_count}   expected-miss: {expect_miss}   total_count: {total_count}')\n",
    "    return accuracy, correct_count, expect_miss, total_count\n",
    "\n",
    "            \n",
    "# Validate the loaded model\n",
    "validateModel(model_pt, ptmodel=True)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbee89f",
   "metadata": {},
   "source": [
    "# Implementation Using torch.tensor Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fa66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the weights as torch.tensors\n",
    "for key in Model_state_dict:\n",
    "    print(f'{key:10}:', Model_state_dict[key].size())\n",
    "\n",
    "fc1_weight_pt = Model_state_dict['fc1.weight']\n",
    "fc1_bias_pt = Model_state_dict['fc1.bias']\n",
    "fc2_weight_pt = Model_state_dict['fc2.weight']\n",
    "fc2_bias_pt = Model_state_dict['fc2.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a68351a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using pytorch tensor operations.\n",
    "# Input interface is the same as the \n",
    "def tensorModel(features):\n",
    "    x1 = fc1_weight_pt @ features + fc1_bias_pt\n",
    "    fc1_out = F.relu(x1)\n",
    "    fc2_out = fc2_weight_pt @ fc1_out + fc2_bias_pt\n",
    "    return fc2_out\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item)\n",
    "pred = predict(example, model=tensorModel, ptmodel=False)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)\n",
    "\n",
    "# Delete names\n",
    "del item, example, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d258fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the tensor operation based model\n",
    "validateModel(tensorModel, ptmodel=False)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0744bc",
   "metadata": {},
   "source": [
    "# Implement Using Numpy Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy weights as numpy ndarray\n",
    "fc1_weight_np = fc1_weight_pt.detach().numpy()\n",
    "fc1_bias_np   = fc1_bias_pt.detach().numpy()\n",
    "fc2_weight_np = fc2_weight_pt.detach().numpy()\n",
    "fc2_bias_np   = fc2_bias_pt.detach().numpy()\n",
    "\n",
    "print('fc1_weight_np:', fc1_weight_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ef73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu on numpy array\n",
    "def npReLU(np_arr):\n",
    "    return np.maximum(0, np_arr)\n",
    "\n",
    "\n",
    "# Define the model using numpy matrix operations\n",
    "# Input interface is the same as the \n",
    "def numpyModel(features):\n",
    "    x1 = fc1_weight_np @ features + fc1_bias_np\n",
    "    fc1_out = npReLU(x1)\n",
    "    fc2_out = fc2_weight_np @ fc1_out + fc2_bias_np\n",
    "    return fc2_out\n",
    "\n",
    "\n",
    "# Test this model\n",
    "item = DS_loaded['dataset'][0]\n",
    "example = make_example(item, numpytype=True)\n",
    "pred = predict(example, model=numpyModel, ptmodel=False)\n",
    "print('pred:',pred)\n",
    "print_dataitem(item)\n",
    "\n",
    "# Delete names\n",
    "del item, example, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24d6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the tensor operation based model\n",
    "validateModel(numpyModel, ptmodel=False, numpytype=True)\n",
    "print('Expected', Model_perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd4262",
   "metadata": {},
   "source": [
    "# Update the dataset with the Numpy Model Predicted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "enum_iter = tqdm( enumerate(DS_loaded['dataset']), total=len(DS_loaded['dataset']) )\n",
    "fix_count = 0\n",
    "for index, item in enum_iter:\n",
    "    # Make prediction using Numpy model\n",
    "    example = make_example(item, numpytype=True)\n",
    "    pred = predict(example, model=numpyModel, ptmodel=False)\n",
    "    # Check and fix the predicted_index in the dataset\n",
    "    if pred!=item[2]:\n",
    "        DS_loaded['dataset'][index][2] = pred\n",
    "        fix_count += 1\n",
    "\n",
    "print(f'INFO: Fixed {fix_count} predicted_index in the dataset')\n",
    "\n",
    "# Delete names\n",
    "del enum_iter, fix_count, index, item, example, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf154f6",
   "metadata": {},
   "source": [
    "# Export Numpy Model as sqlite3 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c5cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the cache and import sqlite3 utilities\n",
    "!rm -rf __pycache__/\n",
    "from utilsqlite3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database file\n",
    "DB_path = './saved/trained-mlp12.s3db'\n",
    "createDB(DB_path, overwrite=True)\n",
    "!ls -ltrh ./saved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc6714",
   "metadata": {},
   "source": [
    "## Write the header table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the header table\n",
    "def createHeaderTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = '''CREATE TABLE IF NOT EXISTS Header (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        key TEXT,\n",
    "                        value NUMERIC,\n",
    "                        description TEXT\n",
    "                    )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createHeaderTable(DB_path)\n",
    "table_names = getTableNames(DB_path)\n",
    "print(table_names)\n",
    "del table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8fd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts a record into the Header table (will be called by insertRecordList() utility function)\n",
    "def insertHeaderRecord(cursor, record):\n",
    "    key, value, description = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    cursor.execute('''INSERT INTO Header (key, value, description)\n",
    "                      VALUES (?, ?, ?)''', (key, value, description))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB_path, insertHeaderRecord, [('example_key', 'example_value', 'example_description')])\n",
    "getRecords(DB_path, 'Header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd69144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the records as a list\n",
    "Fc1w_table = 'FC1_Weight_T'\n",
    "Fc1b_table = 'FC1_Bias_T'\n",
    "Fc2w_table = 'FC2_Weight_T'\n",
    "Fc2b_table = 'FC2_Bias_T'\n",
    "Hparam_table = 'Hparam_T'\n",
    "\n",
    "header_records = [\n",
    "    ('name', 'MLP-12', ''),\n",
    "    ('architecture', '784-FC:12-10', 'It is an MLP with 1 hidden layer with 12 units with ReLU activation. Trained on MNIST dataset (output layer with 10 units).'),\n",
    "    ('accuracy', Accuracy, 'Accuracy% of the trained model on the test dataset.'),\n",
    "    ('correct_count', Correct_count, 'Number of correct predictions by the trained model on the test dataset.'),\n",
    "    \n",
    "    ('Hparam.table',   Hparam_table, 'This is the name of the table that contains different parameters of the model.'),\n",
    "    ('fc1.weight.table', Fc1w_table, 'Name of the table containing the fc1.weight matrix'),\n",
    "    ('fc1.bias.table',   Fc1b_table, 'Name of the table containing the fc1.bias vector'),\n",
    "    ('fc2.weight.table', Fc2w_table, 'Name of the table containing the fc2.weight matrix'),\n",
    "    ('fc2.bias.table',   Fc2b_table, 'Name of the table containing the fc2.bias vector'),\n",
    "]\n",
    "\n",
    "# Insert the header records\n",
    "deleteRows(DB_path, 'Header')  # delete previous records\n",
    "insertRecordList(DB_path, insertHeaderRecord, header_records)\n",
    "records = getRecords(DB_path, 'Header')\n",
    "for r in records: print(r[:-1])   # print all but description field\n",
    "    \n",
    "# Delete names\n",
    "del records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e580c",
   "metadata": {},
   "source": [
    "## Write the Hparam Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the Hparam table\n",
    "def createHparamTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = f'''CREATE TABLE IF NOT EXISTS {Hparam_table} (\n",
    "                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                        key TEXT,\n",
    "                        value NUMERIC,\n",
    "                        description TEXT\n",
    "                    )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createHparamTable(DB_path)\n",
    "table_names = getTableNames(DB_path)\n",
    "print(table_names)\n",
    "\n",
    "# Delete neames\n",
    "del table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts a record into the Hparam table\n",
    "def insertHparamRecord(cursor, record):\n",
    "    key, value, description = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    cursor.execute(f'''INSERT INTO {Hparam_table} (key, value, description)\n",
    "                      VALUES (?, ?, ?)''', (key, value, description))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB_path, insertHparamRecord, [('example_key', 123, 'example_description')])\n",
    "getRecords(DB_path, Hparam_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9d360d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the Hparam records\n",
    "print('Hparam from pytorch:', Hparam)\n",
    "\n",
    "hparam_records = [\n",
    "    ('input_size', Hparam['input_size'], 'Input size of the MLP'),\n",
    "    ('num_classes', Hparam['num_classes'], 'Output size of the MLP'),\n",
    "    \n",
    "    ('fc1.weight.row', len(fc1_weight_np), 'No. of rows in fc1.weight'),\n",
    "    ('fc1.weight.col', len(fc1_weight_np[0]), 'No. of columns in fc1.weight'),\n",
    "    ('fc1.bias.len', len(fc1_bias_np), 'Lengths of the fc1.bias vector'),\n",
    "    \n",
    "    ('fc2.weight.row', len(fc2_weight_np), 'No. of rows in fc2.weight'),\n",
    "    ('fc2.weight.col', len(fc2_weight_np[0]), 'No. of columns in fc2.weight'),\n",
    "    ('fc2.bias.len', len(fc2_bias_np), 'Lengths of the fc2.bias vector'),\n",
    "]\n",
    "\n",
    "\n",
    "deleteRows(DB_path, Hparam_table)\n",
    "insertRecordList(DB_path, insertHparamRecord, hparam_records)\n",
    "getRecords(DB_path, Hparam_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d2e685",
   "metadata": {},
   "source": [
    "## Write Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves a numpy 2D array as a table in the database.\n",
    "# Columns: ID, row_no, col_0, col_1, ..., col_n\n",
    "def createMatrixTable(db_path, table_name, nparray, overwrite=False):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop the table if it exists and overwrite requested\n",
    "    table_exist = existTable(db_path, table_name)\n",
    "    if overwrite and table_exist:\n",
    "        print(f'WARN: Overwriting table {table_name}')\n",
    "        dropTable(db_path, table_name)\n",
    "        \n",
    "    # Create the table\n",
    "    rows, cols = nparray.shape\n",
    "    column_names = \"row_no, \" + \", \".join([f\"col_{i}\" for i in range(cols)])  # Generate the column names string\n",
    "    cursor.execute(f\"CREATE TABLE {table_name} ({column_names})\")\n",
    "\n",
    "    # Insert the array rows into the table\n",
    "    for i in range(rows):\n",
    "        vals = f'{i}, ' + ', '.join(map(str, nparray[i]))\n",
    "        cursor.execute(f\"INSERT INTO {table_name} VALUES ({vals})\")\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# test createMatrixTable()\n",
    "createMatrixTable(DB_path, 'test', fc1_weight_np, overwrite=True)\n",
    "col_names = getColNames(DB_path, 'test')\n",
    "records = getRecords(DB_path, 'test')\n",
    "print('col_names:', col_names[:5], '...', col_names[-5:])\n",
    "print('records[i]:', records[2][:5], '...')\n",
    "\n",
    "# Delete names\n",
    "del col_names, records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b18e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "createMatrixTable(DB_path, Fc1w_table, fc1_weight_np, overwrite=True)\n",
    "createMatrixTable(DB_path, Fc2w_table, fc2_weight_np, overwrite=True)\n",
    "\n",
    "# Convert vectors into 2D array for the table\n",
    "fc1b = np.expand_dims(fc1_bias_np, axis=0)\n",
    "fc2b = np.expand_dims(fc2_bias_np, axis=0)\n",
    "print('fc1b shape:', fc1b.shape)\n",
    "\n",
    "createMatrixTable(DB_path, Fc1b_table, fc1b, overwrite=True)\n",
    "createMatrixTable(DB_path, Fc2b_table, fc2b, overwrite=True)\n",
    "\n",
    "print('')\n",
    "print(getTableNames(DB_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5263b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra tables\n",
    "keep_tables = {'sqlite_sequence', 'Header', 'Hparam_T', 'FC1_Weight_T', \n",
    "               'FC2_Weight_T', 'FC1_Bias_T', 'FC2_Bias_T'}\n",
    "\n",
    "all_tables = getTableNames(DB_path)\n",
    "cnt = 0\n",
    "for name in all_tables:\n",
    "    if name not in keep_tables:\n",
    "        dropTable(DB_path, name)\n",
    "        print(f'WARN: Dropped table {name}')\n",
    "        cnt += 1\n",
    "print(f'INFO: {cnt} tables dropped')\n",
    "\n",
    "all_tables = getTableNames(DB_path)\n",
    "ipd.display(all_tables)\n",
    "\n",
    "# Delete names\n",
    "del all_tables, cnt, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c94f67",
   "metadata": {},
   "source": [
    "## Import Saved Model and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb07a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the meta tables\n",
    "rec_list = getRecords(DB_path, 'Header')\n",
    "print('Header:')\n",
    "for r in rec_list: print(r[1:-1])\n",
    "\n",
    "print('')\n",
    "rec_list = getRecords(DB_path, 'Hparam_T')\n",
    "print('Hparam:')\n",
    "for r in rec_list: print(r[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a table saved using createMatrixTable as a list of tuples\n",
    "def readMatrixTable(db_path, table_name):\n",
    "    # read the records\n",
    "    rec_list = getRecords(db_path, table_name)\n",
    "    # build the matrix\n",
    "    rec_list.sort()         # sort by row_no (first column)\n",
    "    matrix = []\n",
    "    for rec in rec_list:\n",
    "        matrix.append(rec[1:])  # stripe off the row_no columns\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# test this functions\n",
    "mat1 = np.array(readMatrixTable(DB_path, Fc1w_table))\n",
    "mat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a1a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the weights and biases as a dictionary\n",
    "def readModelParam(db_path, table_names):\n",
    "    model_params = {}\n",
    "    for name in table_names:\n",
    "        # read the matrix as a list of tuples\n",
    "        mat = readMatrixTable(db_path, name)\n",
    "        # Check if it is a matrix or a vector\n",
    "        if len(mat)==1: is_vector = True\n",
    "        else: is_vector = False\n",
    "        # convert to numpy array\n",
    "        if is_vector: mat = np.array(mat[0])    # make a 1D array for vectors\n",
    "        else: mat = np.array(mat)\n",
    "        # save it for returning\n",
    "        model_params[name] = mat\n",
    "    return model_params\n",
    "        \n",
    "\n",
    "# test this function\n",
    "param_tables = [\n",
    "    'FC1_Weight_T',\n",
    "    'FC2_Weight_T',\n",
    "    'FC1_Bias_T',\n",
    "    'FC2_Bias_T',\n",
    "]\n",
    "\n",
    "model_params = readModelParam(DB_path, param_tables)\n",
    "for k, v in model_params.items():\n",
    "    print(f'{k}:', v.shape, v.dtype)\n",
    "    \n",
    "\n",
    "# Delete names\n",
    "del k, v, mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3a229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with original weights\n",
    "org_params = {\n",
    "    'FC1_Weight_T': fc1_weight_np,\n",
    "    'FC2_Weight_T': fc2_weight_np,\n",
    "    'FC1_Bias_T': fc1_bias_np,\n",
    "    'FC2_Bias_T': fc2_bias_np,\n",
    "}\n",
    "\n",
    "\n",
    "def compare_model_params(model_params, org_params, tolerance):\n",
    "    for k in model_params:\n",
    "        print('\\nComparing:', k)\n",
    "        db_val = model_params[k]\n",
    "        org_val = org_params[k]\n",
    "        dmin = np.min(org_val)\n",
    "        dmax = np.max(org_val)\n",
    "        print('min:', dmin, '  max:', dmax)\n",
    "        diff_val = np.max(np.abs(db_val - org_val))   # get the maximum difference\n",
    "        print('diff:', diff_val)\n",
    "        assert diff_val <= tolerance   # use manual check\n",
    "        assert np.allclose(db_val, org_val, rtol=tolerance)  # use numpy built-in check\n",
    "\n",
    "\n",
    "# compare with tolerance\n",
    "tolerance = 1e-6\n",
    "compare_model_params(model_params, org_params, tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda20039",
   "metadata": {},
   "source": [
    "# Export the Dataset as sqlite3 DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64e2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database file\n",
    "DB_ds_path = './saved/mnist_test_data.s3db'\n",
    "createDB(DB_ds_path, overwrite=True)\n",
    "!ls -ltr saved/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50547cf1",
   "metadata": {},
   "source": [
    "## Save the Header Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the table and check \n",
    "createHeaderTable(DB_ds_path)\n",
    "table_names = getTableNames(DB_ds_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ecac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the records as a list\n",
    "Label_table = 'Labels_T'\n",
    "Dataitem_table = 'DataItems_T'\n",
    "Feature_table = 'Features_T'\n",
    "\n",
    "item = DS_loaded['dataset'][0]\n",
    "Feature_len = len(item[-1])    # last field in the item is the feature_vector\n",
    "\n",
    "header_records = [\n",
    "    ('name', 'MNIST-Test', 'Features (flattened images) extracted from the test dataset of MNIST.'),\n",
    "    ('feature_length', Feature_len, 'The length of the feature. These features can be directly fed to the MLP12 model'),\n",
    "    ('accuracy', Accuracy, 'Accuracy of the MLP12 model used to generate the \"predicted_index\" values.'),\n",
    "    \n",
    "    ('labels.table',  Label_table, 'Index to label mapping. The model predicts an index, which can be converted to the label using this table'),\n",
    "    ('dataset.table', Dataitem_table, 'This table serves as the (label, feature) list. The actual features are stored in a separate table.'), \n",
    "    ('dataitem.schema', '', 'There are 3 label-related fields in the dataset.table: \"label\" is the ground-truth, \"label_index\" is the index into the index-to-label mapping, \"predicted_index\" is the index predicted by the trained MLP12 model'),\n",
    "    ('features.table', Feature_table, 'Contains the actual features for the model.'),\n",
    "]\n",
    "\n",
    "# Insert the header records\n",
    "deleteRows(DB_ds_path, 'Header')  # delete previous records\n",
    "insertRecordList(DB_ds_path, insertHeaderRecord, header_records)\n",
    "records = getRecords(DB_ds_path, 'Header')\n",
    "for r in records: print(r[:-1])   # print all but description field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f479b970",
   "metadata": {},
   "source": [
    "## Save the Index-to-label mapping table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b955f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the labels table\n",
    "def createLableTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    # Create the table\n",
    "    query_str = f'''CREATE TABLE IF NOT EXISTS {Label_table} (\n",
    "                    label_index INTEGER PRIMARY KEY,\n",
    "                    label TEXT\n",
    "                )'''\n",
    "    cursor.execute(query_str)\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table and check \n",
    "createLableTable(DB_ds_path)\n",
    "table_names = getTableNames(DB_ds_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beed184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts a record into the labels table\n",
    "def insertLabelRecord(cursor, record):\n",
    "    label_index, label = record  # this serves as a soft check for the record format\n",
    "    # Insert the record into the table\n",
    "    query = f\"INSERT INTO {Label_table} (label_index, label) VALUES (?, ?)\"\n",
    "    cursor.execute(query, (label_index, label))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "deleteRows(DB_ds_path, Label_table)\n",
    "insertRecordList(DB_ds_path, insertLabelRecord, [(-1, 'test')])\n",
    "getRecords(DB_ds_path, Label_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9e8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the label records\n",
    "labels_dict = DS_loaded['label_dict']\n",
    "label_records = [(label_index, str(label)) for label, label_index in labels_dict.items()]\n",
    "print('label_records:', label_records)\n",
    "\n",
    "# Store them in the table\n",
    "deleteRows(DB_ds_path, Label_table)\n",
    "insertRecordList(DB_ds_path, insertLabelRecord, label_records)\n",
    "rec_list = getRecords(DB_ds_path, Label_table)\n",
    "print('rec_list:', rec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4eeb67",
   "metadata": {},
   "source": [
    "## Save the data-items and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07282db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataitems for DataItem table and Features table\n",
    "dataitem_records = []\n",
    "feature_records = []\n",
    "\n",
    "item = DS_loaded['dataset'][0]\n",
    "print('item[-1].type:', type(item[-1]))\n",
    "\n",
    "for item_index, item in enumerate(DS_loaded['dataset']):\n",
    "    label, label_index, pred_index, feat_len, feat_vec = item   # parse the item\n",
    "    feat_id = item_index      # use the index in the dataset as the feature ID\n",
    "    item_rec = [label, label_index, pred_index, feat_id]\n",
    "    feat_rec = [feat_id] + feat_vec    # feature-record: (feature-id, col_0, col_1, ...)\n",
    "    dataitem_records.append(item_rec)\n",
    "    feature_records.append(feat_rec)\n",
    "\n",
    "# check the records\n",
    "print('')\n",
    "print('dataitem_records:', len(dataitem_records), len(dataitem_records[0]))\n",
    "print('feature_records:', len(feature_records), len(feature_records[0]))\n",
    "\n",
    "check_index = 100\n",
    "item = DS_loaded['dataset'][check_index]\n",
    "assert feature_records[check_index][1:] == item[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a7527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del item_index, item, feat_id, item_rec, feat_rec, feat_len, feat_vec, label\n",
    "del key, r, rec_list, table_names, records, org_params, check_index, keep_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5ec08",
   "metadata": {},
   "source": [
    "## Save Data Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904f4eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the dataset table to save the data-items\n",
    "def createDataTable(db_path):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Define the table name and column names\n",
    "    table_name = Dataitem_table\n",
    "    columns = [\"id INTEGER PRIMARY KEY AUTOINCREMENT\",\n",
    "               \"label TEXT\",\n",
    "               \"label_index INTEGER\",\n",
    "               \"predicted_index INTEGER\",\n",
    "               \"feature_id INTEGER\"]\n",
    "\n",
    "    # Create the table\n",
    "    query = f\"CREATE TABLE IF NOT EXISTS {table_name} ({', '.join(columns)})\"\n",
    "    cursor.execute(query)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table\n",
    "createDataTable(DB_ds_path)\n",
    "table_names = getTableNames(DB_ds_path)\n",
    "print(table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83b744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts a record into the Dataset table\n",
    "def insertDataRecord(cursor, record):\n",
    "    label, label_index, predicted_index, feature_id = record  # this serves as a soft check for the record\n",
    "    # Insert the record into the table\n",
    "    query = f\"INSERT INTO {Dataitem_table} (label, label_index, predicted_index, feature_id) VALUES (?, ?, ?, ?)\"\n",
    "    cursor.execute(query, (label, label_index, predicted_index, feature_id))\n",
    "\n",
    "\n",
    "# Call the function to insert a record\n",
    "insertRecordList(DB_ds_path, insertDataRecord, [(\"Item 1\", 1, 2, 3)])\n",
    "getRecords(DB_ds_path, Dataitem_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617fe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert all dataset records\n",
    "deleteRows(DB_ds_path, Dataitem_table)   # delete old records\n",
    "insertRecordList(DB_ds_path, insertDataRecord, dataitem_records)\n",
    "rec_list = getRecords(DB_ds_path, Dataitem_table)\n",
    "print('rec_list:', len(rec_list), len(rec_list[0]))\n",
    "print('rec_list[0]', rec_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ab16c",
   "metadata": {},
   "source": [
    "## Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0df0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the features table to save the feature_vectors\n",
    "# Columns: feature_id, col_0, col_1, ..., col_n\n",
    "def createFeatureTable(db_path, table_name, feature_list, overwrite=False):\n",
    "    # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Drop the table if it exists and overwrite requested\n",
    "    table_exist = existTable(db_path, table_name)\n",
    "    if overwrite and table_exist:\n",
    "        print(f'WARN: Overwriting table {table_name}')\n",
    "        dropTable(db_path, table_name)\n",
    "        \n",
    "    # Create the table\n",
    "    cols = len(feature_list[0]) - 1    # ommitting the feature-id column from count\n",
    "    column_names = \"feature_id, \" + \", \".join([f\"col_{i}\" for i in range(cols)])  # Generate the column names string\n",
    "    cursor.execute(f\"CREATE TABLE {table_name} ({column_names})\")\n",
    "\n",
    "    # Insert the featurs into the table\n",
    "    for feat_item in tqdm(feature_list):\n",
    "        feat_id = feat_item[0]\n",
    "        feat_vec = feat_item[1:]\n",
    "        vals = f'{feat_id}, ' + ', '.join(map(str, feat_vec))\n",
    "        cursor.execute(f\"INSERT INTO {table_name} VALUES ({vals})\")\n",
    "\n",
    "    # Commit the changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Create the table\n",
    "createFeatureTable(DB_ds_path, Feature_table, feature_records, overwrite=True)\n",
    "table_names = getTableNames(DB_ds_path)\n",
    "print(table_names)\n",
    "\n",
    "rec_list = getRecords(DB_ds_path, Feature_table)\n",
    "print('rec_list:', len(rec_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6582ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_item = rec_list[0]\n",
    "print('feat_item:', len(feat_item))\n",
    "type_count = {}\n",
    "for d in feat_item: \n",
    "    t = type(d)\n",
    "    if t not in type_count: type_count[t] = 0\n",
    "    type_count[t] += 1\n",
    "print(type_count)\n",
    "\n",
    "\n",
    "# Delete names\n",
    "del feat_item, type_count, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583a130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra tables ---------------\n",
    "keep_tables = {'Header', 'sqlite_sequence', 'Labels_T', 'DataItems_T', 'Features_T'}\n",
    "\n",
    "all_tables = getTableNames(DB_ds_path)\n",
    "cnt = 0\n",
    "for name in all_tables:\n",
    "    if name not in keep_tables:\n",
    "        dropTable(DB_ds_path, name)\n",
    "        print(f'WARN: Dropped table {name}')\n",
    "        cnt += 1\n",
    "print(f'INFO: {cnt} tables dropped')\n",
    "\n",
    "all_tables = getTableNames(DB_ds_path)\n",
    "ipd.display(all_tables)\n",
    "\n",
    "# Delete names\n",
    "del all_tables, cnt, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5484707",
   "metadata": {},
   "source": [
    "## Import Saved Dataset and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe5f195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the meta tables\n",
    "rec_list = getRecords(DB_ds_path, 'Header')\n",
    "print('Header:')\n",
    "for r in rec_list: print(r[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2db20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with original features and dataitems\n",
    "def compare_dataset(db_items, org_items, tolerance):\n",
    "    range_iter = tqdm(range(len(db_items)))\n",
    "    for i in range_iter:\n",
    "        #print(i)\n",
    "        db_rec = db_items[i]\n",
    "        org_rec = org_items[i]\n",
    "        # Compare the labels\n",
    "        db_labels = db_rec[:3]\n",
    "        org_labels = org_rec[:3]\n",
    "        #print('db_rec:', db_rec)\n",
    "        #print('org_rec:', org_rec)\n",
    "        assert db_labels==org_labels, \"Labels mismatch\"\n",
    "        #if i==5: break\n",
    "        # Check features\n",
    "        org_feat = np.array(org_rec[-1])\n",
    "        db_feat = np.array(db_rec[-1])\n",
    "        assert np.allclose(org_feat, db_feat, tolerance), \"Feature vector mismatch\"\n",
    "    print(f'INFO: Compared {(i+1)} records')\n",
    "\n",
    "\n",
    "# merge the tables to make similar records as in DS_loaded['dataset']\n",
    "# build a feat_id: feat_vec map for merging.\n",
    "feat_records = getRecords(DB_ds_path, Feature_table)\n",
    "feat_rec_map = {}\n",
    "for fitem in feat_records:\n",
    "    feat_id = fitem[0]\n",
    "    feat_vec = fitem[1:]\n",
    "    feat_rec_map[feat_id] = feat_vec\n",
    "\n",
    "# merge feature vectors with dataset items for comparison\n",
    "data_records = getRecords(DB_ds_path, Dataitem_table)\n",
    "db_items = []\n",
    "for drec in data_records:\n",
    "    feat_id = drec[-1]\n",
    "    feat_vec = feat_rec_map[feat_id]\n",
    "    merged_item = list(drec[1:4]) + [feat_vec]   # remove ID column and concat feature vector\n",
    "    merged_item[0] = int(merged_item[0])\n",
    "    db_items.append(merged_item)\n",
    "        \n",
    "# compare with tolerance\n",
    "tolerance = 1e-6\n",
    "compare_dataset(db_items, DS_loaded['dataset'], tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c008d1fe",
   "metadata": {},
   "source": [
    "# Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97670aa2",
   "metadata": {},
   "source": [
    "Now you can use these databases to translate the Numpy model into C implementations. You can also run experiments on fixed-point precisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
